{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INLP - Assignment 2\n",
    "## Harshavardhan P - 2021111003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import conllu\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pprint as pp\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = ''\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word   pos\n",
      "0  what  PRON\n",
      "1    is   AUX\n",
      "2   the   DET\n",
      "3  cost  NOUN\n",
      "4    of   ADP\n",
      "       word   pos\n",
      "0         i  PRON\n",
      "1     would   AUX\n",
      "2      like  VERB\n",
      "3       the   DET\n",
      "4  cheapest   ADJ\n",
      "      word   pos\n",
      "0     what  PRON\n",
      "1      are   AUX\n",
      "2      the   DET\n",
      "3    coach  NOUN\n",
      "4  flights  NOUN\n"
     ]
    }
   ],
   "source": [
    "# import the data files\n",
    "dataset_path_train = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-train.conllu'\n",
    "dataset_path_dev = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-dev.conllu'\n",
    "dataset_path_test = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-test.conllu'\n",
    "\n",
    "dataset_train = conllu.parse_incr(open(dataset_path_train))\n",
    "dataset_dev = conllu.parse_incr(open(dataset_path_dev))\n",
    "dataset_test = conllu.parse_incr(open(dataset_path_test))\n",
    "\n",
    "# create a dataframe from the data\n",
    "def create_dataframe(dataset):\n",
    "    data = []\n",
    "    for tokenlist in dataset:\n",
    "        for token in tokenlist:\n",
    "            # data.append([token['form'], token['upostag']])\n",
    "            data.append([token['form'], token['upostag']])\n",
    "    # return pd.DataFrame(data, columns=['', 'word', 'pos'])\n",
    "    return pd.DataFrame(data, columns=['word', 'pos'])\n",
    "\n",
    "df_train = create_dataframe(dataset_train)\n",
    "df_dev = create_dataframe(dataset_dev)\n",
    "df_test = create_dataframe(dataset_test)\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_dev.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train\n",
    "vocab = set(df['word'])\n",
    "pos_tags = set(df['pos'])\n",
    "word_vectors_all = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(df, embedding_type='glove-wiki-gigaword-100'):\n",
    "    vocab = set(df['word'])\n",
    "    pos_tags = set(df['pos'])\n",
    "    word_vectors_all = api.load(embedding_type)\n",
    "\n",
    "    word_vectors = {}\n",
    "    for word in vocab:\n",
    "        if word in word_vectors_all:\n",
    "            word_vectors[word] = word_vectors_all[word]\n",
    "        else:\n",
    "            word_vectors[word] = np.zeros(len(word_vectors_all['the']))\n",
    "\n",
    "        # one hot encode the pos tags\n",
    "    pos_tags_one_hot = {}\n",
    "    for i, tag in enumerate(pos_tags):\n",
    "        one_hot = np.zeros(len(pos_tags) + 1)\n",
    "        one_hot[i] = 1\n",
    "        pos_tags_one_hot[tag] = one_hot\n",
    "    pos_tags_one_hot[''] = np.zeros(len(pos_tags) + 1)\n",
    "    pos_tags_one_hot[''][-1] = 1\n",
    "\n",
    "    # convert the df to list\n",
    "    data = df.values.tolist()\n",
    "    dataset = [[word_vectors[data[i][0]], pos_tags_one_hot[data[i][1]]] for i in range(len(data))]\n",
    "\n",
    "    return dataset, word_vectors, pos_tags_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dev_test(df, word_vectors, pos_tags_one_hot):\n",
    "    data = df.values.tolist()\n",
    "\n",
    "    dataset = []\n",
    "    for i in range(len(data)):\n",
    "        word = data[i][0]\n",
    "        if word in word_vectors:\n",
    "            word_vector = word_vectors[word]\n",
    "        else:\n",
    "            word_vector = np.zeros(len(word_vectors['the']))\n",
    "        if data[i][1] in pos_tags_one_hot:\n",
    "            pos_vector = pos_tags_one_hot[data[i][1]]\n",
    "        else:\n",
    "            pos_vector = pos_tags_one_hot['']\n",
    "        dataset.append([word_vector, pos_vector])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLUDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_vector = self.dataset[idx][0]\n",
    "        output_vector = self.dataset[idx][1]\n",
    "        return input_vector, output_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, word_vectors, pos_tags_one_hot = preprocess_train(df_train)\n",
    "dev_data = preprocess_dev_test(df_dev, word_vectors, pos_tags_one_hot)\n",
    "test_data = preprocess_dev_test(df_test, word_vectors, pos_tags_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conllu_dataset = CoNLLUDataset(train_data)\n",
    "dev_conllu_dataset = CoNLLUDataset(dev_data)\n",
    "test_conllu_dataset = CoNLLUDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_conllu_dataset, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_conllu_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_conllu_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an RNN which takes n dim input and returns pos tag vector\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, output_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.04050808957836699\n",
      "Epoch: 1, Loss: 0.02081149911375093\n",
      "Epoch: 2, Loss: 0.018400817108922256\n",
      "Epoch: 3, Loss: 0.017536231405410933\n",
      "Epoch: 4, Loss: 0.01684209633677375\n",
      "Epoch: 5, Loss: 0.016426075518446288\n",
      "Epoch: 6, Loss: 0.015941046073215046\n",
      "Epoch: 7, Loss: 0.015703355029638964\n",
      "Epoch: 8, Loss: 0.015553262761133042\n",
      "Epoch: 9, Loss: 0.015291220115547153\n",
      "Loss: 0.01847691132206819\n",
      "Accuracy: 0.9574051776038531\n",
      "Precision: 0.9479596221066687\n",
      "Recall: 0.9574051776038531\n",
      "F1: 0.9517832612114447\n"
     ]
    }
   ],
   "source": [
    "# Loss for classification\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def train_model(model, dataloader, epochs, optimizer, criterion):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch: {epoch}, Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            running_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    print(f'Loss: {running_loss / len(dataloader)}')\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_preds = all_preds.argmax(axis=1)\n",
    "    all_labels = all_labels.argmax(axis=1)\n",
    "    print(f'Accuracy: {accuracy_score(all_labels, all_preds)}')\n",
    "    print(f'Precision: {precision_score(all_labels, all_preds, average=\"weighted\")}')\n",
    "    print(f'Recall: {recall_score(all_labels, all_preds, average=\"weighted\")}')\n",
    "    print(f'F1: {f1_score(all_labels, all_preds, average=\"weighted\")}')\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Train the model\n",
    "model = RNN(100, 100, len(pos_tags_one_hot))\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_model(model, train_loader, 10, optimizer, criterion)\n",
    "\n",
    "# Evaluate the model\n",
    "all_labels, all_preds = evaluate_model(model, dev_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  word   pos\n",
      "0   1  what  PRON\n",
      "1   2    is   AUX\n",
      "2   3   the   DET\n",
      "3   4  cost  NOUN\n",
      "4   5    of   ADP\n",
      "   id      word   pos\n",
      "0   1         i  PRON\n",
      "1   2     would   AUX\n",
      "2   3      like  VERB\n",
      "3   4       the   DET\n",
      "4   5  cheapest   ADJ\n",
      "   id     word   pos\n",
      "0   1     what  PRON\n",
      "1   2      are   AUX\n",
      "2   3      the   DET\n",
      "3   4    coach  NOUN\n",
      "4   5  flights  NOUN\n"
     ]
    }
   ],
   "source": [
    "# import the data files\n",
    "dataset_path_train = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-train.conllu'\n",
    "dataset_path_dev = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-dev.conllu'\n",
    "dataset_path_test = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-test.conllu'\n",
    "\n",
    "dataset_train = conllu.parse_incr(open(dataset_path_train))\n",
    "dataset_dev = conllu.parse_incr(open(dataset_path_dev))\n",
    "dataset_test = conllu.parse_incr(open(dataset_path_test))\n",
    "\n",
    "# create a dataframe from the data\n",
    "def create_dataframe(dataset):\n",
    "    data = []\n",
    "    for tokenlist in dataset:\n",
    "        for token in tokenlist:\n",
    "            # data.append([token['form'], token['upostag']])\n",
    "            data.append([token['id'], token['form'], token['upostag']])\n",
    "    # return pd.DataFrame(data, columns=['', 'word', 'pos'])\n",
    "    return pd.DataFrame(data, columns=['id', 'word', 'pos'])\n",
    "\n",
    "df_train = create_dataframe(dataset_train)\n",
    "df_dev = create_dataframe(dataset_dev)\n",
    "df_test = create_dataframe(dataset_test)\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_dev.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train\n",
    "vocab = set(df['word'])\n",
    "pos_tags = set(df['pos'])\n",
    "word_vectors_all = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find vocabulary and POS tags, as well as load the word embeddings to be used\n",
    "def preprocess_train(df, p=3, s=3, embedding_type='glove-wiki-gigaword-100'):\n",
    "    vocab = set(df['word'])\n",
    "    pos_tags = set(df['pos'])\n",
    "    word_vectors_all = api.load(embedding_type)\n",
    "\n",
    "    word_vectors = {}\n",
    "    for word in vocab:\n",
    "        if word in word_vectors_all:\n",
    "            word_vectors[word] = word_vectors_all[word]\n",
    "        else:\n",
    "            word_vectors[word] = torch.zeros(len(word_vectors_all['the']))\n",
    "\n",
    "    # one hot encode the POS tags\n",
    "    pos_tags_one_hot = {}\n",
    "    for i, tag in enumerate(pos_tags):\n",
    "        one_hot = torch.zeros(len(pos_tags) + 1)\n",
    "        one_hot[i] = 1\n",
    "        pos_tags_one_hot[tag] = one_hot\n",
    "    pos_tags_one_hot[''] = torch.zeros(len(pos_tags) + 1)\n",
    "    pos_tags_one_hot[''][-1] = 1\n",
    "\n",
    "    # convert the df to list\n",
    "    data = df.values.tolist()\n",
    "    dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "    # pp.pprint(dataset)\n",
    "\n",
    "    max = 0\n",
    "    for i in range(len(dataset)):\n",
    "        if data[i][0] > max:\n",
    "            max = data[i][0]\n",
    "\n",
    "    split_dataset = []\n",
    "    curr = 0\n",
    "    for i in range(1, len(dataset)):\n",
    "        if data[i][0] == 1:\n",
    "            split_dataset.append(dataset[curr:i])\n",
    "            curr = i\n",
    "    split_dataset.append(dataset[curr:])\n",
    "    # print(len(split_dataset))\n",
    "    # print(max)\n",
    "\n",
    "    final_dataset = []\n",
    "    for i in range(len(split_dataset)):\n",
    "        dataset = split_dataset[i]\n",
    "        dataset1 = dataset.copy()\n",
    "        dataset2 = dataset.copy()\n",
    "        for j in range(p):\n",
    "            dataset1 = dataset1[:-1]\n",
    "            dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "            dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "        for j in range(s):\n",
    "            dataset2 = dataset2[1:]\n",
    "            dataset2 = np.append(dataset2, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "            dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "        final_dataset.append(dataset)\n",
    "\n",
    "    # pp.pprint(final_dataset[0][0])\n",
    "    # pp.pprint(final_dataset[0][-1])\n",
    "\n",
    "    dataset = []\n",
    "    for lst in final_dataset:\n",
    "        dataset.extend(lst)\n",
    "    dataset = np.reshape(dataset, (len(dataset), 1, len(dataset[0])))\n",
    "    print(dataset.shape)\n",
    "\n",
    "    final_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        final_dataset.append([torch.tensor(dataset[i][0]), pos_tags_one_hot[data[i][2]]])\n",
    "\n",
    "    print(len(final_dataset))\n",
    "    print(len(final_dataset[0]))\n",
    "    print(len(final_dataset[0][0]))\n",
    "    print(len(final_dataset[0][1]))\n",
    "    # print(type(final_dataset[0][0]))\n",
    "    # print(type(final_dataset[0][1]))\n",
    "    # print(final_dataset[22][0])\n",
    "    # print(final_dataset[0][1])\n",
    "\n",
    "    return final_dataset, word_vectors, pos_tags_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess the dev and test data, using the word vectors and POS tags from the training data\n",
    "def preprocess_dev_test(df, word_vectors, pos_tags_one_hot, p=3, s=3):\n",
    "    data = df.values.tolist()\n",
    "    # dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "    dataset = []\n",
    "    for i in range(len(data)):\n",
    "        if data[i][1] in word_vectors:\n",
    "            dataset.append(word_vectors[data[i][1]])\n",
    "        else:\n",
    "            dataset.append(torch.zeros(len(word_vectors['the'])))\n",
    "    dataset = np.array(dataset, dtype=np.float32)\n",
    "    # pp.pprint(dataset)\n",
    "\n",
    "    max = 0\n",
    "    for i in range(len(dataset)):\n",
    "        if data[i][0] > max:\n",
    "            max = data[i][0]\n",
    "\n",
    "    split_dataset = []\n",
    "    curr = 0\n",
    "    for i in range(1, len(dataset)):\n",
    "        if data[i][0] == 1:\n",
    "            split_dataset.append(dataset[curr:i])\n",
    "            curr = i\n",
    "    split_dataset.append(dataset[curr:])\n",
    "    # print(len(split_dataset))\n",
    "    # print(max)\n",
    "\n",
    "    final_dataset = []\n",
    "    for i in range(len(split_dataset)):\n",
    "        dataset = split_dataset[i]\n",
    "        dataset1 = dataset.copy()\n",
    "        dataset2 = dataset.copy()\n",
    "        for j in range(p):\n",
    "            dataset1 = dataset1[:-1]\n",
    "            dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors['the']))], axis=0)\n",
    "            dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "        for j in range(s):\n",
    "            dataset2 = dataset2[1:]\n",
    "            dataset2 = np.append(dataset2, [np.zeros(len(word_vectors['the']))], axis=0)\n",
    "            dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "        final_dataset.append(dataset)\n",
    "\n",
    "    # pp.pprint(final_dataset[0][0])\n",
    "    # pp.pprint(final_dataset[0][-1])\n",
    "\n",
    "    dataset = []\n",
    "    for lst in final_dataset:\n",
    "        dataset.extend(lst)\n",
    "    dataset = np.reshape(dataset, (len(dataset), 1, len(dataset[0])))\n",
    "    print(dataset.shape)\n",
    "\n",
    "    final_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        # final_dataset.append([torch.tensor(dataset[i][0]), pos_tags_one_hot[data[i][2]]])\n",
    "        tensor1 = torch.tensor(dataset[i][0])\n",
    "        try:\n",
    "            tensor2 = pos_tags_one_hot[data[i][2]]\n",
    "        except:\n",
    "            tensor2 = pos_tags_one_hot['']\n",
    "        final_dataset.append([tensor1, tensor2])\n",
    "\n",
    "    print(len(final_dataset))\n",
    "    print(len(final_dataset[0]))\n",
    "    print(len(final_dataset[0][0]))\n",
    "    print(len(final_dataset[0][1]))\n",
    "    # print(type(final_dataset[0][0]))\n",
    "    # print(type(final_dataset[0][1]))\n",
    "    # print(final_dataset[22][0])\n",
    "    # print(final_dataset[0][1])\n",
    "\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLUDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.dataset = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # input_tensor = torch.tensor(self.dataset[idx][0], dtype=torch.float32, device=device)\n",
    "        input_tensor = self.dataset[idx][0].to(device)\n",
    "        # target_tensor = torch.tensor(self.dataset[idx][1], dtype=torch.float32, device=device)\n",
    "        target_tensor = self.dataset[idx][1].to(device)\n",
    "        return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48655, 1, 700)\n",
      "48655\n",
      "2\n",
      "700\n",
      "14\n",
      "(6644, 1, 700)\n",
      "6644\n",
      "2\n",
      "700\n",
      "14\n",
      "(6580, 1, 700)\n",
      "6580\n",
      "2\n",
      "700\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# preprocess the data\n",
    "train_data, word_vectors, pos_tags_one_hot = preprocess_train(df_train)\n",
    "dev_data = preprocess_dev_test(df_dev, word_vectors, pos_tags_one_hot)\n",
    "test_data = preprocess_dev_test(df_test, word_vectors, pos_tags_one_hot)\n",
    "\n",
    "# create the dataloaders\n",
    "train_conllu_dataset = CoNLLUDataset(train_data)\n",
    "dev_conllu_dataset = CoNLLUDataset(dev_data)\n",
    "test_conllu_dataset = CoNLLUDataset(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_conllu_dataset, batch_size=64, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_conllu_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_conllu_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an FNN which takes n dim input and returns pos tag vector\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, embed_dim, prev_n, succ_n, hidden_params, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        # for each element in hidden_params, we will create a linear layer\n",
    "        hidden_layers = []\n",
    "        hidden_layers.append(nn.Linear(embed_dim * (prev_n + 1 + succ_n), hidden_params[0]))\n",
    "        hidden_layers.append(nn.ReLU())\n",
    "        for i in range(1, len(hidden_params)):\n",
    "            hidden_layers.append(nn.Linear(hidden_params[i-1], hidden_params[i]))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "        # softmax layer for output\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "        self.output_layer = nn.Linear(hidden_params[-1], output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1243.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.06294232297507599\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1296.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss: 0.01834471497296768\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1312.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss: 0.013945934147547695\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1306.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss: 0.012291995934382051\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1316.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss: 0.011048197265929798\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1309.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss: 0.010227406191708135\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1315.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss: 0.00956300159065119\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1310.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 0.00933217855935865\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1337.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, loss: 0.008715343046792674\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1326.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, loss: 0.008524825179070704\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1222.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 0.008333699633082822\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1328.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, loss: 0.008177918466221836\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1322.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, loss: 0.008006280092748907\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1121.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, loss: 0.007875322624025242\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1324.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, loss: 0.007768998321503126\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1310.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, loss: 0.007626331794585386\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1268.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, loss: 0.007518779004464457\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1319.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, loss: 0.0074832055258164465\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1309.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, loss: 0.007388393546554961\n",
      "Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1326.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, loss: 0.007197025139925025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:00<00:00, 2602.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.00977199338376522\n",
      "\n",
      "Accuracy: 0.9790273556231003\n",
      "Precision: 0.9787557668916996\n",
      "Recall: 0.9790273556231003\n",
      "F1 Score: 0.978636193731978\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZvklEQVR4nO3da2xU97nv8d/YxmPHsifYKbZH2MGJkAjgEIIBgaMWhBUOIgRUJZTIaSzQaavWFIx1UqCtoREBB5oixEUmsNWEStzyIlyCdqhch4s44e44O6gNFwWBFbZNs5XMgBETM7POi55M43C1WTPPjPl+pPVi1iz8fyaX+WqNF2s8juM4AgAgzlKsBwAAPJgIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMJFmPcD3RSIRXbp0SdnZ2fJ4PNbjAAC6yXEcXblyRX6/Xykptz/PSbgAXbp0SUVFRdZjAADuU2trq/r373/b5xMuQNnZ2ZKkEf/rd0rtkxHTtR56/0RMfz4APIhuqFOH9J/R9/PbSbgAffuxW2qfDKXFOEBpnj4x/fkA8ED6/3cYvduvUbgIAQBgggABAEwQIACACQIEADBBgAAAJggQAMBEzAK0bt06DRgwQBkZGRo9erSOHTsWq6UAAEkoJgHavn27amtrtXjxYjU3N2vYsGGaOHGiLl++HIvlAABJKCYBWrlypX72s59p5syZGjx4sNavX6+HHnpIf/7zn2OxHAAgCbkeoG+++UYnT55URUXFvxdJSVFFRYUOHz580/GhUEjBYLDLBgDo/VwP0JdffqlwOKz8/Pwu+/Pz89XW1nbT8fX19fL5fNGNG5ECwIPB/Cq4hQsXKhAIRLfW1lbrkQAAceD6zUgfeeQRpaamqr29vcv+9vZ2FRQU3HS81+uV1+t1ewwAQIJz/QwoPT1dI0aMUFNTU3RfJBJRU1OTxowZ4/ZyAIAkFZOvY6itrVVVVZXKyso0atQorVq1Sh0dHZo5c2YslgMAJKGYBOgnP/mJ/vnPf2rRokVqa2vTU089pb179950YQIA4MEVsy+kmz17tmbPnh2rHw8ASHLmV8EBAB5MBAgAYIIAAQBMECAAgAkCBAAwEbOr4O7XQ++fUJqnT0zX+Oullpj+/O+a6H8qbmsBQDLgDAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMpFkPYGmi/6m4rZXy5KC4rBP5r8/isg4A3C/OgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACZcD1B9fb1Gjhyp7Oxs9evXT9OmTdPp06fdXgYAkORcD9CBAwdUXV2tI0eOqLGxUZ2dnXr22WfV0dHh9lIAgCTm+r3g9u7d2+XxO++8o379+unkyZP64Q9/6PZyAIAkFfObkQYCAUlSbm7uLZ8PhUIKhULRx8FgMNYjAQASQEwvQohEIqqpqVF5ebmGDh16y2Pq6+vl8/miW1FRUSxHAgAkiJgGqLq6WqdOndK2bdtue8zChQsVCASiW2trayxHAgAkiJh9BDd79mzt2bNHBw8eVP/+/W97nNfrldfrjdUYAIAE5XqAHMfRr3/9a+3YsUP79+9XSUmJ20sAAHoB1wNUXV2tLVu2aNeuXcrOzlZbW5skyefzKTMz0+3lAABJyvXfATU0NCgQCGjcuHEqLCyMbtu3b3d7KQBAEovJR3AAANwN94IDAJggQAAAEwQIAGCCAAEATBAgAICJmN+MFP8S+a/P4rKOJ453lXC+cxNZAOguzoAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADARJr1AHCXEwrFba1L/2dsXNbxv/lRXNaJq5TU+KwTCcdnHaAHOAMCAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwETMA/TGG2/I4/GopqYm1ksBAJJITAN0/PhxvfXWW3ryySdjuQwAIAnFLEBXr15VZWWlNm7cqL59+8ZqGQBAkopZgKqrqzV58mRVVFTc8bhQKKRgMNhlAwD0fjG5Gem2bdvU3Nys48eP3/XY+vp6vfbaa7EYAwCQwFw/A2ptbdXcuXO1efNmZWRk3PX4hQsXKhAIRLfW1la3RwIAJCDXz4BOnjypy5cv6+mnn47uC4fDOnjwoNauXatQKKTU1H/fit7r9crr9bo9BgAgwbkeoAkTJujTTz/tsm/mzJkaNGiQ5s+f3yU+AIAHl+sBys7O1tChQ7vsy8rKUl5e3k37AQAPLu6EAAAwEZev5N6/f388lgEAJBHOgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMxOUybPRO/jc/iss6//O/x8Rlnbz/OByXdSRJkXD81gISFGdAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYCLNegBTHk/81nKc+K3Vy+T9x+H4rPN/+8ZlHUn6n/Kv4rYWkKg4AwIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAREwC9MUXX+jll19WXl6eMjMzVVpaqhMnTsRiKQBAknL9TghfffWVysvLNX78eH3wwQf6wQ9+oLNnz6pv3/j9LXMAQOJzPUDLly9XUVGR3n777ei+kpISt5cBACQ51z+C2717t8rKyvTiiy+qX79+Gj58uDZu3Hjb40OhkILBYJcNAND7uR6gzz//XA0NDRo4cKD++te/6pe//KXmzJmjTZs23fL4+vp6+Xy+6FZUVOT2SACABORxHHdv05yenq6ysjJ99NFH0X1z5szR8ePHdfjwzXc1DoVCCoVC0cfBYFBFRUUap6lK8/Rxc7SbcTdsfAd3wwbcccPp1H7tUiAQUE5Ozm2Pc/0MqLCwUIMHD+6y74knntDFixdvebzX61VOTk6XDQDQ+7keoPLycp0+fbrLvjNnzujRRx91eykAQBJzPUDz5s3TkSNHtGzZMp07d05btmzRhg0bVF1d7fZSAIAk5nqARo4cqR07dmjr1q0aOnSolixZolWrVqmystLtpQAASSwmX8n93HPP6bnnnovFjwYA9BLcCw4AYIIAAQBMECAAgAkCBAAwQYAAACYIEADAREwuw04a3J8N3xHP+7O99vnJuKyz+LERcVkH6AnOgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJtKsBzDl8cRvLceJ31pIeIsfGxGXddIKC+KyjiTd+O+2uK2F3oEzIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAnXAxQOh1VXV6eSkhJlZmbq8ccf15IlS+RwJwAAwHe4fiue5cuXq6GhQZs2bdKQIUN04sQJzZw5Uz6fT3PmzHF7OQBAknI9QB999JGmTp2qyZMnS5IGDBigrVu36tixY24vBQBIYq5/BDd27Fg1NTXpzJkzkqRPPvlEhw4d0qRJk255fCgUUjAY7LIBAHo/18+AFixYoGAwqEGDBik1NVXhcFhLly5VZWXlLY+vr6/Xa6+95vYYAIAE5/oZ0LvvvqvNmzdry5Ytam5u1qZNm/Tmm29q06ZNtzx+4cKFCgQC0a21tdXtkQAACcj1M6BXX31VCxYs0IwZMyRJpaWlunDhgurr61VVVXXT8V6vV16v1+0xAAAJzvUzoGvXriklpeuPTU1NVSQScXspAEASc/0MaMqUKVq6dKmKi4s1ZMgQffzxx1q5cqVmzZrl9lIAgCTmeoDWrFmjuro6/epXv9Lly5fl9/v1i1/8QosWLXJ7KQBAEvM4CXaLgmAwKJ/Pp3GaqjRPn9gu5vHE9ud/V2L9Y8YDIq2wIG5r3fjvtrithcR2w+nUfu1SIBBQTk7ObY/jXnAAABMECABgggABAEwQIACACQIEADBBgAAAJlz/e0BJJZ6XRqekxmedSDg+6yApxPPS6JSsrLisE+noiMs6iD3OgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJtKsB3hgRMLWEwAxFenoiMs6aQX5cVlHkm60tcdtrQcRZ0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAAT3Q7QwYMHNWXKFPn9fnk8Hu3cubPL847jaNGiRSosLFRmZqYqKip09uxZt+YFAPQS3Q5QR0eHhg0bpnXr1t3y+RUrVmj16tVav369jh49qqysLE2cOFHXr1+/72EBAL1Ht+8FN2nSJE2aNOmWzzmOo1WrVun3v/+9pk6dKkn6y1/+ovz8fO3cuVMzZsy4v2kBAL2Gq78DOn/+vNra2lRRURHd5/P5NHr0aB0+fPiWfyYUCikYDHbZAAC9n6sBamtrkyTl53e9W21+fn70ue+rr6+Xz+eLbkVFRW6OBABIUOZXwS1cuFCBQCC6tba2Wo8EAIgDVwNUUFAgSWpv7/odGu3t7dHnvs/r9SonJ6fLBgDo/VwNUElJiQoKCtTU1BTdFwwGdfToUY0ZM8bNpQAASa7bV8FdvXpV586diz4+f/68WlpalJubq+LiYtXU1Oj111/XwIEDVVJSorq6Ovn9fk2bNs3NuQEASa7bATpx4oTGjx8ffVxbWytJqqqq0jvvvKPf/OY36ujo0M9//nN9/fXXeuaZZ7R3715lZGS4NzUAIOl5HMdxrIf4rmAwKJ/Pp3GaqjRPH+txACSYtIL8ux/kkhtt7Xc/CDe54XRqv3YpEAjc8ff65lfBAQAeTAQIAGCCAAEATBAgAIAJAgQAMEGAAAAmuv33gAC4wOOJzzqJ9bcsXBHPS6NTsrLisk6koyMu6yQazoAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACbSrAcAHkiOYz0B7kGkoyM+C40qjc86xz6Nzzr3iDMgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACAiW4H6ODBg5oyZYr8fr88Ho927twZfa6zs1Pz589XaWmpsrKy5Pf79corr+jSpUtuzgwA6AW6HaCOjg4NGzZM69atu+m5a9euqbm5WXV1dWpubtZ7772n06dP6/nnn3dlWABA79Hte8FNmjRJkyZNuuVzPp9PjY2NXfatXbtWo0aN0sWLF1VcXNyzKQEAvU7Mb0YaCATk8Xj08MMP3/L5UCikUCgUfRwMBmM9EgAgAcT0IoTr169r/vz5eumll5STk3PLY+rr6+Xz+aJbUVFRLEcCACSImAWos7NT06dPl+M4amhouO1xCxcuVCAQiG6tra2xGgkAkEBi8hHct/G5cOGCPvzww9ue/UiS1+uV1+uNxRgAgATmeoC+jc/Zs2e1b98+5eXlub0EAKAX6HaArl69qnPnzkUfnz9/Xi0tLcrNzVVhYaFeeOEFNTc3a8+ePQqHw2pra5Mk5ebmKj093b3JAQBJrdsBOnHihMaPHx99XFtbK0mqqqrSH/7wB+3evVuS9NRTT3X5c/v27dO4ceN6PikAoFfpdoDGjRsn5w7fZ3+n5wAA+Bb3ggMAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwEfO7YQMA7uLYp9YTmOAMCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABNp1gN8n+M4kqQb6pQc42EAAN12Q52S/v1+fjsJF6ArV65Ikg7pP40nAQDcjytXrsjn8932eY9zt0TFWSQS0aVLl5SdnS2Px3PPfy4YDKqoqEitra3KycmJ4YTx0dtej8RrSha8psSX6K/HcRxduXJFfr9fKSm3/01Pwp0BpaSkqH///j3+8zk5OQn5L6SnetvrkXhNyYLXlPgS+fXc6cznW1yEAAAwQYAAACZ6TYC8Xq8WL14sr9drPYoretvrkXhNyYLXlPh6y+tJuIsQAAAPhl5zBgQASC4ECABgggABAEwQIACAiV4RoHXr1mnAgAHKyMjQ6NGjdezYMeuReqy+vl4jR45Udna2+vXrp2nTpun06dPWY7nmjTfekMfjUU1NjfUo9+2LL77Qyy+/rLy8PGVmZqq0tFQnTpywHqtHwuGw6urqVFJSoszMTD3++ONasmTJXe/llUgOHjyoKVOmyO/3y+PxaOfOnV2edxxHixYtUmFhoTIzM1VRUaGzZ8/aDHuP7vSaOjs7NX/+fJWWliorK0t+v1+vvPKKLl26ZDdwNyV9gLZv367a2lotXrxYzc3NGjZsmCZOnKjLly9bj9YjBw4cUHV1tY4cOaLGxkZ1dnbq2WefVUdHh/Vo9+348eN666239OSTT1qPct+++uorlZeXq0+fPvrggw/097//XX/605/Ut29f69F6ZPny5WpoaNDatWv1j3/8Q8uXL9eKFSu0Zs0a69HuWUdHh4YNG6Z169bd8vkVK1Zo9erVWr9+vY4ePaqsrCxNnDhR169fj/Ok9+5Or+natWtqbm5WXV2dmpub9d577+n06dN6/vnnDSbtISfJjRo1yqmuro4+DofDjt/vd+rr6w2ncs/ly5cdSc6BAwesR7kvV65ccQYOHOg0NjY6P/rRj5y5c+daj3Rf5s+f7zzzzDPWY7hm8uTJzqxZs7rs+/GPf+xUVlYaTXR/JDk7duyIPo5EIk5BQYHzxz/+Mbrv66+/drxer7N161aDCbvv+6/pVo4dO+ZIci5cuBCfoe5TUp8BffPNNzp58qQqKiqi+1JSUlRRUaHDhw8bTuaeQCAgScrNzTWe5P5UV1dr8uTJXf5dJbPdu3errKxML774ovr166fhw4dr48aN1mP12NixY9XU1KQzZ85Ikj755BMdOnRIkyZNMp7MHefPn1dbW1uX//58Pp9Gjx7da94rpH+9X3g8Hj388MPWo9yThLsZaXd8+eWXCofDys/P77I/Pz9fn332mdFU7olEIqqpqVF5ebmGDh1qPU6Pbdu2Tc3NzTp+/Lj1KK75/PPP1dDQoNraWv32t7/V8ePHNWfOHKWnp6uqqsp6vG5bsGCBgsGgBg0apNTUVIXDYS1dulSVlZXWo7mira1Nkm75XvHtc8nu+vXrmj9/vl566aWEvUHp9yV1gHq76upqnTp1SocOHbIepcdaW1s1d+5cNTY2KiMjw3oc10QiEZWVlWnZsmWSpOHDh+vUqVNav359Ugbo3Xff1ebNm7VlyxYNGTJELS0tqqmpkd/vT8rX86Dp7OzU9OnT5TiOGhoarMe5Z0n9Edwjjzyi1NRUtbe3d9nf3t6ugoICo6ncMXv2bO3Zs0f79u27r6+nsHby5EldvnxZTz/9tNLS0pSWlqYDBw5o9erVSktLUzgcth6xRwoLCzV48OAu+5544gldvHjRaKL78+qrr2rBggWaMWOGSktL9dOf/lTz5s1TfX299Wiu+Pb9oDe+V3wbnwsXLqixsTFpzn6kJA9Qenq6RowYoaampui+SCSipqYmjRkzxnCynnMcR7Nnz9aOHTv04YcfqqSkxHqk+zJhwgR9+umnamlpiW5lZWWqrKxUS0uLUlNTrUfskfLy8psujz9z5oweffRRo4nuz7Vr12764rDU1FRFIhGjidxVUlKigoKCLu8VwWBQR48eTdr3Cunf8Tl79qz+9re/KS8vz3qkbkn6j+Bqa2tVVVWlsrIyjRo1SqtWrVJHR4dmzpxpPVqPVFdXa8uWLdq1a5eys7Ojn0/7fD5lZmYaT9d92dnZN/3+KisrS3l5eUn9e6158+Zp7NixWrZsmaZPn65jx45pw4YN2rBhg/VoPTJlyhQtXbpUxcXFGjJkiD7++GOtXLlSs2bNsh7tnl29elXnzp2LPj5//rxaWlqUm5ur4uJi1dTU6PXXX9fAgQNVUlKiuro6+f1+TZs2zW7ou7jTayosLNQLL7yg5uZm7dmzR+FwOPp+kZubq/T0dKux7531ZXhuWLNmjVNcXOykp6c7o0aNco4cOWI9Uo9JuuX29ttvW4/mmt5wGbbjOM7777/vDB061PF6vc6gQYOcDRs2WI/UY8Fg0Jk7d65TXFzsZGRkOI899pjzu9/9zgmFQtaj3bN9+/bd8v+dqqoqx3H+dSl2XV2dk5+f73i9XmfChAnO6dOnbYe+izu9pvPnz9/2/WLfvn3Wo98Tvo4BAGAiqX8HBABIXgQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACAif8HsFBtxsb7k1MAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:00<00:00, 2220.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.011737177610204807\n",
      "\n",
      "Accuracy: 0.9718543046357616\n",
      "Precision: 0.9713818377510385\n",
      "Recall: 0.9718543046357616\n",
      "F1 Score: 0.9714265129524949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss for classification\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# function to train the model\n",
    "def train_model(model, dataloader, epochs, optimizer, criterion):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        running_loss = 0.0\n",
    "        # for i, data in enumerate(dataloader, 0):\n",
    "        for i, data in enumerate(tqdm.tqdm(dataloader, position=0, leave=True), 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "# function to test the model\n",
    "def test_model(model, dataloader, criterion, pos_tags_one_hot):\n",
    "    # find loss, accuracy, precision, recall, f1 score\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_outputs = []\n",
    "    total_labels = []\n",
    "    with torch.no_grad():\n",
    "        # for data in dataloader:\n",
    "        for data in tqdm.tqdm(dataloader, position=0, leave=True):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, actual = torch.max(labels, 1)\n",
    "            outputs_copy = outputs.clone().detach().cpu().numpy()\n",
    "            outputs_one_hot = np.zeros(outputs_copy.shape)\n",
    "            outputs_one_hot[np.arange(outputs_copy.shape[0]), np.argmax(outputs_copy, axis=1)] = 1\n",
    "\n",
    "            total_outputs.extend(outputs_one_hot)\n",
    "            total_labels.extend(labels.clone().detach().cpu().numpy())\n",
    "\n",
    "    total_outputs = np.array(total_outputs)\n",
    "    total_labels = np.array(total_labels)\n",
    "\n",
    "    print()\n",
    "    print(f\"Loss: {running_loss/len(dataloader)}\")\n",
    "    print()\n",
    "    print(f\"Accuracy: {accuracy_score(total_labels, total_outputs)}\")\n",
    "    print(f\"Precision: {precision_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "    print(f\"Recall: {recall_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "    print(f\"F1 Score: {f1_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "\n",
    "    confusion_matrix = np.zeros((len(pos_tags_one_hot), len(pos_tags_one_hot)))\n",
    "    for i in range(len(total_labels)):\n",
    "        actual = np.argmax(total_labels[i])\n",
    "        predicted = np.argmax(total_outputs[i])\n",
    "        confusion_matrix[actual][predicted] += 1\n",
    "    confusion_matrix2 = confusion_matrix / np.sum(confusion_matrix, axis=1)\n",
    "\n",
    "    plt.imshow(confusion_matrix)\n",
    "    plt.show()\n",
    "    plt.imshow(confusion_matrix2)\n",
    "    plt.show()\n",
    "\n",
    "# train and test the FNN model\n",
    "fnn_model = FNN(100, 3, 3, [20, 20], len(pos_tags_one_hot)).to(device)\n",
    "optimizer = torch.optim.Adam(fnn_model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "train_model(fnn_model, train_dataloader, 20, optimizer, criterion)\n",
    "test_model(fnn_model, test_dataloader, criterion, pos_tags_one_hot)\n",
    "test_model(fnn_model, dev_dataloader, criterion, pos_tags_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNTrainer:\n",
    "    def __init__(self, pos_tags_one_hot, embedding_type, df_train, df_test, df_dev, criterion='bce', optimizer='adam'):\n",
    "        self.pos_tags_one_hot = pos_tags_one_hot\n",
    "        self.criterion, self.optimizer = self.setup_cr_op(criterion, optimizer)\n",
    "        self.train_dataloader, self.test_dataloader, self.dev_dataloader = self.setup_dataloaders(df_train, df_test, df_dev)\n",
    "\n",
    "    def create_model(self, embed_dim, prev_n, succ_n, hidden_params, output_dim):\n",
    "        self.model =  self.FNN(embed_dim, prev_n, succ_n, hidden_params, output_dim)\n",
    "\n",
    "    def setup_cr_op(self, criterion, optimizer):\n",
    "        criterion_ = None\n",
    "        optimizer_ = None\n",
    "        if criterion == 'cross_entropy':\n",
    "            criterion_ = nn.CrossEntropyLoss()\n",
    "        elif criterion == 'bce':\n",
    "            criterion_ = nn.BCELoss()\n",
    "        else:\n",
    "            print('Invalid criterion')\n",
    "\n",
    "        if optimizer == 'adam':\n",
    "            optimizer_ = torch.optim.Adam(self.model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "        elif optimizer == 'sgd':\n",
    "            optimizer_ = torch.optim.SGD(self.model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "        else:\n",
    "            print('Invalid optimizer')\n",
    "\n",
    "        return criterion_, optimizer_\n",
    "\n",
    "    def setup_dataloaders(self, df_train, df_test, df_dev):\n",
    "        train_data, word_vectors, pos_tags_one_hot = self.preprocess_train(df_train)\n",
    "        dev_data = self.preprocess_dev_test(df_dev, word_vectors, pos_tags_one_hot)\n",
    "        test_data = self.preprocess_dev_test(df_test, word_vectors, pos_tags_one_hot)\n",
    "\n",
    "        train_conllu_dataset = self.CoNLLUDataset(train_data)\n",
    "        dev_conllu_dataset = self.CoNLLUDataset(dev_data)\n",
    "        test_conllu_dataset = self.CoNLLUDataset(test_data)\n",
    "\n",
    "        train_dataloader = DataLoader(train_conllu_dataset, batch_size=64, shuffle=True)\n",
    "        dev_dataloader = DataLoader(dev_conllu_dataset, batch_size=64, shuffle=True)\n",
    "        test_dataloader = DataLoader(test_conllu_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        return train_dataloader, test_dataloader, dev_dataloader\n",
    "\n",
    "    # creating an FNN which takes n dim input and returns pos tag vector\n",
    "    class FNN(nn.Module):\n",
    "        def __init__(self, embed_dim, prev_n, succ_n, hidden_params, output_dim):\n",
    "            super(FNN, self).__init__()\n",
    "            # for each element in hidden_params, we will create a linear layer\n",
    "            hidden_layers = []\n",
    "            hidden_layers.append(nn.Linear(embed_dim * (prev_n + 1 + succ_n), hidden_params[0]))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "            for i in range(1, len(hidden_params)):\n",
    "                hidden_layers.append(nn.Linear(hidden_params[i-1], hidden_params[i]))\n",
    "                hidden_layers.append(nn.ReLU())\n",
    "            # softmax layer for output\n",
    "            self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "            self.output_layer = nn.Linear(hidden_params[-1], output_dim)\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.hidden_layers(x)\n",
    "            x = self.output_layer(x)\n",
    "            x = self.softmax(x)\n",
    "            return x\n",
    "\n",
    "    # function to find vocabulary and POS tags, as well as load the word embeddings to be used\n",
    "    def preprocess_train(self, df, p=3, s=3, embedding_type='glove-wiki-gigaword-100'):\n",
    "        vocab = set(df['word'])\n",
    "        pos_tags = set(df['pos'])\n",
    "        word_vectors_all = api.load(embedding_type)\n",
    "\n",
    "        word_vectors = {}\n",
    "        for word in vocab:\n",
    "            if word in word_vectors_all:\n",
    "                word_vectors[word] = word_vectors_all[word]\n",
    "            else:\n",
    "                word_vectors[word] = torch.zeros(len(word_vectors_all['the']))\n",
    "\n",
    "        # one hot encode the POS tags\n",
    "        pos_tags_one_hot = {}\n",
    "        for i, tag in enumerate(pos_tags):\n",
    "            one_hot = torch.zeros(len(pos_tags) + 1)\n",
    "            one_hot[i] = 1\n",
    "            pos_tags_one_hot[tag] = one_hot\n",
    "        pos_tags_one_hot[''] = torch.zeros(len(pos_tags) + 1)\n",
    "        pos_tags_one_hot[''][-1] = 1\n",
    "\n",
    "        # convert the df to list\n",
    "        data = df.values.tolist()\n",
    "        dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "        # pp.pprint(dataset)\n",
    "\n",
    "        max = 0\n",
    "        for i in range(len(dataset)):\n",
    "            if data[i][0] > max:\n",
    "                max = data[i][0]\n",
    "\n",
    "        split_dataset = []\n",
    "        curr = 0\n",
    "        for i in range(1, len(dataset)):\n",
    "            if data[i][0] == 1:\n",
    "                split_dataset.append(dataset[curr:i])\n",
    "                curr = i\n",
    "        split_dataset.append(dataset[curr:])\n",
    "        # print(len(split_dataset))\n",
    "        # print(max)\n",
    "\n",
    "        final_dataset = []\n",
    "        for i in range(len(split_dataset)):\n",
    "            dataset = split_dataset[i]\n",
    "            dataset1 = dataset.copy()\n",
    "            dataset2 = dataset.copy()\n",
    "            for j in range(p):\n",
    "                dataset1 = dataset1[:-1]\n",
    "                dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "                dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "            for j in range(s):\n",
    "                dataset2 = dataset2[1:]\n",
    "                dataset2 = np.append(dataset2, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "                dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "            final_dataset.append(dataset)\n",
    "\n",
    "        # pp.pprint(final_dataset[0][0])\n",
    "        # pp.pprint(final_dataset[0][-1])\n",
    "\n",
    "        dataset = []\n",
    "        for lst in final_dataset:\n",
    "            dataset.extend(lst)\n",
    "        dataset = np.reshape(dataset, (len(dataset), 1, len(dataset[0])))\n",
    "        print(dataset.shape)\n",
    "\n",
    "        final_dataset = []\n",
    "        for i in range(len(dataset)):\n",
    "            final_dataset.append([torch.tensor(dataset[i][0]), pos_tags_one_hot[data[i][2]]])\n",
    "\n",
    "        print(len(final_dataset))\n",
    "        print(len(final_dataset[0]))\n",
    "        print(len(final_dataset[0][0]))\n",
    "        print(len(final_dataset[0][1]))\n",
    "        # print(type(final_dataset[0][0]))\n",
    "        # print(type(final_dataset[0][1]))\n",
    "        # print(final_dataset[22][0])\n",
    "        # print(final_dataset[0][1])\n",
    "\n",
    "        return final_dataset, word_vectors, pos_tags_one_hot\n",
    "\n",
    "    # function to preprocess the dev and test data, using the word vectors and POS tags from the training data\n",
    "    def preprocess_dev_test(self, df, word_vectors, pos_tags_one_hot, p=3, s=3):\n",
    "        data = df.values.tolist()\n",
    "        # dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "        dataset = []\n",
    "        for i in range(len(data)):\n",
    "            if data[i][1] in word_vectors:\n",
    "                dataset.append(word_vectors[data[i][1]])\n",
    "            else:\n",
    "                dataset.append(torch.zeros(len(word_vectors['the'])))\n",
    "        dataset = np.array(dataset, dtype=np.float32)\n",
    "        # pp.pprint(dataset)\n",
    "\n",
    "        max = 0\n",
    "        for i in range(len(dataset)):\n",
    "            if data[i][0] > max:\n",
    "                max = data[i][0]\n",
    "\n",
    "        split_dataset = []\n",
    "        curr = 0\n",
    "        for i in range(1, len(dataset)):\n",
    "            if data[i][0] == 1:\n",
    "                split_dataset.append(dataset[curr:i])\n",
    "                curr = i\n",
    "        split_dataset.append(dataset[curr:])\n",
    "        # print(len(split_dataset))\n",
    "        # print(max)\n",
    "\n",
    "        final_dataset = []\n",
    "        for i in range(len(split_dataset)):\n",
    "            dataset = split_dataset[i]\n",
    "            dataset1 = dataset.copy()\n",
    "            dataset2 = dataset.copy()\n",
    "            for j in range(p):\n",
    "                dataset1 = dataset1[:-1]\n",
    "                dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors['the']))], axis=0)\n",
    "                dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "            for j in range(s):\n",
    "                dataset2 = dataset2[1:]\n",
    "                dataset2 = np.append(dataset2, [np.zeros(len(word_vectors['the']))], axis=0)\n",
    "                dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "            final_dataset.append(dataset)\n",
    "\n",
    "        # pp.pprint(final_dataset[0][0])\n",
    "        # pp.pprint(final_dataset[0][-1])\n",
    "\n",
    "        dataset = []\n",
    "        for lst in final_dataset:\n",
    "            dataset.extend(lst)\n",
    "        dataset = np.reshape(dataset, (len(dataset), 1, len(dataset[0])))\n",
    "        print(dataset.shape)\n",
    "\n",
    "        final_dataset = []\n",
    "        for i in range(len(dataset)):\n",
    "            # final_dataset.append([torch.tensor(dataset[i][0]), pos_tags_one_hot[data[i][2]]])\n",
    "            tensor1 = torch.tensor(dataset[i][0])\n",
    "            try:\n",
    "                tensor2 = pos_tags_one_hot[data[i][2]]\n",
    "            except:\n",
    "                tensor2 = pos_tags_one_hot['']\n",
    "            final_dataset.append([tensor1, tensor2])\n",
    "\n",
    "        print(len(final_dataset))\n",
    "        print(len(final_dataset[0]))\n",
    "        print(len(final_dataset[0][0]))\n",
    "        print(len(final_dataset[0][1]))\n",
    "        # print(type(final_dataset[0][0]))\n",
    "        # print(type(final_dataset[0][1]))\n",
    "        # print(final_dataset[22][0])\n",
    "        # print(final_dataset[0][1])\n",
    "\n",
    "        return final_dataset\n",
    "\n",
    "    class CoNLLUDataset(Dataset):\n",
    "        def __init__(self, data):\n",
    "            self.dataset = data\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # input_tensor = torch.tensor(self.dataset[idx][0], dtype=torch.float32, device=device)\n",
    "            input_tensor = self.dataset[idx][0].to(device)\n",
    "            # target_tensor = torch.tensor(self.dataset[idx][1], dtype=torch.float32, device=device)\n",
    "            target_tensor = self.dataset[idx][1].to(device)\n",
    "            return input_tensor, target_tensor\n",
    "\n",
    "    def train(self, epochs, train_dataloader):\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1}\")\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(tqdm.tqdm(train_dataloader, position=0, leave=True), 0):\n",
    "                inputs, labels = data\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print(f\"Epoch {epoch+1}, loss: {running_loss/len(train_dataloader)}\")\n",
    "\n",
    "    def test(self, test_dataloader):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        total_outputs = []\n",
    "        total_labels = []\n",
    "\n",
    "        print('Test Set Results:\\n')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm.tqdm(test_dataloader, position=0, leave=True):\n",
    "                inputs, labels = data\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, actual = torch.max(labels, 1)\n",
    "                outputs_copy = outputs.clone().detach().cpu().numpy()\n",
    "                outputs_one_hot = np.zeros(outputs_copy.shape)\n",
    "                outputs_one_hot[np.arange(outputs_copy.shape[0]), np.argmax(outputs_copy, axis=1)] = 1\n",
    "\n",
    "                total_outputs.extend(outputs_one_hot)\n",
    "                total_labels.extend(labels.clone().detach().cpu().numpy())\n",
    "\n",
    "        total_outputs = np.array(total_outputs)\n",
    "        total_labels = np.array(total_labels)\n",
    "\n",
    "        print()\n",
    "        print(f\"Loss: {running_loss/len(test_dataloader)}\")\n",
    "        print()\n",
    "        print(f\"Accuracy: {accuracy_score(total_labels, total_outputs)}\")\n",
    "        print(f\"Precision: {precision_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "        print(f\"Recall: {recall_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "        print(f\"F1 Score: {f1_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "\n",
    "        confusion_matrix = np.zeros((len(self.pos_tags_one_hot), len(self.pos_tags_one_hot)))\n",
    "        for i in range(len(total_labels)):\n",
    "            actual = np.argmax(total_labels[i])\n",
    "            predicted = np.argmax(total_outputs[i])\n",
    "            confusion_matrix[actual][predicted] += 1\n",
    "        confusion_matrix2 = confusion_matrix / np.sum(confusion_matrix, axis=1)\n",
    "\n",
    "        plt.imshow(confusion_matrix)\n",
    "        plt.show()\n",
    "        plt.imshow(confusion_matrix2)\n",
    "        plt.show()\n",
    "\n",
    "    def dev(self, dev_dataloader):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        total_outputs = []\n",
    "        total_labels = []\n",
    "\n",
    "        print('Dev Set Results:\\n')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm.tqdm(dev_dataloader, position=0, leave=True):\n",
    "                inputs, labels = data\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, actual = torch.max(labels, 1)\n",
    "                outputs_copy = outputs.clone().detach().cpu().numpy()\n",
    "                outputs_one_hot = np.zeros(outputs_copy.shape)\n",
    "                outputs_one_hot[np.arange(outputs_copy.shape[0]), np.argmax(outputs_copy, axis=1)] = 1\n",
    "\n",
    "                total_outputs.extend(outputs_one_hot)\n",
    "                total_labels.extend(labels.clone().detach().cpu().numpy())\n",
    "\n",
    "        total_outputs = np.array(total_outputs)\n",
    "        total_labels = np.array(total_labels)\n",
    "\n",
    "        print()\n",
    "        print(f\"Loss: {running_loss/len(dev_dataloader)}\")\n",
    "        print()\n",
    "        print(f\"Accuracy: {accuracy_score(total_labels, total_outputs)}\")\n",
    "        print(f\"Precision: {precision_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "        print(f\"Recall: {recall_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "        print(f\"F1 Score: {f1_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "\n",
    "        confusion_matrix = np.zeros((len(self.pos_tags_one_hot), len(self.pos_tags_one_hot)))\n",
    "        for i in range(len(total_labels)):\n",
    "            actual = np.argmax(total_labels[i])\n",
    "            predicted = np.argmax(total_outputs[i])\n",
    "            confusion_matrix[actual][predicted] += 1\n",
    "        confusion_matrix2 = confusion_matrix / np.sum(confusion_matrix, axis=1)\n",
    "\n",
    "        plt.imshow(confusion_matrix)\n",
    "        plt.show()\n",
    "        plt.imshow(confusion_matrix2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in training dataset:  4274\n",
      "Number of sentences in dev dataset:  572\n",
      "Number of sentences in test dataset:  586\n"
     ]
    }
   ],
   "source": [
    "# import conllu from dataset paths\n",
    "dataset_path_train = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-train.conllu'\n",
    "dataset_path_dev = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-dev.conllu'\n",
    "dataset_path_test = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-test.conllu'\n",
    "\n",
    "dataset_train = conllu.parse_incr(open(dataset_path_train))\n",
    "dataset_dev = conllu.parse_incr(open(dataset_path_dev))\n",
    "dataset_test = conllu.parse_incr(open(dataset_path_test))\n",
    "\n",
    "print(\"Number of sentences in training dataset: \", len(list(dataset_train)))\n",
    "print(\"Number of sentences in dev dataset: \", len(list(dataset_dev)))\n",
    "print(\"Number of sentences in test dataset: \", len(list(dataset_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  form lemma  upos  xpos  \\\n",
      "0   1  what  what  PRON  None   \n",
      "1   2    is    be   AUX  None   \n",
      "2   3   the   the   DET  None   \n",
      "3   4  cost  cost  NOUN  None   \n",
      "4   5    of    of   ADP  None   \n",
      "\n",
      "                                               feats  head deprel  deps  misc  \n",
      "0                            {'PronType': 'Int,Rel'}     0   root  None  None  \n",
      "1  {'Mood': 'Ind', 'Number': 'Sing', 'Person': '3...     1    cop  None  None  \n",
      "2                                {'PronType': 'Art'}     4    det  None  None  \n",
      "3                                 {'Number': 'Sing'}     1  nsubj  None  None  \n",
      "4                                               None     7   case  None  None  \n"
     ]
    }
   ],
   "source": [
    "# bring the pointer back to the beginning of the file\n",
    "dataset_train = conllu.parse_incr(open(dataset_path_train))\n",
    "dataset_dev = conllu.parse_incr(open(dataset_path_dev))\n",
    "dataset_test = conllu.parse_incr(open(dataset_path_test))\n",
    "\n",
    "# convert this data to a pandas dataframe\n",
    "def conllu_to_pandas(dataset):\n",
    "    data = []\n",
    "    for tokenlist in dataset:\n",
    "        for token in tokenlist:\n",
    "            data.append(token)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_train = conllu_to_pandas(dataset_train)\n",
    "df_dev = conllu_to_pandas(dataset_dev)\n",
    "df_test = conllu_to_pandas(dataset_test)\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the training set:  863\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary of the words in the training set\n",
    "vocab = df_train['form'].unique()\n",
    "print(\"Number of unique words in the training set: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the training set after adding <s>, </s>, <unk>:  866\n"
     ]
    }
   ],
   "source": [
    "# add <s>, </s> and <unk> to the vocabulary\n",
    "vocab = ['<s>', '</s>', '<unk>'] + list(vocab)\n",
    "print(\"Number of unique words in the training set after adding <s>, </s>, <unk>: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique upos in the training set:  13\n",
      "['PRON' 'AUX' 'DET' 'NOUN' 'ADP' 'PROPN' 'VERB' 'NUM' 'ADJ' 'CCONJ' 'ADV'\n",
      " 'PART' 'INTJ']\n"
     ]
    }
   ],
   "source": [
    "# finding all unique pos tags\n",
    "upos = df_train['upos'].unique()\n",
    "print(\"Number of unique upos in the training set: \", len(upos))\n",
    "print(upos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique upos in the training set after adding STRT, END, UNK:  16\n"
     ]
    }
   ],
   "source": [
    "# adding pos tags for <s>, </s>, <unk> to the upos\n",
    "upos = ['STRT', 'END', 'UNK'] + list(upos)\n",
    "\n",
    "print(\"Number of unique upos in the training set after adding STRT, END, UNK: \", len(upos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s>': 0, '</s>': 1, '<unk>': 2, 'what': 3, 'is': 4, 'the': 5, 'cost': 6, 'of': 7, 'a': 8, 'round': 9, 'trip': 10, 'flight': 11, 'from': 12, 'pittsburgh': 13, 'to': 14, 'atlanta': 15, 'beginning': 16, 'on': 17, 'april': 18, 'twenty': 19, 'fifth': 20, 'and': 21, 'returning': 22, 'may': 23, 'sixth': 24, 'now': 25, 'i': 26, 'need': 27, 'leaving': 28, 'fort': 29, 'worth': 30, 'arriving': 31, 'in': 32, 'denver': 33, 'no': 34, 'later': 35, 'than': 36, '2': 37, 'pm': 38, 'next': 39, 'monday': 40, 'fly': 41, 'kansas': 42, 'city': 43, 'chicago': 44, 'wednesday': 45, 'following': 46, 'day': 47, 'meaning': 48, 'meal': 49, 'code': 50, 's': 51, 'show': 52, 'me': 53, 'all': 54, 'flights': 55, 'which': 56, 'serve': 57, 'for': 58, 'after': 59, 'tomorrow': 60, 'us': 61, 'air': 62, 'list': 63, 'nonstop': 64, 'early': 65, 'tuesday': 66, 'morning': 67, 'dallas': 68, 'st.': 69, 'petersburg': 70, 'toronto': 71, 'that': 72, 'arrive': 73, 'listing': 74, 'new': 75, 'york': 76, 'montreal': 77, 'canada': 78, 'departing': 79, 'thursday': 80, 'american': 81, 'airlines': 82, 'ontario': 83, 'with': 84, 'stopover': 85, 'louis': 86, 'ground': 87, 'transportation': 88, 'houston': 89, 'afternoon': 90, 'schedule': 91, 'philadelphia': 92, 'san': 93, 'francisco': 94, 'evening': 95, 'diego': 96, 'layover': 97, 'washington': 98, 'dc': 99, 'are': 100, 'there': 101, 'any': 102, 'boston': 103, 'stop': 104, 'restrictions': 105, 'cheapest': 106, 'one': 107, 'way': 108, 'fare': 109, 'between': 110, 'oakland': 111, 'airfare': 112, '416': 113, 'dollars': 114, \"'s\": 115, 'restriction': 116, 'ap68': 117, 'california': 118, 'airports': 119, 'available': 120, 'texas': 121, 'airport': 122, 'closest': 123, 'nevada': 124, 'arizona': 125, 'actually': 126, 'las': 127, 'vegas': 128, 'burbank': 129, 'saturday': 130, 'two': 131, 'how': 132, 'many': 133, 'going': 134, 'july': 135, 'seventh': 136, 'would': 137, 'like': 138, 'an': 139, 'february': 140, 'eighth': 141, 'before': 142, '9': 143, 'am': 144, 'second': 145, 'late': 146, 'okay': 147, 'june': 148, 'first': 149, \"'d\": 150, 'go': 151, 'phoenix': 152, 'detroit': 153, 'milwaukee': 154, 'indianapolis': 155, 'does': 156, 'ls': 157, 'stand': 158, 'designate': 159, 'as': 160, 'baltimore': 161, '1115': 162, '1245': 163, 'miami': 164, 'daily': 165, '8': 166, 'trans': 167, 'world': 168, 'airline': 169, '1030': 170, '1130': 171, '5': 172, '730': 173, 'leave': 174, 'charlotte': 175, 'north': 176, 'carolina': 177, '4': 178, 'find': 179, 'newark': 180, 'jersey': 181, 'cleveland': 182, 'ohio': 183, 'do': 184, 'you': 185, 'have': 186, 'connect': 187, 'international': 188, 'minneapolis': 189, 'rental': 190, 'cars': 191, \"'ll\": 192, 'rent': 193, 'car': 194, 'sort': 195, 'near': 196, 'fine': 197, 'can': 198, 'give': 199, 'information': 200, 'downtown': 201, 'economy': 202, 'class': 203, 'fares': 204, 'december': 205, 'sixteenth': 206, 'codes': 207, 'belong': 208, 'coach': 209, 'night': 210, 'service': 211, 'november': 212, 'twelfth': 213, 'eleventh': 214, 'want': 215, 'know': 216, 'or': 217, '1': 218, \"o'clock\": 219, '3': 220, '6': 221, '10': 222, 'august': 223, 'display': 224, 'depart': 225, 'please': 226, 'snacks': 227, 'served': 228, 'tower': 229, 'types': 230, 'meals': 231, 'ever': 232, 'my': 233, 'options': 234, '382': 235, 'get': 236, 'bwi': 237, 'eastern': 238, '210': 239, 'delta': 240, '852': 241, 'latest': 242, 'return': 243, 'same': 244, 'back': 245, 'most': 246, 'hours': 247, 'take': 248, 'so': 249, 'when': 250, 'will': 251, 'maximum': 252, 'amount': 253, 'time': 254, 'still': 255, 'earliest': 256, 'departure': 257, 'be': 258, 'travel': 259, 'at': 260, 'around': 261, '7': 262, 'route': 263, 'lastest': 264, 'longest': 265, 'but': 266, 'possible': 267, 'only': 268, 'weekdays': 269, 'red': 270, 'eye': 271, 'los': 272, 'angeles': 273, 'ten': 274, 'people': 275, 'during': 276, 'week': 277, 'days': 278, 'out': 279, 'arrives': 280, 'salt': 281, 'lake': 282, 'cincinnati': 283, 'area': 284, 'explain': 285, 'ap': 286, '57': 287, '20': 288, 'mean': 289, '80': 290, 'twa': 291, '497766': 292, 'has': 293, 'stops': 294, 'friday': 295, '705': 296, 'number': 297, 'book': 298, 'least': 299, '813': 300, 'goes': 301, 'straight': 302, 'through': 303, 'without': 304, 'stopping': 305, 'another': 306, 'florida': 307, 'tell': 308, 'about': 309, 'by': 310, 'memphis': 311, 'tennessee': 312, 'noon': 313, '530': 314, 'off': 315, 'love': 316, 'field': 317, 'united': 318, 'la': 319, 'guardia': 320, 'jfk': 321, 'mco': 322, 'sfo': 323, '1991': 324, 'orlando': 325, 'lowest': 326, 'dfw': 327, 'ticket': 328, 'oak': 329, 'atl': 330, 'logan': 331, 'march': 332, 'numbers': 333, 'expensive': 334, 'continental': 335, 'leaves': 336, '1220': 337, 'seattle': 338, 'columbus': 339, 'minnesota': 340, 'those': 341, 'via': 342, 'rentals': 343, 'sunday': 344, 'rates': 345, 'costs': 346, 'limousine': 347, 'taxi': 348, 'operation': 349, 'ap80': 350, 'ap57': 351, 'ninth': 352, '12': 353, 'america': 354, 'west': 355, 'could': 356, 'fifteenth': 357, 'serves': 358, 'dinner': 359, 'provided': 360, 'cities': 361, 'where': 362, 'lester': 363, 'pearson': 364, 'canadian': 365, 'other': 366, 'earlier': 367, '1017': 368, 'northwest': 369, 'general': 370, 'mitchell': 371, 'located': 372, 'both': 373, 'nationair': 374, 'midwest': 375, 'express': 376, 'flies': 377, 'zone': 378, 'flying': 379, 'into': 380, 'much': 381, 'price': 382, 'it': 383, 'tacoma': 384, 'anywhere': 385, '1850': 386, 'midnight': 387, 'january': 388, '1992': 389, 'not': 390, 'exceeding': 391, '300': 392, 'tenth': 393, '1993': 394, '1505': 395, 'october': 396, '1994': 397, 'carries': 398, 'smallest': 399, 'passengers': 400, 'thirty': 401, 'third': 402, 'arrival': 403, 'schedules': 404, 'times': 405, 'your': 406, '269': 407, '428': 408, 'westchester': 409, 'county': 410, 'right': 411, 'september': 412, 'twentieth': 413, 'f28': 414, '755': 415, 'nights': 416, 'their': 417, 'prices': 418, '1039': 419, 'less': 420, '1100': 421, 'nashville': 422, 'again': 423, 'repeat': 424, 'make': 425, 'iah': 426, 'ord': 427, 'ewr': 428, 'dca': 429, 'cvg': 430, 'bna': 431, 'mci': 432, 'hou': 433, 'lga': 434, 'lax': 435, 'yyz': 436, 'bur': 437, 'long': 438, 'distance': 439, 'far': 440, 'paul': 441, 'miles': 442, 'name': 443, 'serviced': 444, 'regarding': 445, 'tampa': 446, 'names': 447, 'describe': 448, 'nineteenth': 449, 'seating': 450, 'capacity': 451, 'fourteenth': 452, 'aircraft': 453, 'largest': 454, 'plane': 455, 'eight': 456, 'sixteen': 457, 'departures': 458, 'seventeenth': 459, 'arrivals': 460, 'type': 461, 'greatest': 462, 'more': 463, 'business': 464, 'total': 465, 'instead': 466, 'besides': 467, 'turboprop': 468, '1059': 469, 'advertises': 470, 'having': 471, 'land': 472, 'various': 473, 'dulles': 474, 'boeing': 475, '767': 476, '466': 477, '329': 478, 'under': 479, '932': 480, '1000': 481, '200': 482, '124': 483, 'along': 484, 'equal': 485, '150': 486, 'each': 487, '400': 488, 'fit': 489, '72s': 490, 'airplane': 491, 'l1011': 492, 'hold': 493, '733': 494, 'airplanes': 495, 'uses': 496, '73s': 497, 'seats': 498, '734': 499, 'm80': 500, 'l10': 501, 'carried': 502, 'capacities': 503, '757': 504, 'planes': 505, 'd9s': 506, '100': 507, 'thrift': 508, 'level': 509, 'see': 510, 'thirtieth': 511, '505': 512, '163': 513, 'tonight': 514, 'connecting': 515, 'also': 516, 'making': 517, \"'m\": 518, 'looking': 519, 'hopefully': 520, 'makes': 521, 'yes': 522, 'breakfast': 523, 'direct': 524, 'itinerary': 525, 'departs': 526, '1940': 527, 'connects': 528, 'provide': 529, 'used': 530, 'including': 531, 'connections': 532, 'if': 533, 'either': 534, 'preferably': 535, 'local': 536, 'beach': 537, 'then': 538, 'mornings': 539, 'four': 540, 'combination': 541, 'thank': 542, 'using': 543, 'well': 544, 'run': 545, 'colorado': 546, 'fourth': 547, 'who': 548, 'sure': 549, 'determine': 550, 'use': 551, '1765': 552, 'lufthansa': 553, 'eighteenth': 554, 'f': 555, 'today': 556, 'come': 557, '320': 558, 'booking': 559, 'k': 560, 'classes': 561, 'yn': 562, 'j31': 563, 'different': 564, 'dh8': 565, 'minimum': 566, 'connection': 567, 'intercontinental': 568, 'last': 569, 'aa': 570, '459': 571, 'limousines': 572, 'services': 573, 'jose': 574, 'too': 575, 'train': 576, 'stapleton': 577, 'limo': 578, 'georgia': 579, 'pennsylvania': 580, 'utah': 581, 'missouri': 582, 'interested': 583, 'shortest': 584, 'quebec': 585, 'michigan': 586, 'indiana': 587, 'this': 588, 'wednesdays': 589, '82': 590, '139': 591, 'tickets': 592, 'sounds': 593, 'great': 594, 'let': 595, 'takeoffs': 596, 'landings': 597, 'grounds': 598, 'offer': 599, 'transport': 600, 'kind': 601, 'hi': 602, 'calling': 603, 'coming': 604, 'soon': 605, 'thereafter': 606, 'anything': 607, 'bring': 608, 'up': 609, 'y': 610, 'm': 611, 'difference': 612, 'q': 613, 'b': 614, 'qo': 615, 'qw': 616, 'qx': 617, 'fn': 618, 'qualify': 619, 'h': 620, 'basis': 621, 'bh': 622, 'offers': 623, 'included': 624, 'serving': 625, 'trying': 626, 'include': 627, 'whether': 628, 'offered': 629, 'ua': 630, '270': 631, 'being': 632, '747': 633, 'be1': 634, '737': 635, 'very': 636, 'working': 637, 'scenario': 638, 'three': 639, '727': 640, 'called': 641, 'dc10': 642, 'abbreviation': 643, 'd10': 644, 'includes': 645, '296': 646, 'should': 647, 'lunch': 648, '343': 649, 'travels': 650, 'snack': 651, 'supper': 652, '838': 653, '1110': 654, 'reaches': 655, 'sometime': 656, 'some': 657, 'reaching': 658, 'saturdays': 659, 'vicinity': 660, 'good': 661, '1800': 662, 'overnight': 663, 'final': 664, 'destination': 665, 'over': 666, 'summer': 667, '297': 668, '1222': 669, '281': 670, 'listed': 671, 'dl': 672, '1055': 673, '405': 674, '201': 675, '315': 676, '21': 677, '486': 678, '825': 679, '555': 680, '1207': 681, '1500': 682, '639': 683, '217': 684, '71': 685, '106': 686, '539': 687, '3724': 688, '271': 689, '1291': 690, '4400': 691, '3357': 692, '345': 693, '771': 694, 'co': 695, '1209': 696, 'ea': 697, '212': 698, '257': 699, '608': 700, '746': 701, 'taking': 702, '311': 703, '417': 704, 'try': 705, 'inform': 706, 'kinds': 707, 'traveling': 708, '419': 709, 'they': 710, 'these': 711, 'kindly': 712, 'proper': 713, 'town': 714, 'takeoff': 715, 'kennedy': 716, 'close': 717, '230': 718, 'nonstops': 719, 'thursdays': 720, \"'re\": 721, '1230': 722, '1200': 723, 'within': 724, 'reservation': 725, 'friends': 726, 'visit': 727, 'here': 728, 'them': 729, 'lives': 730, '0900': 731, '1600': 732, '11': 733, 'we': 734, 'nighttime': 735, 'southwest': 736, 'usa': 737, 'able': 738, 'put': 739, '630': 740, 'nw': 741, 'hp': 742, 'define': 743, 'ff': 744, 'symbols': 745, 'stands': 746, 'kw': 747, 'sam': 748, 'ac': 749, '718': 750, 'wn': 751, 'arrangements': 752, 'sorry': 753, 'must': 754, 'originating': 755, '225': 756, '1158': 757, 'equipment': 758, 'choices': 759, '1205': 760, '1145': 761, 'abbreviations': 762, 'jet': 763, 'companies': 764, 'continuing': 765, 'represented': 766, 'database': 767, 'single': 768, 'rate': 769, 'trips': 770, 'stopovers': 771, 'directly': 772, 'starting': 773, 'afterwards': 774, 'reservations': 775, 'scheduled': 776, 'seat': 777, 'india': 778, 'buy': 779, 'six': 780, '1700': 781, 'say': 782, 'mealtime': 783, '2100': 784, 'economic': 785, 'wish': 786, 'discount': 787, 'staying': 788, 'while': 789, 'look': 790, 'across': 791, 'continent': 792, 'transcontinental': 793, 'begins': 794, 'lands': 795, 'landing': 796, 'month': 797, 'help': 798, '720': 799, '110': 800, 'such': 801, '1045': 802, '934': 803, 'heading': 804, 'toward': 805, '430': 806, 'approximately': 807, '324': 808, '1300': 809, '723': 810, '1020': 811, '645': 812, 'weekday': 813, 'inexpensive': 814, 'thing': 815, 'cheap': 816, 'thanks': 817, 'question': 818, 'live': 819, 'spend': 820, 'seventeen': 821, 'highest': 822, 'priced': 823, 'charges': 824, 'dinnertime': 825, '305': 826, '845': 827, 'noontime': 828, '1026': 829, '823': 830, '2134': 831, '1024': 832, '130': 833, \"'ve\": 834, 'got': 835, 'somebody': 836, 'else': 837, 'wants': 838, '420': 839, 'seven': 840, 'catch': 841, 'fifteen': 842, 'thirteenth': 843, 'tuesdays': 844, 'midway': 845, 'alaska': 846, 'arrange': 847, 'plan': 848, 'oh': 849, 'hello': 850, \"n't\": 851, 'prefer': 852, 'requesting': 853, 'comes': 854, 'mondays': 855, 'bound': 856, 'fridays': 857, 'sundays': 858, 'bay': 859, 'planning': 860, 'home': 861, 'reverse': 862, 'order': 863, 'a.m.': 864, 'philly': 865}\n",
      "Embedding(866, 100)\n",
      "4274\n",
      "[3, 25, 26, 3, 52, 52, 63, 52, 26, 52, 25, 52, 52, 26, 100, 26, 3, 3, 3, 63, 3, 56, 63, 126, 132, 26, 52, 4, 147, 26, 26, 3, 3, 3, 52, 63, 63, 63, 63, 52, 63, 26, 3, 26, 55, 3, 55, 52, 55, 190, 26, 3, 52, 52, 197, 52, 3, 198, 132, 3, 3, 54, 54, 26, 54, 63, 224, 226, 63, 3, 226, 55, 100, 3, 100, 3, 3, 3, 184, 52, 4, 179, 132, 3, 179, 179, 26, 179, 26, 63, 3, 3, 209, 26, 26, 26, 3, 3, 3, 3, 3, 3, 226, 179, 54, 52, 285, 3, 3, 3, 3, 285, 3, 3, 3, 4, 3, 4, 63, 26, 26, 92, 4, 52, 52, 26, 63, 3, 52, 26, 45, 308, 52, 52, 199, 100, 63, 52, 308, 3, 52, 55, 55, 3, 56, 52, 323, 52, 3, 3, 3, 132, 3, 26, 26, 3, 11, 26, 3, 11, 226, 23, 11, 11, 11, 11, 26, 63, 11, 3, 3, 26, 198, 3, 190, 3, 3, 3, 3, 4, 198, 4, 198, 52, 3, 52, 285, 285, 3, 3, 63, 52, 5, 56, 55, 52, 4, 3, 63, 52, 55, 3, 52, 52, 3, 52, 3, 54, 3, 356, 3, 52, 3, 3, 156, 3, 63, 3, 362, 3, 52, 100, 3, 362, 362, 56, 52, 52, 63, 3, 3, 3, 52, 3, 52, 52, 52, 100, 179, 3, 3, 132, 132, 132, 132, 3, 132, 132, 132, 132, 3, 3, 52, 55, 226, 4, 52, 3, 52, 3, 56, 3, 3, 3, 52, 52, 3, 3, 3, 52, 52, 11, 63, 226, 17, 26, 3, 226, 61, 226, 3, 3, 52, 54, 226, 3, 3, 52, 63, 226, 52, 226, 55, 226, 26, 63, 54, 52, 3, 3, 52, 52, 54, 54, 26, 63, 63, 26, 63, 149, 423, 52, 199, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 226, 3, 3, 3, 3, 226, 3, 3, 132, 3, 132, 132, 132, 63, 3, 3, 132, 63, 132, 3, 132, 132, 132, 132, 63, 132, 132, 132, 63, 3, 132, 3, 63, 156, 226, 52, 63, 63, 308, 119, 226, 63, 308, 3, 3, 447, 3, 3, 26, 448, 56, 52, 3, 3, 226, 3, 56, 3, 3, 56, 3, 56, 199, 3, 3, 466, 7, 3, 3, 3, 4, 52, 3, 3, 52, 52, 52, 26, 52, 52, 3, 52, 226, 52, 63, 63, 63, 26, 132, 226, 3, 226, 63, 52, 52, 21, 52, 63, 56, 52, 9, 52, 52, 52, 9, 9, 52, 55, 9, 63, 9, 63, 4, 226, 52, 54, 9, 9, 55, 199, 52, 9, 132, 132, 132, 3, 3, 132, 132, 3, 3, 3, 3, 3, 3, 132, 3, 63, 3, 63, 3, 132, 3, 3, 3, 3, 3, 3, 132, 132, 54, 9, 52, 3, 100, 3, 198, 198, 198, 52, 52, 3, 3, 3, 26, 3, 52, 52, 132, 147, 52, 26, 52, 127, 26, 13, 3, 26, 4, 147, 26, 26, 100, 179, 26, 56, 26, 4, 52, 26, 26, 522, 3, 26, 26, 26, 179, 26, 26, 522, 26, 26, 4, 26, 26, 52, 26, 100, 4, 3, 4, 100, 52, 4, 52, 52, 52, 52, 52, 4, 4, 56, 26, 100, 52, 52, 226, 26, 52, 52, 26, 4, 100, 63, 63, 52, 26, 52, 132, 538, 26, 26, 26, 52, 26, 17, 100, 26, 26, 52, 147, 26, 199, 26, 26, 26, 26, 63, 3, 26, 26, 147, 26, 147, 52, 63, 199, 17, 199, 26, 26, 3, 542, 3, 55, 63, 55, 63, 63, 56, 132, 26, 55, 3, 63, 63, 63, 63, 63, 55, 63, 55, 63, 356, 63, 63, 63, 180, 63, 63, 63, 165, 52, 63, 63, 52, 26, 4, 63, 26, 3, 52, 63, 63, 3, 3, 100, 226, 100, 549, 26, 226, 3, 100, 3, 3, 63, 147, 226, 26, 63, 63, 3, 63, 3, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 226, 132, 132, 63, 132, 132, 63, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 25, 3, 3, 26, 3, 3, 3, 52, 3, 3, 3, 3, 3, 226, 3, 198, 26, 3, 3, 3, 52, 26, 3, 52, 3, 3, 100, 198, 576, 3, 26, 198, 26, 198, 3, 32, 26, 4, 26, 26, 54, 63, 26, 584, 26, 52, 226, 226, 26, 26, 226, 63, 3, 63, 200, 26, 26, 63, 63, 26, 26, 52, 226, 3, 26, 3, 17, 63, 26, 3, 199, 3, 3, 26, 52, 56, 3, 3, 3, 3, 26, 3, 3, 3, 3, 26, 63, 5, 3, 3, 52, 3, 3, 26, 3, 3, 52, 63, 3, 3, 3, 3, 3, 3, 147, 52, 3, 52, 52, 26, 52, 52, 63, 3, 3, 63, 226, 3, 3, 63, 226, 63, 4, 156, 52, 4, 156, 4, 4, 52, 3, 3, 132, 3, 3, 308, 3, 4, 52, 87, 3, 52, 4, 52, 4, 26, 3, 3, 87, 87, 3, 226, 87, 26, 3, 32, 52, 52, 308, 87, 260, 52, 3, 26, 602, 52, 52, 52, 52, 55, 26, 26, 3, 56, 32, 63, 52, 3, 52, 52, 52, 52, 52, 199, 52, 26, 26, 26, 52, 52, 26, 607, 236, 608, 3, 26, 109, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 63, 3, 3, 3, 3, 3, 3, 3, 3, 3, 63, 3, 3, 3, 3, 3, 3, 3, 3, 3, 226, 3, 3, 3, 285, 3, 3, 285, 448, 52, 26, 199, 26, 226, 3, 52, 63, 3, 199, 26, 52, 3, 156, 3, 199, 63, 52, 52, 5, 179, 226, 52, 32, 17, 3, 199, 5, 63, 3, 199, 179, 26, 184, 63, 52, 308, 636, 356, 100, 3, 184, 3, 26, 26, 308, 3, 156, 156, 100, 142, 3, 52, 226, 63, 3, 26, 356, 179, 26, 3, 3, 4, 3, 17, 156, 198, 26, 52, 3, 179, 52, 179, 26, 3, 3, 26, 3, 3, 3, 3, 52, 226, 26, 52, 156, 26, 3, 226, 52, 3, 26, 3, 52, 26, 25, 569, 56, 26, 26, 26, 52, 52, 26, 26, 26, 25, 52, 26, 226, 198, 56, 63, 3, 26, 52, 26, 26, 26, 226, 63, 26, 52, 56, 4, 356, 52, 92, 26, 56, 56, 63, 26, 63, 179, 4, 11, 63, 226, 3, 63, 63, 55, 26, 54, 52, 3, 132, 26, 55, 3, 3, 52, 26, 63, 63, 63, 63, 26, 179, 3, 198, 3, 26, 52, 11, 156, 26, 156, 26, 3, 26, 519, 3, 519, 3, 12, 132, 4, 256, 3, 26, 63, 3, 26, 26, 52, 26, 3, 132, 3, 3, 156, 52, 226, 3, 3, 52, 17, 52, 52, 17, 3, 132, 3, 4, 3, 308, 52, 61, 3, 3, 26, 3, 3, 81, 52, 226, 198, 61, 238, 61, 3, 672, 52, 3, 61, 3, 17, 3, 362, 52, 132, 198, 132, 11, 132, 3, 3, 3, 28, 3, 52, 3, 63, 544, 3, 147, 3, 3, 52, 63, 3, 3, 226, 3, 3, 3, 226, 3, 3, 26, 26, 224, 3, 3, 3, 550, 63, 26, 3, 3, 3, 3, 4, 52, 54, 26, 712, 26, 26, 3, 3, 3, 3, 63, 3, 308, 3, 63, 52, 308, 132, 52, 4, 63, 63, 4, 4, 52, 52, 132, 356, 3, 184, 3, 3, 3, 21, 132, 52, 52, 4, 132, 26, 3, 52, 4, 226, 26, 63, 63, 3, 4, 52, 63, 4, 132, 3, 63, 52, 63, 52, 198, 26, 3, 199, 26, 26, 52, 25, 52, 26, 11, 3, 52, 26, 63, 26, 3, 56, 3, 52, 3, 26, 3, 4, 26, 226, 52, 52, 26, 52, 199, 26, 198, 226, 52, 26, 3, 3, 226, 3, 3, 199, 52, 3, 26, 26, 26, 3, 3, 4, 199, 52, 63, 52, 55, 63, 56, 199, 63, 63, 26, 3, 199, 63, 26, 52, 3, 199, 100, 63, 63, 26, 26, 5, 3, 3, 199, 3, 63, 63, 63, 63, 52, 3, 3, 63, 3, 52, 11, 63, 3, 52, 147, 26, 236, 100, 52, 8, 8, 52, 198, 92, 226, 39, 56, 3, 61, 26, 52, 26, 3, 199, 3, 3, 39, 226, 179, 26, 25, 54, 52, 56, 56, 52, 26, 226, 26, 3, 308, 198, 147, 226, 226, 52, 199, 52, 542, 26, 26, 184, 184, 26, 8, 199, 52, 56, 3, 26, 198, 52, 26, 56, 3, 52, 63, 52, 199, 3, 55, 3, 52, 156, 56, 52, 26, 26, 199, 184, 156, 54, 55, 26, 55, 52, 52, 54, 56, 52, 52, 52, 200, 3, 52, 199, 3, 26, 63, 3, 12, 3, 52, 3, 52, 52, 52, 54, 52, 56, 54, 26, 63, 63, 52, 52, 54, 4, 52, 52, 52, 226, 132, 26, 52, 55, 26, 26, 17, 23, 26, 26, 3, 3, 55, 26, 26, 23, 11, 17, 26, 226, 3, 55, 3, 11, 63, 52, 602, 3, 23, 3, 56, 23, 52, 26, 52, 11, 156, 3, 147, 3, 56, 26, 26, 63, 226, 3, 224, 3, 155, 3, 26, 3, 3, 17, 11, 26, 226, 3, 52, 224, 106, 199, 26, 3, 26, 3, 17, 52, 15, 56, 4, 3, 52, 199, 52, 3, 3, 17, 26, 137, 23, 308, 199, 3, 52, 740, 106, 308, 63, 3, 224, 184, 356, 132, 3, 26, 56, 17, 199, 12, 3, 4, 106, 226, 26, 3, 3, 3, 743, 3, 3, 3, 3, 3, 285, 3, 3, 52, 3, 3, 156, 3, 356, 3, 3, 3, 3, 26, 26, 3, 3, 3, 52, 26, 52, 4, 56, 3, 52, 52, 52, 56, 3, 209, 3, 3, 3, 3, 169, 3, 226, 26, 26, 3, 156, 226, 132, 3, 3, 26, 56, 26, 3, 3, 52, 3, 3, 3, 743, 3, 3, 4, 4, 55, 3, 3, 26, 26, 26, 179, 3, 52, 3, 179, 63, 26, 100, 3, 3, 3, 11, 52, 17, 147, 4, 52, 226, 179, 56, 26, 63, 52, 3, 179, 3, 26, 26, 179, 522, 26, 3, 3, 56, 199, 3, 26, 179, 3, 26, 26, 52, 26, 3, 52, 3, 25, 224, 26, 63, 26, 52, 56, 26, 17, 3, 52, 3, 56, 26, 26, 26, 26, 52, 26, 52, 226, 52, 226, 156, 56, 3, 26, 56, 56, 52, 3, 56, 3, 226, 56, 3, 56, 63, 184, 3, 3, 356, 226, 3, 52, 63, 156, 3, 56, 56, 56, 56, 3, 52, 52, 3, 56, 226, 56, 3, 3, 63, 56, 3, 198, 226, 226, 226, 3, 63, 52, 3, 52, 3, 52, 156, 3, 4, 82, 3, 56, 52, 226, 56, 52, 56, 3, 199, 198, 3, 3, 52, 3, 226, 3, 4, 56, 198, 4, 52, 308, 56, 63, 52, 3, 56, 3, 52, 56, 3, 56, 52, 199, 226, 63, 52, 52, 198, 26, 179, 52, 52, 64, 4, 9, 3, 179, 52, 226, 26, 52, 63, 63, 52, 199, 52, 52, 26, 52, 52, 100, 52, 26, 26, 52, 26, 64, 3, 199, 64, 52, 3, 198, 26, 132, 52, 4, 52, 147, 52, 63, 63, 52, 63, 199, 52, 23, 139, 3, 52, 52, 149, 3, 26, 52, 52, 52, 132, 200, 132, 156, 147, 52, 52, 52, 3, 26, 132, 52, 3, 226, 3, 3, 52, 52, 3, 107, 226, 63, 26, 356, 3, 3, 54, 56, 52, 132, 3, 3, 52, 3, 52, 52, 3, 26, 3, 52, 52, 52, 52, 132, 226, 3, 3, 132, 26, 3, 52, 26, 52, 3, 226, 149, 3, 3, 52, 189, 52, 200, 3, 3, 26, 54, 63, 52, 9, 132, 74, 26, 52, 26, 52, 52, 226, 52, 3, 52, 26, 52, 26, 26, 3, 179, 26, 52, 52, 52, 26, 26, 52, 52, 308, 3, 3, 3, 3, 149, 3, 3, 52, 3, 3, 56, 3, 3, 54, 199, 26, 132, 3, 63, 87, 87, 198, 3, 3, 3, 156, 26, 3, 87, 52, 3, 52, 26, 52, 3, 32, 63, 3, 156, 308, 52, 226, 26, 63, 3, 52, 3, 3, 87, 3, 52, 52, 156, 52, 3, 3, 3, 3, 87, 68, 4, 15, 156, 52, 4, 52, 3, 87, 156, 52, 87, 4, 3, 789, 32, 87, 32, 226, 52, 3, 226, 26, 3, 87, 52, 4, 26, 3, 87, 26, 4, 132, 156, 87, 3, 200, 52, 3, 87, 448, 3, 3, 3, 87, 3, 52, 3, 103, 3, 3, 52, 63, 26, 26, 100, 198, 56, 4, 56, 3, 54, 602, 26, 26, 63, 26, 56, 26, 26, 26, 26, 5, 63, 198, 184, 318, 26, 184, 26, 26, 26, 3, 26, 26, 54, 3, 3, 179, 56, 52, 184, 56, 56, 100, 52, 52, 4, 179, 3, 179, 3, 26, 26, 26, 26, 369, 184, 52, 32, 100, 56, 3, 224, 26, 184, 224, 318, 26, 56, 4, 52, 52, 26, 179, 3, 26, 26, 54, 26, 56, 3, 179, 522, 26, 26, 26, 52, 93, 52, 26, 3, 26, 132, 26, 63, 52, 26, 3, 156, 318, 63, 52, 184, 3, 4, 356, 52, 179, 52, 26, 26, 3, 26, 100, 226, 179, 4, 3, 3, 198, 198, 4, 132, 26, 4, 3, 26, 200, 87, 87, 3, 198, 3, 3, 52, 179, 63, 52, 26, 3, 26, 26, 17, 4, 54, 226, 52, 26, 3, 100, 52, 17, 226, 179, 26, 3, 59, 21, 52, 3, 26, 179, 63, 63, 63, 26, 52, 26, 52, 26, 12, 54, 602, 198, 226, 3, 52, 26, 56, 52, 55, 17, 226, 63, 224, 63, 26, 26, 3, 54, 26, 52, 100, 174, 52, 63, 17, 52, 3, 3, 226, 69, 147, 3, 63, 26, 52, 55, 179, 3, 26, 3, 26, 3, 52, 184, 3, 55, 4, 308, 26, 542, 3, 52, 52, 52, 26, 226, 198, 52, 226, 54, 3, 184, 52, 26, 26, 3, 3, 356, 17, 3, 52, 179, 238, 602, 26, 26, 26, 3, 63, 3, 15, 339, 100, 52, 26, 3, 3, 63, 199, 26, 52, 63, 26, 100, 184, 3, 52, 52, 52, 63, 54, 52, 63, 4, 226, 26, 26, 63, 4, 4, 63, 3, 63, 63, 26, 52, 3, 3, 226, 3, 3, 3, 52, 52, 63, 26, 52, 3, 179, 3, 52, 3, 3, 52, 3, 3, 26, 3, 3, 63, 3, 52, 3, 242, 147, 3, 52, 179, 3, 26, 3, 52, 27, 3, 149, 139, 3, 63, 52, 3, 3, 3, 63, 3, 236, 584, 3, 3, 198, 3, 3, 226, 26, 3, 26, 52, 356, 26, 356, 3, 3, 52, 52, 3, 3, 26, 236, 3, 26, 3, 3, 26, 52, 3, 3, 3, 3, 3, 3, 3, 356, 198, 3, 308, 3, 63, 3, 52, 3, 3, 26, 26, 3, 3, 26, 198, 3, 817, 516, 3, 26, 3, 3, 3, 26, 28, 3, 3, 26, 3, 3, 198, 3, 3, 3, 3, 26, 52, 3, 443, 3, 233, 3, 250, 147, 58, 52, 3, 63, 26, 26, 3, 26, 3, 179, 3, 3, 3, 3, 256, 226, 584, 26, 3, 3, 3, 250, 250, 3, 52, 813, 52, 584, 63, 26, 3, 106, 3, 63, 3, 52, 3, 3, 26, 3, 52, 52, 52, 3, 26, 3, 26, 106, 3, 3, 63, 179, 3, 26, 379, 26, 3, 3, 3, 26, 26, 3, 52, 3, 3, 26, 3, 3, 26, 326, 52, 52, 26, 26, 179, 356, 26, 3, 198, 106, 106, 52, 63, 179, 132, 3, 199, 326, 26, 199, 3, 3, 3, 52, 52, 3, 26, 3, 26, 356, 3, 3, 26, 52, 3, 3, 3, 52, 3, 26, 3, 17, 3, 199, 52, 3, 226, 52, 26, 3, 52, 106, 3, 3, 3, 26, 3, 3, 179, 356, 179, 3, 26, 52, 226, 3, 52, 3, 3, 226, 3, 54, 198, 52, 26, 3, 3, 3, 3, 3, 52, 106, 198, 3, 137, 3, 3, 3, 52, 3, 3, 3, 3, 3, 106, 3, 52, 52, 26, 52, 3, 52, 356, 26, 3, 52, 3, 3, 26, 26, 26, 63, 3, 63, 3, 226, 52, 3, 226, 3, 63, 26, 3, 3, 3, 52, 68, 63, 63, 55, 26, 356, 3, 26, 26, 89, 3, 226, 21, 63, 63, 3, 226, 52, 3, 52, 3, 54, 11, 356, 26, 52, 52, 198, 9, 356, 55, 3, 198, 26, 199, 3, 63, 226, 226, 52, 26, 3, 3, 52, 26, 52, 9, 179, 26, 63, 198, 9, 52, 52, 52, 3, 226, 26, 602, 3, 3, 179, 226, 8, 26, 147, 26, 356, 107, 52, 3, 52, 63, 147, 132, 9, 26, 26, 3, 25, 63, 52, 54, 52, 63, 52, 3, 25, 63, 26, 179, 52, 52, 52, 52, 52, 52, 52, 132, 356, 226, 52, 52, 52, 52, 26, 226, 26, 4, 17, 26, 26, 226, 3, 63, 26, 63, 26, 63, 3, 52, 52, 13, 522, 224, 26, 17, 226, 17, 26, 198, 52, 522, 26, 226, 26, 3, 52, 56, 56, 233, 100, 26, 184, 55, 56, 26, 3, 52, 26, 199, 52, 52, 26, 26, 52, 52, 63, 52, 26, 522, 52, 3, 56, 26, 3, 226, 156, 3, 17, 52, 3, 26, 224, 26, 26, 224, 55, 63, 226, 52, 52, 100, 26, 184, 11, 52, 26, 226, 17, 26, 224, 26, 3, 26, 3, 26, 26, 3, 100, 3, 63, 26, 26, 17, 26, 226, 26, 55, 3, 3, 3, 26, 26, 52, 3, 63, 3, 236, 226, 382, 226, 52, 3, 52, 204, 3, 52, 52, 3, 382, 132, 198, 132, 132, 236, 52, 52, 132, 132, 54, 3, 3, 26, 52, 236, 132, 3, 3, 3, 204, 52, 3, 3, 52, 236, 204, 204, 236, 52, 3, 52, 204, 224, 3, 52, 52, 3, 132, 52, 63, 3, 3, 26, 156, 63, 200, 26, 199, 4, 63, 3, 63, 4, 362, 3, 55, 3, 226, 3, 226, 147, 63, 199, 52, 226, 243, 199, 54, 63, 26, 147, 156, 226, 226, 226, 63, 81, 3, 63, 26, 3, 26, 156, 156, 199, 63, 156, 26, 26, 156, 63, 184, 52, 3, 52, 3, 63, 26, 132, 26, 26, 52, 63, 199, 156, 849, 52, 156, 26, 199, 63, 26, 63, 3, 156, 147, 379, 26, 52, 52, 26, 52, 156, 26, 81, 3, 17, 52, 199, 3, 26, 26, 4, 226, 3, 63, 56, 200, 147, 156, 199, 52, 156, 226, 52, 52, 3, 184, 100, 4, 63, 4, 199, 56, 52, 226, 8, 52, 199, 850, 156, 4, 130, 52, 3, 100, 156, 26, 63, 52, 26, 156, 156, 156, 26, 55, 226, 602, 3, 226, 63, 147, 63, 52, 4, 81, 56, 3, 3, 63, 63, 199, 44, 56, 52, 199, 182, 3, 63, 3, 52, 3, 3, 3, 3, 52, 67, 52, 198, 3, 52, 52, 272, 3, 226, 132, 90, 26, 26, 52, 26, 200, 52, 52, 26, 42, 226, 226, 63, 52, 21, 52, 26, 199, 52, 63, 52, 3, 52, 63, 63, 55, 56, 156, 3, 156, 44, 67, 226, 199, 226, 52, 4, 26, 52, 226, 3, 3, 52, 226, 40, 132, 52, 63, 26, 26, 52, 3, 3, 63, 144, 26, 3, 3, 226, 3, 52, 226, 52, 3, 52, 250, 3, 226, 26, 26, 3, 164, 26, 52, 26, 199, 26, 52, 3, 226, 38, 52, 26, 26, 26, 100, 3, 55, 12, 52, 52, 3, 138, 3, 75, 26, 67, 75, 26, 199, 26, 3, 3, 26, 3, 26, 3, 311, 63, 54, 63, 272, 52, 67, 3, 3, 45, 3, 272, 55, 52, 63, 52, 226, 11, 226, 13, 3, 52, 52, 52, 26, 63, 17, 40, 3, 63, 63, 226, 26, 3, 66, 198, 26, 63, 3, 55, 26, 226, 52, 26, 3, 3, 55, 226, 199, 26, 226, 63, 356, 26, 199, 26, 226, 25, 12, 26, 26, 52, 52, 3, 11, 52, 26, 199, 199, 63, 63, 226, 52, 26, 38, 3, 3, 52, 81, 52, 26, 17, 226, 55, 3, 52, 54, 226, 198, 17, 52, 26, 184, 26, 199, 3, 199, 3, 52, 63, 52, 516, 52, 52, 55, 356, 26, 52, 26, 52, 26, 26, 226, 26, 26, 26, 52, 26, 63, 226, 3, 52, 52, 52, 132, 52, 52, 226, 26, 52, 54, 52, 55, 56, 26, 52, 179, 4, 184, 226, 12, 63, 226, 81, 52, 236, 63, 3, 63, 55, 3, 63, 93, 63, 198, 198, 52, 52, 3, 23, 308, 240, 52, 55, 63, 52, 26, 226, 52, 226, 63, 52, 26, 11, 63, 3, 26, 156, 130, 3, 52, 52, 3, 26, 3, 63, 26, 26, 3, 26, 226, 199, 26, 226, 26, 3, 56, 52, 226, 52, 75, 55, 52, 226, 3, 52, 52, 55, 26, 199, 55, 226, 63, 3, 199, 226, 26, 55, 26, 226, 52, 130, 26, 63, 52, 52, 132, 52, 26, 63, 226, 26, 3, 52, 55, 52, 3, 55, 52, 52, 55, 3, 226, 52, 3, 3, 56, 3, 52, 362, 179, 3, 63, 3, 156, 41, 156, 156, 3, 100, 100, 52, 44, 3, 52, 156, 81, 63, 156, 3, 240, 63, 26, 3, 52, 52, 52, 100, 850, 52, 199, 63, 184, 226, 3, 63, 52, 52, 132, 4, 3, 3, 25, 3, 156, 156, 26, 199, 56, 52, 81, 4, 26, 184, 199, 3, 63, 240, 63, 52, 156, 55, 3, 226, 26, 52, 3, 52, 63, 3, 3, 63, 52, 152, 52, 3, 26, 55, 26, 3, 52, 147, 26, 12, 45, 63, 63, 3, 103, 356, 52, 26, 175, 68, 92, 3, 63, 63, 3, 52, 3, 52, 52, 52, 52, 152, 3, 52, 52, 853, 25, 226, 63, 52, 226, 52, 3, 179, 63, 26, 26, 63, 52, 3, 52, 55, 63, 55, 3, 179, 199, 63, 68, 26, 63, 52, 853, 26, 63, 152, 179, 52, 15, 52, 63, 52, 17, 226, 344, 26, 92, 3, 52, 26, 52, 26, 13, 3, 52, 154, 295, 26, 26, 3, 180, 199, 26, 179, 3, 52, 55, 25, 189, 152, 3, 180, 226, 52, 92, 3, 63, 55, 226, 52, 26, 26, 68, 52, 199, 3, 3, 26, 26, 52, 3, 26, 92, 52, 26, 3, 199, 52, 3, 111, 52, 3, 52, 3, 3, 52, 3, 17, 63, 226, 3, 226, 55, 26, 52, 26, 308, 52, 52, 3, 198, 26, 26, 3, 226, 63, 26, 356, 25, 26, 12, 198, 26, 52, 52, 52, 63, 52, 55, 3, 26, 26, 52, 11, 26, 55, 26, 55, 55, 55, 26, 184, 226, 26, 52, 52, 26, 3, 63, 226, 52, 52, 52, 198, 734, 63, 52, 55, 519, 26, 26, 52, 52, 26, 55, 3, 26, 226, 26, 519, 26, 26, 52, 3, 63, 226, 54, 11, 52, 3, 3, 3, 52, 26, 26, 26, 26, 3, 26, 226, 63, 12, 52, 26, 200, 63, 26, 52, 3, 26, 226, 200, 3, 3, 519, 52, 56, 26, 52, 52, 26, 63, 52, 26, 52, 26, 63, 26, 4, 52, 3, 226, 63, 23, 52, 63, 3, 522, 11, 54, 519, 52, 11, 54, 26, 26, 12, 92, 200, 26, 26, 26, 26, 3, 147, 54, 52, 3, 3, 63, 26, 100, 26, 199, 26, 26, 52, 26, 54, 63, 52, 52, 26, 11, 26, 52, 26, 26, 226, 26, 26, 26, 55, 52, 52, 226, 52, 52, 52, 184, 26, 52, 52, 63, 52, 103, 63, 52, 52, 200, 184, 41, 63, 3, 52, 226, 52, 3, 3, 3, 3, 26, 26, 52, 26, 26, 3, 52, 23, 3, 52, 26, 308, 3, 68, 52, 56, 3, 3, 26, 199, 26, 3, 63, 52, 26, 55, 26, 52, 3, 3, 13, 198, 26, 26, 3, 55, 198, 52, 52, 11, 52, 52, 3, 602, 11, 52, 3, 3, 138, 55, 26, 52, 55, 63, 52, 55, 226, 226, 52, 226, 56, 147, 55, 3, 226, 26, 52, 226, 26, 52, 63, 198, 52, 55, 55, 26, 26, 26, 52, 26, 226, 3, 26, 12, 26, 52, 26, 26, 26, 179, 26, 147, 199, 3, 26, 54, 52, 52, 63, 26, 147, 542, 3, 356, 26, 26, 12, 26, 3, 26, 52, 236, 26, 26, 3, 111, 52, 52, 52, 52, 3, 356, 52, 26, 52, 3, 52, 519, 52, 26, 52, 52, 199, 132, 54, 595, 26, 52, 11, 52, 3, 147, 3, 3, 52, 26, 198, 3, 52, 198, 11, 3, 52, 3, 63, 52, 356, 3, 3, 26, 3, 199, 52, 52, 26, 52, 44, 52, 52, 52, 52, 12, 63, 3, 226, 26, 63, 179, 52, 56, 25, 52, 63, 55, 3, 52, 63, 3, 26, 52, 52, 100, 26, 25, 26, 55, 52, 26, 63, 26, 11, 26, 55, 26, 52, 55, 52, 52, 356, 3, 199, 55, 226, 55, 226, 26, 3, 152, 52, 3, 52, 132, 52, 3, 52, 26, 132, 63, 63, 26, 52, 3, 226, 26, 198, 26, 63, 3, 26, 63, 26, 63, 52, 26, 63, 52, 198, 3, 52, 26, 52, 250, 52, 161, 226, 52, 26, 27, 55, 52, 200, 26, 3, 356, 26, 52, 52, 184, 549, 52, 3, 100, 52, 52, 26, 26, 26, 198, 26, 52, 3, 26, 33, 26, 26, 52, 26, 3, 26, 26, 52, 63, 189, 63, 63, 52, 26, 26, 26, 26, 52, 52, 52, 54, 54, 52, 26, 52, 850, 25, 52, 26, 52, 3, 56, 226, 200, 26, 26, 26, 52, 63, 11, 3, 55, 26, 356, 52, 26, 52, 3, 11, 26, 26, 3, 63, 63, 63, 3, 26, 26, 55, 52, 3, 100, 52, 52, 52, 26, 3, 52, 52, 52, 26, 3, 226, 52]\n",
      "tensor([  3,  25,  26,  ...,   3, 226,  52])\n",
      "tensor([[-0.1419,  1.7592, -1.0372,  ..., -1.5850, -0.1384,  2.4632],\n",
      "        [-0.0678, -0.8564, -0.2361,  ..., -0.3785,  0.8328,  0.8355],\n",
      "        [-0.5345,  0.4818, -0.5177,  ..., -0.6516, -0.0477, -1.4785],\n",
      "        ...,\n",
      "        [-0.1419,  1.7592, -1.0372,  ..., -1.5850, -0.1384,  2.4632],\n",
      "        [ 0.6519,  1.9776,  0.9924,  ..., -1.5006, -0.5848,  1.2640],\n",
      "        [-1.1528,  2.3473, -0.5628,  ...,  0.3462,  1.0668, -1.2032]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([4274, 100])\n"
     ]
    }
   ],
   "source": [
    "# for the vocab, we must create a nn embedding\n",
    "# we will use the nn.Embedding class from pytorch\n",
    "\n",
    "# create a dictionary to map words to indices\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "print(word_to_idx)\n",
    "\n",
    "embedding = nn.Embedding(len(vocab), 100, device=device)\n",
    "print(embedding)\n",
    "\n",
    "# create a tensor of indices for the words in the first sentence\n",
    "sentence = df_train[df_train['id'] == 1]\n",
    "print(len(sentence))\n",
    "# print(sentence)\n",
    "\n",
    "word_indices = [word_to_idx.get(word, word_to_idx['<unk>']) for word in sentence['form']]\n",
    "print(word_indices)\n",
    "\n",
    "word_indices = torch.tensor(word_indices, dtype=torch.long, device=device)\n",
    "print(word_indices)\n",
    "\n",
    "# pass the tensor of indices to the embedding\n",
    "embedded = embedding(word_indices)\n",
    "print(embedded)\n",
    "print(embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n",
      "[-0.32307  -0.87616   0.21977   0.25268   0.22976   0.7388   -0.37954\n",
      " -0.35307  -0.84369  -1.1113   -0.30266   0.33178  -0.25113   0.30448\n",
      " -0.077491 -0.89815   0.092496 -1.1407   -0.58324   0.66869  -0.23122\n",
      " -0.95855   0.28262  -0.078848  0.75315   0.26584   0.3422   -0.33949\n",
      "  0.95608   0.065641  0.45747   0.39835   0.57965   0.39267  -0.21851\n",
      "  0.58795  -0.55999   0.63368  -0.043983 -0.68731  -0.37841   0.38026\n",
      "  0.61641  -0.88269  -0.12346  -0.37928  -0.38318   0.23868   0.6685\n",
      " -0.43321  -0.11065   0.081723  1.1569    0.78958  -0.21223  -2.3211\n",
      " -0.67806   0.44561   0.65707   0.1045    0.46217   0.19912   0.25802\n",
      "  0.057194  0.53443  -0.43133  -0.34311   0.59789  -0.58417   0.068995\n",
      "  0.23944  -0.85181   0.30379  -0.34177  -0.25746  -0.031101 -0.16285\n",
      "  0.45169  -0.91627   0.64521   0.73281  -0.22752   0.30226   0.044801\n",
      " -0.83741   0.55006  -0.52506  -1.7357    0.4751   -0.70487   0.056939\n",
      " -0.7132    0.089623  0.41394  -1.3363   -0.61915  -0.33089  -0.52881\n",
      "  0.16483  -0.98878 ]\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained word2vec model - wiki 100\n",
    "word2vec = api.load(\"glove-wiki-gigaword-100\")\n",
    "print('loaded')\n",
    "print(word2vec['king'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing label rep. to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the pos tags to one-hot vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLUDataset(Dataset):\n",
    "    def __init__(self, conllu_file):\n",
    "        self.data = self.load_conllu(conllu_file)\n",
    "\n",
    "    def load_conllu(self, conllu_file):\n",
    "        dataset = conllu.parse_incr(open(conllu_file))\n",
    "        data = []\n",
    "        for tokenlist in dataset:\n",
    "            for token in tokenlist:\n",
    "                data.append(token)\n",
    "        dataset = pd.DataFrame(data)\n",
    "        # only retain the columns form and upos\n",
    "        dataset = dataset[['form', 'upos']]\n",
    "        # convert dataset to normal list\n",
    "        dataset = dataset.values.tolist()\n",
    "        return dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'PRON']\n"
     ]
    }
   ],
   "source": [
    "# print a element in the dataset\n",
    "dataset = CoNLLUDataset(dataset_path_train)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find vocabulary and POS tags, as well as load the word embeddings to be used\n",
    "def preprocess_train(df, p=3, s=3, embedding_type='glove-wiki-gigaword-100'):\n",
    "    vocab = set(df['word'])\n",
    "    pos_tags = set(df['pos'])\n",
    "    word_vectors_all = api.load(embedding_type)\n",
    "\n",
    "    word_vectors = {}\n",
    "    for word in vocab:\n",
    "        if word in word_vectors_all:\n",
    "            word_vectors[word] = word_vectors_all[word]\n",
    "        else:\n",
    "            word_vectors[word] = torch.zeros(len(word_vectors_all['the']))\n",
    "\n",
    "    # one hot encode the POS tags\n",
    "    pos_tags_one_hot = {}\n",
    "    for i, tag in enumerate(pos_tags):\n",
    "        one_hot = torch.zeros(len(pos_tags))\n",
    "        one_hot[i] = 1\n",
    "        pos_tags_one_hot[tag] = one_hot\n",
    "\n",
    "    # # convert the df to list\n",
    "    # data = df.values.tolist()\n",
    "    # dataset = []\n",
    "    # for i in range(len(data)):\n",
    "    #     vector = []\n",
    "    #     for j in range(p):\n",
    "    #         if i - j >= 0:\n",
    "    #             vector.append(data[i - j][0])\n",
    "    #         else:\n",
    "    #             vector.append(torch.zeros(len(word_vectors_all['the'])))\n",
    "    #     # for j in range(s):\n",
    "\n",
    "    #     dataset.append([word_vectors[data[i][0]], pos_tags_one_hot[data[i][1]]])\n",
    "\n",
    "    # convert the df to list\n",
    "    data = df.values.tolist()\n",
    "    dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "    # pp.pprint(dataset)\n",
    "\n",
    "    # make copies of the np.array\n",
    "    dataset1 = dataset.copy()\n",
    "    dataset2 = dataset.copy()\n",
    "\n",
    "    for i in range(p):\n",
    "        dataset1 = dataset1[:-1]\n",
    "        dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "        # extend the ith element of the dataset with the ith element of dataset1\n",
    "        dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "\n",
    "    for i in range(s):\n",
    "        dataset2 = dataset2[1:]\n",
    "        dataset2 = np.insert(dataset2, -1, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "        # extend the ith element of the dataset with the ith element of dataset1\n",
    "        dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "\n",
    "    print(dataset.shape)\n",
    "\n",
    "    return dataset, word_vectors, pos_tags_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48655, 700)\n"
     ]
    }
   ],
   "source": [
    "word_vectors = {}\n",
    "for word in vocab:\n",
    "    if word in word_vectors_all:\n",
    "        word_vectors[word] = word_vectors_all[word]\n",
    "    else:\n",
    "        word_vectors[word] = torch.zeros(len(word_vectors_all['the']))\n",
    "\n",
    "# one hot encode the POS tags\n",
    "pos_tags_one_hot = {}\n",
    "for i, tag in enumerate(pos_tags):\n",
    "    one_hot = torch.zeros(len(pos_tags))\n",
    "    one_hot[i] = 1\n",
    "    pos_tags_one_hot[tag] = one_hot\n",
    "\n",
    "data = df.values.tolist()\n",
    "dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "# pp.pprint(dataset)\n",
    "\n",
    "# make copies of the np.array\n",
    "dataset1 = dataset.copy()\n",
    "dataset2 = dataset.copy()\n",
    "\n",
    "for i in range(3):\n",
    "    dataset1 = dataset1[:-1]\n",
    "    dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "    # extend the ith element of the dataset with the ith element of dataset1\n",
    "    dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "\n",
    "for i in range(3):\n",
    "    dataset2 = dataset2[1:]\n",
    "    dataset2 = np.insert(dataset2, -1, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "    # extend the ith element of the dataset with the ith element of dataset1\n",
    "    dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4274\n",
      "46\n",
      "(48655, 1, 700)\n"
     ]
    }
   ],
   "source": [
    "data = df.values.tolist()\n",
    "\n",
    "dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "# pp.pprint(dataset)\n",
    "\n",
    "# find max value of the dataset's elements 0th index\n",
    "max = 0\n",
    "for i in range(len(dataset)):\n",
    "    if data[i][0] > max:\n",
    "        max = data[i][0]\n",
    "\n",
    "split_dataset = []\n",
    "curr = 0\n",
    "for i in range(1, len(dataset)):\n",
    "    if data[i][0] == 1:\n",
    "        split_dataset.append(dataset[curr:i])\n",
    "        curr = i\n",
    "split_dataset.append(dataset[curr:])\n",
    "print(len(split_dataset))\n",
    "print(max)\n",
    "\n",
    "final_dataset = []\n",
    "for i in range(len(split_dataset)):\n",
    "    dataset = split_dataset[i]\n",
    "    dataset1 = dataset.copy()\n",
    "    dataset2 = dataset.copy()\n",
    "    for j in range(3):\n",
    "        dataset1 = dataset1[:-1]\n",
    "        dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "        dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "    for j in range(3):\n",
    "        dataset2 = dataset2[1:]\n",
    "        dataset2 = np.insert(dataset2, -1, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "        dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "    final_dataset.append(dataset)\n",
    "\n",
    "dataset = []\n",
    "for lst in final_dataset:\n",
    "    dataset.extend(lst)\n",
    "dataset = np.reshape(dataset, (len(dataset), 1, len(dataset[0])))\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a FNN which takes n dim input and returns pos tag\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, embed_dim, prev_n, succ_n, hidden_params, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        # for each element in hidden_params, we will create a linear layer\n",
    "        hidden_layers = []\n",
    "        hidden_layers.append(nn.Linear(embed_dim * (prev_n + 1 + succ_n), hidden_params[0]))\n",
    "        hidden_layers.append(nn.ReLU())\n",
    "        for i in range(1, len(hidden_params)):\n",
    "            hidden_layers.append(nn.Linear(hidden_params[i-1], hidden_params[i]))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "        # softmax layer for output\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "        self.output_layer = nn.Linear(hidden_params[-1], output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# create a model\n",
    "input_dim = 100\n",
    "hidden_params = [100, 50]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
