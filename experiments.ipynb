{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INLP - Assignment 2\n",
    "## Harshavardhan P - 2021111003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/56/z1dt99295w9gw5yvgg1k3f9c0000gn/T/ipykernel_21770/2546619026.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import conllu\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = ''\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word   pos\n",
      "0  what  PRON\n",
      "1    is   AUX\n",
      "2   the   DET\n",
      "3  cost  NOUN\n",
      "4    of   ADP\n",
      "       word   pos\n",
      "0         i  PRON\n",
      "1     would   AUX\n",
      "2      like  VERB\n",
      "3       the   DET\n",
      "4  cheapest   ADJ\n",
      "      word   pos\n",
      "0     what  PRON\n",
      "1      are   AUX\n",
      "2      the   DET\n",
      "3    coach  NOUN\n",
      "4  flights  NOUN\n"
     ]
    }
   ],
   "source": [
    "# import the data files\n",
    "dataset_path_train = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-train.conllu'\n",
    "dataset_path_dev = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-dev.conllu'\n",
    "dataset_path_test = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-test.conllu'\n",
    "\n",
    "dataset_train = conllu.parse_incr(open(dataset_path_train))\n",
    "dataset_dev = conllu.parse_incr(open(dataset_path_dev))\n",
    "dataset_test = conllu.parse_incr(open(dataset_path_test))\n",
    "\n",
    "# create a dataframe from the data\n",
    "def create_dataframe(dataset):\n",
    "    data = []\n",
    "    for tokenlist in dataset:\n",
    "        for token in tokenlist:\n",
    "            data.append([token['form'], token['upostag']])\n",
    "    return pd.DataFrame(data, columns=['word', 'pos'])\n",
    "\n",
    "df_train = create_dataframe(dataset_train)\n",
    "df_dev = create_dataframe(dataset_dev)\n",
    "df_test = create_dataframe(dataset_test)\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_dev.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find vocabulary and POS tags, as well as load the word embeddings to be used\n",
    "def preprocess_train(df, embedding_type='glove-wiki-gigaword-100'):\n",
    "    vocab = set(df['word'])\n",
    "    pos_tags = set(df['pos'])\n",
    "    word_vectors_all = api.load(embedding_type)\n",
    "\n",
    "    word_vectors = {}\n",
    "    for word in vocab:\n",
    "        if word in word_vectors_all:\n",
    "            word_vectors[word] = word_vectors_all[word]\n",
    "        else:\n",
    "            word_vectors[word] = torch.zeros(len(word_vectors_all['the']))\n",
    "\n",
    "    # one hot encode the POS tags\n",
    "    pos_tags_one_hot = {}\n",
    "    for i, tag in enumerate(pos_tags):\n",
    "        one_hot = torch.zeros(len(pos_tags))\n",
    "        one_hot[i] = 1\n",
    "        pos_tags_one_hot[tag] = one_hot\n",
    "\n",
    "    # convert the df to list\n",
    "    data = df.values.tolist()\n",
    "    dataset = []\n",
    "    for i in range(len(data)):\n",
    "        dataset.append([word_vectors[data[i][0]], pos_tags_one_hot[data[i][1]]])\n",
    "\n",
    "    return dataset, word_vectors, pos_tags_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess the dev and test data, using the word vectors and POS tags from the training data\n",
    "def preprocess_dev_test(df, word_vectors, pos_tags_one_hot):\n",
    "    data = df.values.tolist()\n",
    "    dataset = []\n",
    "    for i in range(len(data)):\n",
    "        # dataset.append([word_vectors[data[i][0]], pos_tags_one_hot[data[i][1]]])\n",
    "        word_embedding = word_vectors[data[i][0]] if data[i][0] in word_vectors else torch.zeros(len(word_vectors['the']))\n",
    "        pos_tag = pos_tags_one_hot[data[i][1]] if data[i][1] in pos_tags_one_hot else torch.zeros(len(pos_tags_one_hot['NOUN']))\n",
    "        dataset.append([word_embedding, pos_tag])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLUDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.dataset = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_tensor = torch.tensor(self.dataset[idx][0], dtype=torch.float32, device=device)\n",
    "        target_tensor = torch.tensor(self.dataset[idx][1], dtype=torch.float32, device=device)\n",
    "        return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "train_data, word_vectors, pos_tags_one_hot = preprocess_train(df_train)\n",
    "dev_data = preprocess_dev_test(df_dev, word_vectors, pos_tags_one_hot)\n",
    "test_data = preprocess_dev_test(df_test, word_vectors, pos_tags_one_hot)\n",
    "\n",
    "# create the dataloaders\n",
    "train_conllu_dataset = CoNLLUDataset(train_data)\n",
    "dev_conllu_dataset = CoNLLUDataset(dev_data)\n",
    "test_conllu_dataset = CoNLLUDataset(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_conllu_dataset, batch_size=32, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_conllu_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_conllu_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in training dataset:  4274\n",
      "Number of sentences in dev dataset:  572\n",
      "Number of sentences in test dataset:  586\n"
     ]
    }
   ],
   "source": [
    "# import conllu from dataset paths\n",
    "dataset_path_train = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-train.conllu'\n",
    "dataset_path_dev = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-dev.conllu'\n",
    "dataset_path_test = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-test.conllu'\n",
    "\n",
    "dataset_train = conllu.parse_incr(open(dataset_path_train))\n",
    "dataset_dev = conllu.parse_incr(open(dataset_path_dev))\n",
    "dataset_test = conllu.parse_incr(open(dataset_path_test))\n",
    "\n",
    "print(\"Number of sentences in training dataset: \", len(list(dataset_train)))\n",
    "print(\"Number of sentences in dev dataset: \", len(list(dataset_dev)))\n",
    "print(\"Number of sentences in test dataset: \", len(list(dataset_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  form lemma  upos  xpos  \\\n",
      "0   1  what  what  PRON  None   \n",
      "1   2    is    be   AUX  None   \n",
      "2   3   the   the   DET  None   \n",
      "3   4  cost  cost  NOUN  None   \n",
      "4   5    of    of   ADP  None   \n",
      "\n",
      "                                               feats  head deprel  deps  misc  \n",
      "0                            {'PronType': 'Int,Rel'}     0   root  None  None  \n",
      "1  {'Mood': 'Ind', 'Number': 'Sing', 'Person': '3...     1    cop  None  None  \n",
      "2                                {'PronType': 'Art'}     4    det  None  None  \n",
      "3                                 {'Number': 'Sing'}     1  nsubj  None  None  \n",
      "4                                               None     7   case  None  None  \n"
     ]
    }
   ],
   "source": [
    "# bring the pointer back to the beginning of the file\n",
    "dataset_train = conllu.parse_incr(open(dataset_path_train))\n",
    "dataset_dev = conllu.parse_incr(open(dataset_path_dev))\n",
    "dataset_test = conllu.parse_incr(open(dataset_path_test))\n",
    "\n",
    "# convert this data to a pandas dataframe\n",
    "def conllu_to_pandas(dataset):\n",
    "    data = []\n",
    "    for tokenlist in dataset:\n",
    "        for token in tokenlist:\n",
    "            data.append(token)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_train = conllu_to_pandas(dataset_train)\n",
    "df_dev = conllu_to_pandas(dataset_dev)\n",
    "df_test = conllu_to_pandas(dataset_test)\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the training set:  863\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary of the words in the training set\n",
    "vocab = df_train['form'].unique()\n",
    "print(\"Number of unique words in the training set: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the training set after adding <s>, </s>, <unk>:  866\n"
     ]
    }
   ],
   "source": [
    "# add <s>, </s> and <unk> to the vocabulary\n",
    "vocab = ['<s>', '</s>', '<unk>'] + list(vocab)\n",
    "print(\"Number of unique words in the training set after adding <s>, </s>, <unk>: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique upos in the training set:  13\n",
      "['PRON' 'AUX' 'DET' 'NOUN' 'ADP' 'PROPN' 'VERB' 'NUM' 'ADJ' 'CCONJ' 'ADV'\n",
      " 'PART' 'INTJ']\n"
     ]
    }
   ],
   "source": [
    "# finding all unique pos tags\n",
    "upos = df_train['upos'].unique()\n",
    "print(\"Number of unique upos in the training set: \", len(upos))\n",
    "print(upos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique upos in the training set after adding STRT, END, UNK:  16\n"
     ]
    }
   ],
   "source": [
    "# adding pos tags for <s>, </s>, <unk> to the upos\n",
    "upos = ['STRT', 'END', 'UNK'] + list(upos)\n",
    "\n",
    "print(\"Number of unique upos in the training set after adding STRT, END, UNK: \", len(upos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s>': 0, '</s>': 1, '<unk>': 2, 'what': 3, 'is': 4, 'the': 5, 'cost': 6, 'of': 7, 'a': 8, 'round': 9, 'trip': 10, 'flight': 11, 'from': 12, 'pittsburgh': 13, 'to': 14, 'atlanta': 15, 'beginning': 16, 'on': 17, 'april': 18, 'twenty': 19, 'fifth': 20, 'and': 21, 'returning': 22, 'may': 23, 'sixth': 24, 'now': 25, 'i': 26, 'need': 27, 'leaving': 28, 'fort': 29, 'worth': 30, 'arriving': 31, 'in': 32, 'denver': 33, 'no': 34, 'later': 35, 'than': 36, '2': 37, 'pm': 38, 'next': 39, 'monday': 40, 'fly': 41, 'kansas': 42, 'city': 43, 'chicago': 44, 'wednesday': 45, 'following': 46, 'day': 47, 'meaning': 48, 'meal': 49, 'code': 50, 's': 51, 'show': 52, 'me': 53, 'all': 54, 'flights': 55, 'which': 56, 'serve': 57, 'for': 58, 'after': 59, 'tomorrow': 60, 'us': 61, 'air': 62, 'list': 63, 'nonstop': 64, 'early': 65, 'tuesday': 66, 'morning': 67, 'dallas': 68, 'st.': 69, 'petersburg': 70, 'toronto': 71, 'that': 72, 'arrive': 73, 'listing': 74, 'new': 75, 'york': 76, 'montreal': 77, 'canada': 78, 'departing': 79, 'thursday': 80, 'american': 81, 'airlines': 82, 'ontario': 83, 'with': 84, 'stopover': 85, 'louis': 86, 'ground': 87, 'transportation': 88, 'houston': 89, 'afternoon': 90, 'schedule': 91, 'philadelphia': 92, 'san': 93, 'francisco': 94, 'evening': 95, 'diego': 96, 'layover': 97, 'washington': 98, 'dc': 99, 'are': 100, 'there': 101, 'any': 102, 'boston': 103, 'stop': 104, 'restrictions': 105, 'cheapest': 106, 'one': 107, 'way': 108, 'fare': 109, 'between': 110, 'oakland': 111, 'airfare': 112, '416': 113, 'dollars': 114, \"'s\": 115, 'restriction': 116, 'ap68': 117, 'california': 118, 'airports': 119, 'available': 120, 'texas': 121, 'airport': 122, 'closest': 123, 'nevada': 124, 'arizona': 125, 'actually': 126, 'las': 127, 'vegas': 128, 'burbank': 129, 'saturday': 130, 'two': 131, 'how': 132, 'many': 133, 'going': 134, 'july': 135, 'seventh': 136, 'would': 137, 'like': 138, 'an': 139, 'february': 140, 'eighth': 141, 'before': 142, '9': 143, 'am': 144, 'second': 145, 'late': 146, 'okay': 147, 'june': 148, 'first': 149, \"'d\": 150, 'go': 151, 'phoenix': 152, 'detroit': 153, 'milwaukee': 154, 'indianapolis': 155, 'does': 156, 'ls': 157, 'stand': 158, 'designate': 159, 'as': 160, 'baltimore': 161, '1115': 162, '1245': 163, 'miami': 164, 'daily': 165, '8': 166, 'trans': 167, 'world': 168, 'airline': 169, '1030': 170, '1130': 171, '5': 172, '730': 173, 'leave': 174, 'charlotte': 175, 'north': 176, 'carolina': 177, '4': 178, 'find': 179, 'newark': 180, 'jersey': 181, 'cleveland': 182, 'ohio': 183, 'do': 184, 'you': 185, 'have': 186, 'connect': 187, 'international': 188, 'minneapolis': 189, 'rental': 190, 'cars': 191, \"'ll\": 192, 'rent': 193, 'car': 194, 'sort': 195, 'near': 196, 'fine': 197, 'can': 198, 'give': 199, 'information': 200, 'downtown': 201, 'economy': 202, 'class': 203, 'fares': 204, 'december': 205, 'sixteenth': 206, 'codes': 207, 'belong': 208, 'coach': 209, 'night': 210, 'service': 211, 'november': 212, 'twelfth': 213, 'eleventh': 214, 'want': 215, 'know': 216, 'or': 217, '1': 218, \"o'clock\": 219, '3': 220, '6': 221, '10': 222, 'august': 223, 'display': 224, 'depart': 225, 'please': 226, 'snacks': 227, 'served': 228, 'tower': 229, 'types': 230, 'meals': 231, 'ever': 232, 'my': 233, 'options': 234, '382': 235, 'get': 236, 'bwi': 237, 'eastern': 238, '210': 239, 'delta': 240, '852': 241, 'latest': 242, 'return': 243, 'same': 244, 'back': 245, 'most': 246, 'hours': 247, 'take': 248, 'so': 249, 'when': 250, 'will': 251, 'maximum': 252, 'amount': 253, 'time': 254, 'still': 255, 'earliest': 256, 'departure': 257, 'be': 258, 'travel': 259, 'at': 260, 'around': 261, '7': 262, 'route': 263, 'lastest': 264, 'longest': 265, 'but': 266, 'possible': 267, 'only': 268, 'weekdays': 269, 'red': 270, 'eye': 271, 'los': 272, 'angeles': 273, 'ten': 274, 'people': 275, 'during': 276, 'week': 277, 'days': 278, 'out': 279, 'arrives': 280, 'salt': 281, 'lake': 282, 'cincinnati': 283, 'area': 284, 'explain': 285, 'ap': 286, '57': 287, '20': 288, 'mean': 289, '80': 290, 'twa': 291, '497766': 292, 'has': 293, 'stops': 294, 'friday': 295, '705': 296, 'number': 297, 'book': 298, 'least': 299, '813': 300, 'goes': 301, 'straight': 302, 'through': 303, 'without': 304, 'stopping': 305, 'another': 306, 'florida': 307, 'tell': 308, 'about': 309, 'by': 310, 'memphis': 311, 'tennessee': 312, 'noon': 313, '530': 314, 'off': 315, 'love': 316, 'field': 317, 'united': 318, 'la': 319, 'guardia': 320, 'jfk': 321, 'mco': 322, 'sfo': 323, '1991': 324, 'orlando': 325, 'lowest': 326, 'dfw': 327, 'ticket': 328, 'oak': 329, 'atl': 330, 'logan': 331, 'march': 332, 'numbers': 333, 'expensive': 334, 'continental': 335, 'leaves': 336, '1220': 337, 'seattle': 338, 'columbus': 339, 'minnesota': 340, 'those': 341, 'via': 342, 'rentals': 343, 'sunday': 344, 'rates': 345, 'costs': 346, 'limousine': 347, 'taxi': 348, 'operation': 349, 'ap80': 350, 'ap57': 351, 'ninth': 352, '12': 353, 'america': 354, 'west': 355, 'could': 356, 'fifteenth': 357, 'serves': 358, 'dinner': 359, 'provided': 360, 'cities': 361, 'where': 362, 'lester': 363, 'pearson': 364, 'canadian': 365, 'other': 366, 'earlier': 367, '1017': 368, 'northwest': 369, 'general': 370, 'mitchell': 371, 'located': 372, 'both': 373, 'nationair': 374, 'midwest': 375, 'express': 376, 'flies': 377, 'zone': 378, 'flying': 379, 'into': 380, 'much': 381, 'price': 382, 'it': 383, 'tacoma': 384, 'anywhere': 385, '1850': 386, 'midnight': 387, 'january': 388, '1992': 389, 'not': 390, 'exceeding': 391, '300': 392, 'tenth': 393, '1993': 394, '1505': 395, 'october': 396, '1994': 397, 'carries': 398, 'smallest': 399, 'passengers': 400, 'thirty': 401, 'third': 402, 'arrival': 403, 'schedules': 404, 'times': 405, 'your': 406, '269': 407, '428': 408, 'westchester': 409, 'county': 410, 'right': 411, 'september': 412, 'twentieth': 413, 'f28': 414, '755': 415, 'nights': 416, 'their': 417, 'prices': 418, '1039': 419, 'less': 420, '1100': 421, 'nashville': 422, 'again': 423, 'repeat': 424, 'make': 425, 'iah': 426, 'ord': 427, 'ewr': 428, 'dca': 429, 'cvg': 430, 'bna': 431, 'mci': 432, 'hou': 433, 'lga': 434, 'lax': 435, 'yyz': 436, 'bur': 437, 'long': 438, 'distance': 439, 'far': 440, 'paul': 441, 'miles': 442, 'name': 443, 'serviced': 444, 'regarding': 445, 'tampa': 446, 'names': 447, 'describe': 448, 'nineteenth': 449, 'seating': 450, 'capacity': 451, 'fourteenth': 452, 'aircraft': 453, 'largest': 454, 'plane': 455, 'eight': 456, 'sixteen': 457, 'departures': 458, 'seventeenth': 459, 'arrivals': 460, 'type': 461, 'greatest': 462, 'more': 463, 'business': 464, 'total': 465, 'instead': 466, 'besides': 467, 'turboprop': 468, '1059': 469, 'advertises': 470, 'having': 471, 'land': 472, 'various': 473, 'dulles': 474, 'boeing': 475, '767': 476, '466': 477, '329': 478, 'under': 479, '932': 480, '1000': 481, '200': 482, '124': 483, 'along': 484, 'equal': 485, '150': 486, 'each': 487, '400': 488, 'fit': 489, '72s': 490, 'airplane': 491, 'l1011': 492, 'hold': 493, '733': 494, 'airplanes': 495, 'uses': 496, '73s': 497, 'seats': 498, '734': 499, 'm80': 500, 'l10': 501, 'carried': 502, 'capacities': 503, '757': 504, 'planes': 505, 'd9s': 506, '100': 507, 'thrift': 508, 'level': 509, 'see': 510, 'thirtieth': 511, '505': 512, '163': 513, 'tonight': 514, 'connecting': 515, 'also': 516, 'making': 517, \"'m\": 518, 'looking': 519, 'hopefully': 520, 'makes': 521, 'yes': 522, 'breakfast': 523, 'direct': 524, 'itinerary': 525, 'departs': 526, '1940': 527, 'connects': 528, 'provide': 529, 'used': 530, 'including': 531, 'connections': 532, 'if': 533, 'either': 534, 'preferably': 535, 'local': 536, 'beach': 537, 'then': 538, 'mornings': 539, 'four': 540, 'combination': 541, 'thank': 542, 'using': 543, 'well': 544, 'run': 545, 'colorado': 546, 'fourth': 547, 'who': 548, 'sure': 549, 'determine': 550, 'use': 551, '1765': 552, 'lufthansa': 553, 'eighteenth': 554, 'f': 555, 'today': 556, 'come': 557, '320': 558, 'booking': 559, 'k': 560, 'classes': 561, 'yn': 562, 'j31': 563, 'different': 564, 'dh8': 565, 'minimum': 566, 'connection': 567, 'intercontinental': 568, 'last': 569, 'aa': 570, '459': 571, 'limousines': 572, 'services': 573, 'jose': 574, 'too': 575, 'train': 576, 'stapleton': 577, 'limo': 578, 'georgia': 579, 'pennsylvania': 580, 'utah': 581, 'missouri': 582, 'interested': 583, 'shortest': 584, 'quebec': 585, 'michigan': 586, 'indiana': 587, 'this': 588, 'wednesdays': 589, '82': 590, '139': 591, 'tickets': 592, 'sounds': 593, 'great': 594, 'let': 595, 'takeoffs': 596, 'landings': 597, 'grounds': 598, 'offer': 599, 'transport': 600, 'kind': 601, 'hi': 602, 'calling': 603, 'coming': 604, 'soon': 605, 'thereafter': 606, 'anything': 607, 'bring': 608, 'up': 609, 'y': 610, 'm': 611, 'difference': 612, 'q': 613, 'b': 614, 'qo': 615, 'qw': 616, 'qx': 617, 'fn': 618, 'qualify': 619, 'h': 620, 'basis': 621, 'bh': 622, 'offers': 623, 'included': 624, 'serving': 625, 'trying': 626, 'include': 627, 'whether': 628, 'offered': 629, 'ua': 630, '270': 631, 'being': 632, '747': 633, 'be1': 634, '737': 635, 'very': 636, 'working': 637, 'scenario': 638, 'three': 639, '727': 640, 'called': 641, 'dc10': 642, 'abbreviation': 643, 'd10': 644, 'includes': 645, '296': 646, 'should': 647, 'lunch': 648, '343': 649, 'travels': 650, 'snack': 651, 'supper': 652, '838': 653, '1110': 654, 'reaches': 655, 'sometime': 656, 'some': 657, 'reaching': 658, 'saturdays': 659, 'vicinity': 660, 'good': 661, '1800': 662, 'overnight': 663, 'final': 664, 'destination': 665, 'over': 666, 'summer': 667, '297': 668, '1222': 669, '281': 670, 'listed': 671, 'dl': 672, '1055': 673, '405': 674, '201': 675, '315': 676, '21': 677, '486': 678, '825': 679, '555': 680, '1207': 681, '1500': 682, '639': 683, '217': 684, '71': 685, '106': 686, '539': 687, '3724': 688, '271': 689, '1291': 690, '4400': 691, '3357': 692, '345': 693, '771': 694, 'co': 695, '1209': 696, 'ea': 697, '212': 698, '257': 699, '608': 700, '746': 701, 'taking': 702, '311': 703, '417': 704, 'try': 705, 'inform': 706, 'kinds': 707, 'traveling': 708, '419': 709, 'they': 710, 'these': 711, 'kindly': 712, 'proper': 713, 'town': 714, 'takeoff': 715, 'kennedy': 716, 'close': 717, '230': 718, 'nonstops': 719, 'thursdays': 720, \"'re\": 721, '1230': 722, '1200': 723, 'within': 724, 'reservation': 725, 'friends': 726, 'visit': 727, 'here': 728, 'them': 729, 'lives': 730, '0900': 731, '1600': 732, '11': 733, 'we': 734, 'nighttime': 735, 'southwest': 736, 'usa': 737, 'able': 738, 'put': 739, '630': 740, 'nw': 741, 'hp': 742, 'define': 743, 'ff': 744, 'symbols': 745, 'stands': 746, 'kw': 747, 'sam': 748, 'ac': 749, '718': 750, 'wn': 751, 'arrangements': 752, 'sorry': 753, 'must': 754, 'originating': 755, '225': 756, '1158': 757, 'equipment': 758, 'choices': 759, '1205': 760, '1145': 761, 'abbreviations': 762, 'jet': 763, 'companies': 764, 'continuing': 765, 'represented': 766, 'database': 767, 'single': 768, 'rate': 769, 'trips': 770, 'stopovers': 771, 'directly': 772, 'starting': 773, 'afterwards': 774, 'reservations': 775, 'scheduled': 776, 'seat': 777, 'india': 778, 'buy': 779, 'six': 780, '1700': 781, 'say': 782, 'mealtime': 783, '2100': 784, 'economic': 785, 'wish': 786, 'discount': 787, 'staying': 788, 'while': 789, 'look': 790, 'across': 791, 'continent': 792, 'transcontinental': 793, 'begins': 794, 'lands': 795, 'landing': 796, 'month': 797, 'help': 798, '720': 799, '110': 800, 'such': 801, '1045': 802, '934': 803, 'heading': 804, 'toward': 805, '430': 806, 'approximately': 807, '324': 808, '1300': 809, '723': 810, '1020': 811, '645': 812, 'weekday': 813, 'inexpensive': 814, 'thing': 815, 'cheap': 816, 'thanks': 817, 'question': 818, 'live': 819, 'spend': 820, 'seventeen': 821, 'highest': 822, 'priced': 823, 'charges': 824, 'dinnertime': 825, '305': 826, '845': 827, 'noontime': 828, '1026': 829, '823': 830, '2134': 831, '1024': 832, '130': 833, \"'ve\": 834, 'got': 835, 'somebody': 836, 'else': 837, 'wants': 838, '420': 839, 'seven': 840, 'catch': 841, 'fifteen': 842, 'thirteenth': 843, 'tuesdays': 844, 'midway': 845, 'alaska': 846, 'arrange': 847, 'plan': 848, 'oh': 849, 'hello': 850, \"n't\": 851, 'prefer': 852, 'requesting': 853, 'comes': 854, 'mondays': 855, 'bound': 856, 'fridays': 857, 'sundays': 858, 'bay': 859, 'planning': 860, 'home': 861, 'reverse': 862, 'order': 863, 'a.m.': 864, 'philly': 865}\n",
      "Embedding(866, 100)\n",
      "4274\n",
      "[3, 25, 26, 3, 52, 52, 63, 52, 26, 52, 25, 52, 52, 26, 100, 26, 3, 3, 3, 63, 3, 56, 63, 126, 132, 26, 52, 4, 147, 26, 26, 3, 3, 3, 52, 63, 63, 63, 63, 52, 63, 26, 3, 26, 55, 3, 55, 52, 55, 190, 26, 3, 52, 52, 197, 52, 3, 198, 132, 3, 3, 54, 54, 26, 54, 63, 224, 226, 63, 3, 226, 55, 100, 3, 100, 3, 3, 3, 184, 52, 4, 179, 132, 3, 179, 179, 26, 179, 26, 63, 3, 3, 209, 26, 26, 26, 3, 3, 3, 3, 3, 3, 226, 179, 54, 52, 285, 3, 3, 3, 3, 285, 3, 3, 3, 4, 3, 4, 63, 26, 26, 92, 4, 52, 52, 26, 63, 3, 52, 26, 45, 308, 52, 52, 199, 100, 63, 52, 308, 3, 52, 55, 55, 3, 56, 52, 323, 52, 3, 3, 3, 132, 3, 26, 26, 3, 11, 26, 3, 11, 226, 23, 11, 11, 11, 11, 26, 63, 11, 3, 3, 26, 198, 3, 190, 3, 3, 3, 3, 4, 198, 4, 198, 52, 3, 52, 285, 285, 3, 3, 63, 52, 5, 56, 55, 52, 4, 3, 63, 52, 55, 3, 52, 52, 3, 52, 3, 54, 3, 356, 3, 52, 3, 3, 156, 3, 63, 3, 362, 3, 52, 100, 3, 362, 362, 56, 52, 52, 63, 3, 3, 3, 52, 3, 52, 52, 52, 100, 179, 3, 3, 132, 132, 132, 132, 3, 132, 132, 132, 132, 3, 3, 52, 55, 226, 4, 52, 3, 52, 3, 56, 3, 3, 3, 52, 52, 3, 3, 3, 52, 52, 11, 63, 226, 17, 26, 3, 226, 61, 226, 3, 3, 52, 54, 226, 3, 3, 52, 63, 226, 52, 226, 55, 226, 26, 63, 54, 52, 3, 3, 52, 52, 54, 54, 26, 63, 63, 26, 63, 149, 423, 52, 199, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 226, 3, 3, 3, 3, 226, 3, 3, 132, 3, 132, 132, 132, 63, 3, 3, 132, 63, 132, 3, 132, 132, 132, 132, 63, 132, 132, 132, 63, 3, 132, 3, 63, 156, 226, 52, 63, 63, 308, 119, 226, 63, 308, 3, 3, 447, 3, 3, 26, 448, 56, 52, 3, 3, 226, 3, 56, 3, 3, 56, 3, 56, 199, 3, 3, 466, 7, 3, 3, 3, 4, 52, 3, 3, 52, 52, 52, 26, 52, 52, 3, 52, 226, 52, 63, 63, 63, 26, 132, 226, 3, 226, 63, 52, 52, 21, 52, 63, 56, 52, 9, 52, 52, 52, 9, 9, 52, 55, 9, 63, 9, 63, 4, 226, 52, 54, 9, 9, 55, 199, 52, 9, 132, 132, 132, 3, 3, 132, 132, 3, 3, 3, 3, 3, 3, 132, 3, 63, 3, 63, 3, 132, 3, 3, 3, 3, 3, 3, 132, 132, 54, 9, 52, 3, 100, 3, 198, 198, 198, 52, 52, 3, 3, 3, 26, 3, 52, 52, 132, 147, 52, 26, 52, 127, 26, 13, 3, 26, 4, 147, 26, 26, 100, 179, 26, 56, 26, 4, 52, 26, 26, 522, 3, 26, 26, 26, 179, 26, 26, 522, 26, 26, 4, 26, 26, 52, 26, 100, 4, 3, 4, 100, 52, 4, 52, 52, 52, 52, 52, 4, 4, 56, 26, 100, 52, 52, 226, 26, 52, 52, 26, 4, 100, 63, 63, 52, 26, 52, 132, 538, 26, 26, 26, 52, 26, 17, 100, 26, 26, 52, 147, 26, 199, 26, 26, 26, 26, 63, 3, 26, 26, 147, 26, 147, 52, 63, 199, 17, 199, 26, 26, 3, 542, 3, 55, 63, 55, 63, 63, 56, 132, 26, 55, 3, 63, 63, 63, 63, 63, 55, 63, 55, 63, 356, 63, 63, 63, 180, 63, 63, 63, 165, 52, 63, 63, 52, 26, 4, 63, 26, 3, 52, 63, 63, 3, 3, 100, 226, 100, 549, 26, 226, 3, 100, 3, 3, 63, 147, 226, 26, 63, 63, 3, 63, 3, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 226, 132, 132, 63, 132, 132, 63, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 25, 3, 3, 26, 3, 3, 3, 52, 3, 3, 3, 3, 3, 226, 3, 198, 26, 3, 3, 3, 52, 26, 3, 52, 3, 3, 100, 198, 576, 3, 26, 198, 26, 198, 3, 32, 26, 4, 26, 26, 54, 63, 26, 584, 26, 52, 226, 226, 26, 26, 226, 63, 3, 63, 200, 26, 26, 63, 63, 26, 26, 52, 226, 3, 26, 3, 17, 63, 26, 3, 199, 3, 3, 26, 52, 56, 3, 3, 3, 3, 26, 3, 3, 3, 3, 26, 63, 5, 3, 3, 52, 3, 3, 26, 3, 3, 52, 63, 3, 3, 3, 3, 3, 3, 147, 52, 3, 52, 52, 26, 52, 52, 63, 3, 3, 63, 226, 3, 3, 63, 226, 63, 4, 156, 52, 4, 156, 4, 4, 52, 3, 3, 132, 3, 3, 308, 3, 4, 52, 87, 3, 52, 4, 52, 4, 26, 3, 3, 87, 87, 3, 226, 87, 26, 3, 32, 52, 52, 308, 87, 260, 52, 3, 26, 602, 52, 52, 52, 52, 55, 26, 26, 3, 56, 32, 63, 52, 3, 52, 52, 52, 52, 52, 199, 52, 26, 26, 26, 52, 52, 26, 607, 236, 608, 3, 26, 109, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 63, 3, 3, 3, 3, 3, 3, 3, 3, 3, 63, 3, 3, 3, 3, 3, 3, 3, 3, 3, 226, 3, 3, 3, 285, 3, 3, 285, 448, 52, 26, 199, 26, 226, 3, 52, 63, 3, 199, 26, 52, 3, 156, 3, 199, 63, 52, 52, 5, 179, 226, 52, 32, 17, 3, 199, 5, 63, 3, 199, 179, 26, 184, 63, 52, 308, 636, 356, 100, 3, 184, 3, 26, 26, 308, 3, 156, 156, 100, 142, 3, 52, 226, 63, 3, 26, 356, 179, 26, 3, 3, 4, 3, 17, 156, 198, 26, 52, 3, 179, 52, 179, 26, 3, 3, 26, 3, 3, 3, 3, 52, 226, 26, 52, 156, 26, 3, 226, 52, 3, 26, 3, 52, 26, 25, 569, 56, 26, 26, 26, 52, 52, 26, 26, 26, 25, 52, 26, 226, 198, 56, 63, 3, 26, 52, 26, 26, 26, 226, 63, 26, 52, 56, 4, 356, 52, 92, 26, 56, 56, 63, 26, 63, 179, 4, 11, 63, 226, 3, 63, 63, 55, 26, 54, 52, 3, 132, 26, 55, 3, 3, 52, 26, 63, 63, 63, 63, 26, 179, 3, 198, 3, 26, 52, 11, 156, 26, 156, 26, 3, 26, 519, 3, 519, 3, 12, 132, 4, 256, 3, 26, 63, 3, 26, 26, 52, 26, 3, 132, 3, 3, 156, 52, 226, 3, 3, 52, 17, 52, 52, 17, 3, 132, 3, 4, 3, 308, 52, 61, 3, 3, 26, 3, 3, 81, 52, 226, 198, 61, 238, 61, 3, 672, 52, 3, 61, 3, 17, 3, 362, 52, 132, 198, 132, 11, 132, 3, 3, 3, 28, 3, 52, 3, 63, 544, 3, 147, 3, 3, 52, 63, 3, 3, 226, 3, 3, 3, 226, 3, 3, 26, 26, 224, 3, 3, 3, 550, 63, 26, 3, 3, 3, 3, 4, 52, 54, 26, 712, 26, 26, 3, 3, 3, 3, 63, 3, 308, 3, 63, 52, 308, 132, 52, 4, 63, 63, 4, 4, 52, 52, 132, 356, 3, 184, 3, 3, 3, 21, 132, 52, 52, 4, 132, 26, 3, 52, 4, 226, 26, 63, 63, 3, 4, 52, 63, 4, 132, 3, 63, 52, 63, 52, 198, 26, 3, 199, 26, 26, 52, 25, 52, 26, 11, 3, 52, 26, 63, 26, 3, 56, 3, 52, 3, 26, 3, 4, 26, 226, 52, 52, 26, 52, 199, 26, 198, 226, 52, 26, 3, 3, 226, 3, 3, 199, 52, 3, 26, 26, 26, 3, 3, 4, 199, 52, 63, 52, 55, 63, 56, 199, 63, 63, 26, 3, 199, 63, 26, 52, 3, 199, 100, 63, 63, 26, 26, 5, 3, 3, 199, 3, 63, 63, 63, 63, 52, 3, 3, 63, 3, 52, 11, 63, 3, 52, 147, 26, 236, 100, 52, 8, 8, 52, 198, 92, 226, 39, 56, 3, 61, 26, 52, 26, 3, 199, 3, 3, 39, 226, 179, 26, 25, 54, 52, 56, 56, 52, 26, 226, 26, 3, 308, 198, 147, 226, 226, 52, 199, 52, 542, 26, 26, 184, 184, 26, 8, 199, 52, 56, 3, 26, 198, 52, 26, 56, 3, 52, 63, 52, 199, 3, 55, 3, 52, 156, 56, 52, 26, 26, 199, 184, 156, 54, 55, 26, 55, 52, 52, 54, 56, 52, 52, 52, 200, 3, 52, 199, 3, 26, 63, 3, 12, 3, 52, 3, 52, 52, 52, 54, 52, 56, 54, 26, 63, 63, 52, 52, 54, 4, 52, 52, 52, 226, 132, 26, 52, 55, 26, 26, 17, 23, 26, 26, 3, 3, 55, 26, 26, 23, 11, 17, 26, 226, 3, 55, 3, 11, 63, 52, 602, 3, 23, 3, 56, 23, 52, 26, 52, 11, 156, 3, 147, 3, 56, 26, 26, 63, 226, 3, 224, 3, 155, 3, 26, 3, 3, 17, 11, 26, 226, 3, 52, 224, 106, 199, 26, 3, 26, 3, 17, 52, 15, 56, 4, 3, 52, 199, 52, 3, 3, 17, 26, 137, 23, 308, 199, 3, 52, 740, 106, 308, 63, 3, 224, 184, 356, 132, 3, 26, 56, 17, 199, 12, 3, 4, 106, 226, 26, 3, 3, 3, 743, 3, 3, 3, 3, 3, 285, 3, 3, 52, 3, 3, 156, 3, 356, 3, 3, 3, 3, 26, 26, 3, 3, 3, 52, 26, 52, 4, 56, 3, 52, 52, 52, 56, 3, 209, 3, 3, 3, 3, 169, 3, 226, 26, 26, 3, 156, 226, 132, 3, 3, 26, 56, 26, 3, 3, 52, 3, 3, 3, 743, 3, 3, 4, 4, 55, 3, 3, 26, 26, 26, 179, 3, 52, 3, 179, 63, 26, 100, 3, 3, 3, 11, 52, 17, 147, 4, 52, 226, 179, 56, 26, 63, 52, 3, 179, 3, 26, 26, 179, 522, 26, 3, 3, 56, 199, 3, 26, 179, 3, 26, 26, 52, 26, 3, 52, 3, 25, 224, 26, 63, 26, 52, 56, 26, 17, 3, 52, 3, 56, 26, 26, 26, 26, 52, 26, 52, 226, 52, 226, 156, 56, 3, 26, 56, 56, 52, 3, 56, 3, 226, 56, 3, 56, 63, 184, 3, 3, 356, 226, 3, 52, 63, 156, 3, 56, 56, 56, 56, 3, 52, 52, 3, 56, 226, 56, 3, 3, 63, 56, 3, 198, 226, 226, 226, 3, 63, 52, 3, 52, 3, 52, 156, 3, 4, 82, 3, 56, 52, 226, 56, 52, 56, 3, 199, 198, 3, 3, 52, 3, 226, 3, 4, 56, 198, 4, 52, 308, 56, 63, 52, 3, 56, 3, 52, 56, 3, 56, 52, 199, 226, 63, 52, 52, 198, 26, 179, 52, 52, 64, 4, 9, 3, 179, 52, 226, 26, 52, 63, 63, 52, 199, 52, 52, 26, 52, 52, 100, 52, 26, 26, 52, 26, 64, 3, 199, 64, 52, 3, 198, 26, 132, 52, 4, 52, 147, 52, 63, 63, 52, 63, 199, 52, 23, 139, 3, 52, 52, 149, 3, 26, 52, 52, 52, 132, 200, 132, 156, 147, 52, 52, 52, 3, 26, 132, 52, 3, 226, 3, 3, 52, 52, 3, 107, 226, 63, 26, 356, 3, 3, 54, 56, 52, 132, 3, 3, 52, 3, 52, 52, 3, 26, 3, 52, 52, 52, 52, 132, 226, 3, 3, 132, 26, 3, 52, 26, 52, 3, 226, 149, 3, 3, 52, 189, 52, 200, 3, 3, 26, 54, 63, 52, 9, 132, 74, 26, 52, 26, 52, 52, 226, 52, 3, 52, 26, 52, 26, 26, 3, 179, 26, 52, 52, 52, 26, 26, 52, 52, 308, 3, 3, 3, 3, 149, 3, 3, 52, 3, 3, 56, 3, 3, 54, 199, 26, 132, 3, 63, 87, 87, 198, 3, 3, 3, 156, 26, 3, 87, 52, 3, 52, 26, 52, 3, 32, 63, 3, 156, 308, 52, 226, 26, 63, 3, 52, 3, 3, 87, 3, 52, 52, 156, 52, 3, 3, 3, 3, 87, 68, 4, 15, 156, 52, 4, 52, 3, 87, 156, 52, 87, 4, 3, 789, 32, 87, 32, 226, 52, 3, 226, 26, 3, 87, 52, 4, 26, 3, 87, 26, 4, 132, 156, 87, 3, 200, 52, 3, 87, 448, 3, 3, 3, 87, 3, 52, 3, 103, 3, 3, 52, 63, 26, 26, 100, 198, 56, 4, 56, 3, 54, 602, 26, 26, 63, 26, 56, 26, 26, 26, 26, 5, 63, 198, 184, 318, 26, 184, 26, 26, 26, 3, 26, 26, 54, 3, 3, 179, 56, 52, 184, 56, 56, 100, 52, 52, 4, 179, 3, 179, 3, 26, 26, 26, 26, 369, 184, 52, 32, 100, 56, 3, 224, 26, 184, 224, 318, 26, 56, 4, 52, 52, 26, 179, 3, 26, 26, 54, 26, 56, 3, 179, 522, 26, 26, 26, 52, 93, 52, 26, 3, 26, 132, 26, 63, 52, 26, 3, 156, 318, 63, 52, 184, 3, 4, 356, 52, 179, 52, 26, 26, 3, 26, 100, 226, 179, 4, 3, 3, 198, 198, 4, 132, 26, 4, 3, 26, 200, 87, 87, 3, 198, 3, 3, 52, 179, 63, 52, 26, 3, 26, 26, 17, 4, 54, 226, 52, 26, 3, 100, 52, 17, 226, 179, 26, 3, 59, 21, 52, 3, 26, 179, 63, 63, 63, 26, 52, 26, 52, 26, 12, 54, 602, 198, 226, 3, 52, 26, 56, 52, 55, 17, 226, 63, 224, 63, 26, 26, 3, 54, 26, 52, 100, 174, 52, 63, 17, 52, 3, 3, 226, 69, 147, 3, 63, 26, 52, 55, 179, 3, 26, 3, 26, 3, 52, 184, 3, 55, 4, 308, 26, 542, 3, 52, 52, 52, 26, 226, 198, 52, 226, 54, 3, 184, 52, 26, 26, 3, 3, 356, 17, 3, 52, 179, 238, 602, 26, 26, 26, 3, 63, 3, 15, 339, 100, 52, 26, 3, 3, 63, 199, 26, 52, 63, 26, 100, 184, 3, 52, 52, 52, 63, 54, 52, 63, 4, 226, 26, 26, 63, 4, 4, 63, 3, 63, 63, 26, 52, 3, 3, 226, 3, 3, 3, 52, 52, 63, 26, 52, 3, 179, 3, 52, 3, 3, 52, 3, 3, 26, 3, 3, 63, 3, 52, 3, 242, 147, 3, 52, 179, 3, 26, 3, 52, 27, 3, 149, 139, 3, 63, 52, 3, 3, 3, 63, 3, 236, 584, 3, 3, 198, 3, 3, 226, 26, 3, 26, 52, 356, 26, 356, 3, 3, 52, 52, 3, 3, 26, 236, 3, 26, 3, 3, 26, 52, 3, 3, 3, 3, 3, 3, 3, 356, 198, 3, 308, 3, 63, 3, 52, 3, 3, 26, 26, 3, 3, 26, 198, 3, 817, 516, 3, 26, 3, 3, 3, 26, 28, 3, 3, 26, 3, 3, 198, 3, 3, 3, 3, 26, 52, 3, 443, 3, 233, 3, 250, 147, 58, 52, 3, 63, 26, 26, 3, 26, 3, 179, 3, 3, 3, 3, 256, 226, 584, 26, 3, 3, 3, 250, 250, 3, 52, 813, 52, 584, 63, 26, 3, 106, 3, 63, 3, 52, 3, 3, 26, 3, 52, 52, 52, 3, 26, 3, 26, 106, 3, 3, 63, 179, 3, 26, 379, 26, 3, 3, 3, 26, 26, 3, 52, 3, 3, 26, 3, 3, 26, 326, 52, 52, 26, 26, 179, 356, 26, 3, 198, 106, 106, 52, 63, 179, 132, 3, 199, 326, 26, 199, 3, 3, 3, 52, 52, 3, 26, 3, 26, 356, 3, 3, 26, 52, 3, 3, 3, 52, 3, 26, 3, 17, 3, 199, 52, 3, 226, 52, 26, 3, 52, 106, 3, 3, 3, 26, 3, 3, 179, 356, 179, 3, 26, 52, 226, 3, 52, 3, 3, 226, 3, 54, 198, 52, 26, 3, 3, 3, 3, 3, 52, 106, 198, 3, 137, 3, 3, 3, 52, 3, 3, 3, 3, 3, 106, 3, 52, 52, 26, 52, 3, 52, 356, 26, 3, 52, 3, 3, 26, 26, 26, 63, 3, 63, 3, 226, 52, 3, 226, 3, 63, 26, 3, 3, 3, 52, 68, 63, 63, 55, 26, 356, 3, 26, 26, 89, 3, 226, 21, 63, 63, 3, 226, 52, 3, 52, 3, 54, 11, 356, 26, 52, 52, 198, 9, 356, 55, 3, 198, 26, 199, 3, 63, 226, 226, 52, 26, 3, 3, 52, 26, 52, 9, 179, 26, 63, 198, 9, 52, 52, 52, 3, 226, 26, 602, 3, 3, 179, 226, 8, 26, 147, 26, 356, 107, 52, 3, 52, 63, 147, 132, 9, 26, 26, 3, 25, 63, 52, 54, 52, 63, 52, 3, 25, 63, 26, 179, 52, 52, 52, 52, 52, 52, 52, 132, 356, 226, 52, 52, 52, 52, 26, 226, 26, 4, 17, 26, 26, 226, 3, 63, 26, 63, 26, 63, 3, 52, 52, 13, 522, 224, 26, 17, 226, 17, 26, 198, 52, 522, 26, 226, 26, 3, 52, 56, 56, 233, 100, 26, 184, 55, 56, 26, 3, 52, 26, 199, 52, 52, 26, 26, 52, 52, 63, 52, 26, 522, 52, 3, 56, 26, 3, 226, 156, 3, 17, 52, 3, 26, 224, 26, 26, 224, 55, 63, 226, 52, 52, 100, 26, 184, 11, 52, 26, 226, 17, 26, 224, 26, 3, 26, 3, 26, 26, 3, 100, 3, 63, 26, 26, 17, 26, 226, 26, 55, 3, 3, 3, 26, 26, 52, 3, 63, 3, 236, 226, 382, 226, 52, 3, 52, 204, 3, 52, 52, 3, 382, 132, 198, 132, 132, 236, 52, 52, 132, 132, 54, 3, 3, 26, 52, 236, 132, 3, 3, 3, 204, 52, 3, 3, 52, 236, 204, 204, 236, 52, 3, 52, 204, 224, 3, 52, 52, 3, 132, 52, 63, 3, 3, 26, 156, 63, 200, 26, 199, 4, 63, 3, 63, 4, 362, 3, 55, 3, 226, 3, 226, 147, 63, 199, 52, 226, 243, 199, 54, 63, 26, 147, 156, 226, 226, 226, 63, 81, 3, 63, 26, 3, 26, 156, 156, 199, 63, 156, 26, 26, 156, 63, 184, 52, 3, 52, 3, 63, 26, 132, 26, 26, 52, 63, 199, 156, 849, 52, 156, 26, 199, 63, 26, 63, 3, 156, 147, 379, 26, 52, 52, 26, 52, 156, 26, 81, 3, 17, 52, 199, 3, 26, 26, 4, 226, 3, 63, 56, 200, 147, 156, 199, 52, 156, 226, 52, 52, 3, 184, 100, 4, 63, 4, 199, 56, 52, 226, 8, 52, 199, 850, 156, 4, 130, 52, 3, 100, 156, 26, 63, 52, 26, 156, 156, 156, 26, 55, 226, 602, 3, 226, 63, 147, 63, 52, 4, 81, 56, 3, 3, 63, 63, 199, 44, 56, 52, 199, 182, 3, 63, 3, 52, 3, 3, 3, 3, 52, 67, 52, 198, 3, 52, 52, 272, 3, 226, 132, 90, 26, 26, 52, 26, 200, 52, 52, 26, 42, 226, 226, 63, 52, 21, 52, 26, 199, 52, 63, 52, 3, 52, 63, 63, 55, 56, 156, 3, 156, 44, 67, 226, 199, 226, 52, 4, 26, 52, 226, 3, 3, 52, 226, 40, 132, 52, 63, 26, 26, 52, 3, 3, 63, 144, 26, 3, 3, 226, 3, 52, 226, 52, 3, 52, 250, 3, 226, 26, 26, 3, 164, 26, 52, 26, 199, 26, 52, 3, 226, 38, 52, 26, 26, 26, 100, 3, 55, 12, 52, 52, 3, 138, 3, 75, 26, 67, 75, 26, 199, 26, 3, 3, 26, 3, 26, 3, 311, 63, 54, 63, 272, 52, 67, 3, 3, 45, 3, 272, 55, 52, 63, 52, 226, 11, 226, 13, 3, 52, 52, 52, 26, 63, 17, 40, 3, 63, 63, 226, 26, 3, 66, 198, 26, 63, 3, 55, 26, 226, 52, 26, 3, 3, 55, 226, 199, 26, 226, 63, 356, 26, 199, 26, 226, 25, 12, 26, 26, 52, 52, 3, 11, 52, 26, 199, 199, 63, 63, 226, 52, 26, 38, 3, 3, 52, 81, 52, 26, 17, 226, 55, 3, 52, 54, 226, 198, 17, 52, 26, 184, 26, 199, 3, 199, 3, 52, 63, 52, 516, 52, 52, 55, 356, 26, 52, 26, 52, 26, 26, 226, 26, 26, 26, 52, 26, 63, 226, 3, 52, 52, 52, 132, 52, 52, 226, 26, 52, 54, 52, 55, 56, 26, 52, 179, 4, 184, 226, 12, 63, 226, 81, 52, 236, 63, 3, 63, 55, 3, 63, 93, 63, 198, 198, 52, 52, 3, 23, 308, 240, 52, 55, 63, 52, 26, 226, 52, 226, 63, 52, 26, 11, 63, 3, 26, 156, 130, 3, 52, 52, 3, 26, 3, 63, 26, 26, 3, 26, 226, 199, 26, 226, 26, 3, 56, 52, 226, 52, 75, 55, 52, 226, 3, 52, 52, 55, 26, 199, 55, 226, 63, 3, 199, 226, 26, 55, 26, 226, 52, 130, 26, 63, 52, 52, 132, 52, 26, 63, 226, 26, 3, 52, 55, 52, 3, 55, 52, 52, 55, 3, 226, 52, 3, 3, 56, 3, 52, 362, 179, 3, 63, 3, 156, 41, 156, 156, 3, 100, 100, 52, 44, 3, 52, 156, 81, 63, 156, 3, 240, 63, 26, 3, 52, 52, 52, 100, 850, 52, 199, 63, 184, 226, 3, 63, 52, 52, 132, 4, 3, 3, 25, 3, 156, 156, 26, 199, 56, 52, 81, 4, 26, 184, 199, 3, 63, 240, 63, 52, 156, 55, 3, 226, 26, 52, 3, 52, 63, 3, 3, 63, 52, 152, 52, 3, 26, 55, 26, 3, 52, 147, 26, 12, 45, 63, 63, 3, 103, 356, 52, 26, 175, 68, 92, 3, 63, 63, 3, 52, 3, 52, 52, 52, 52, 152, 3, 52, 52, 853, 25, 226, 63, 52, 226, 52, 3, 179, 63, 26, 26, 63, 52, 3, 52, 55, 63, 55, 3, 179, 199, 63, 68, 26, 63, 52, 853, 26, 63, 152, 179, 52, 15, 52, 63, 52, 17, 226, 344, 26, 92, 3, 52, 26, 52, 26, 13, 3, 52, 154, 295, 26, 26, 3, 180, 199, 26, 179, 3, 52, 55, 25, 189, 152, 3, 180, 226, 52, 92, 3, 63, 55, 226, 52, 26, 26, 68, 52, 199, 3, 3, 26, 26, 52, 3, 26, 92, 52, 26, 3, 199, 52, 3, 111, 52, 3, 52, 3, 3, 52, 3, 17, 63, 226, 3, 226, 55, 26, 52, 26, 308, 52, 52, 3, 198, 26, 26, 3, 226, 63, 26, 356, 25, 26, 12, 198, 26, 52, 52, 52, 63, 52, 55, 3, 26, 26, 52, 11, 26, 55, 26, 55, 55, 55, 26, 184, 226, 26, 52, 52, 26, 3, 63, 226, 52, 52, 52, 198, 734, 63, 52, 55, 519, 26, 26, 52, 52, 26, 55, 3, 26, 226, 26, 519, 26, 26, 52, 3, 63, 226, 54, 11, 52, 3, 3, 3, 52, 26, 26, 26, 26, 3, 26, 226, 63, 12, 52, 26, 200, 63, 26, 52, 3, 26, 226, 200, 3, 3, 519, 52, 56, 26, 52, 52, 26, 63, 52, 26, 52, 26, 63, 26, 4, 52, 3, 226, 63, 23, 52, 63, 3, 522, 11, 54, 519, 52, 11, 54, 26, 26, 12, 92, 200, 26, 26, 26, 26, 3, 147, 54, 52, 3, 3, 63, 26, 100, 26, 199, 26, 26, 52, 26, 54, 63, 52, 52, 26, 11, 26, 52, 26, 26, 226, 26, 26, 26, 55, 52, 52, 226, 52, 52, 52, 184, 26, 52, 52, 63, 52, 103, 63, 52, 52, 200, 184, 41, 63, 3, 52, 226, 52, 3, 3, 3, 3, 26, 26, 52, 26, 26, 3, 52, 23, 3, 52, 26, 308, 3, 68, 52, 56, 3, 3, 26, 199, 26, 3, 63, 52, 26, 55, 26, 52, 3, 3, 13, 198, 26, 26, 3, 55, 198, 52, 52, 11, 52, 52, 3, 602, 11, 52, 3, 3, 138, 55, 26, 52, 55, 63, 52, 55, 226, 226, 52, 226, 56, 147, 55, 3, 226, 26, 52, 226, 26, 52, 63, 198, 52, 55, 55, 26, 26, 26, 52, 26, 226, 3, 26, 12, 26, 52, 26, 26, 26, 179, 26, 147, 199, 3, 26, 54, 52, 52, 63, 26, 147, 542, 3, 356, 26, 26, 12, 26, 3, 26, 52, 236, 26, 26, 3, 111, 52, 52, 52, 52, 3, 356, 52, 26, 52, 3, 52, 519, 52, 26, 52, 52, 199, 132, 54, 595, 26, 52, 11, 52, 3, 147, 3, 3, 52, 26, 198, 3, 52, 198, 11, 3, 52, 3, 63, 52, 356, 3, 3, 26, 3, 199, 52, 52, 26, 52, 44, 52, 52, 52, 52, 12, 63, 3, 226, 26, 63, 179, 52, 56, 25, 52, 63, 55, 3, 52, 63, 3, 26, 52, 52, 100, 26, 25, 26, 55, 52, 26, 63, 26, 11, 26, 55, 26, 52, 55, 52, 52, 356, 3, 199, 55, 226, 55, 226, 26, 3, 152, 52, 3, 52, 132, 52, 3, 52, 26, 132, 63, 63, 26, 52, 3, 226, 26, 198, 26, 63, 3, 26, 63, 26, 63, 52, 26, 63, 52, 198, 3, 52, 26, 52, 250, 52, 161, 226, 52, 26, 27, 55, 52, 200, 26, 3, 356, 26, 52, 52, 184, 549, 52, 3, 100, 52, 52, 26, 26, 26, 198, 26, 52, 3, 26, 33, 26, 26, 52, 26, 3, 26, 26, 52, 63, 189, 63, 63, 52, 26, 26, 26, 26, 52, 52, 52, 54, 54, 52, 26, 52, 850, 25, 52, 26, 52, 3, 56, 226, 200, 26, 26, 26, 52, 63, 11, 3, 55, 26, 356, 52, 26, 52, 3, 11, 26, 26, 3, 63, 63, 63, 3, 26, 26, 55, 52, 3, 100, 52, 52, 52, 26, 3, 52, 52, 52, 26, 3, 226, 52]\n",
      "tensor([  3,  25,  26,  ...,   3, 226,  52], device='mps:0')\n",
      "tensor([[-0.4120, -0.1676,  0.9630,  ...,  0.1590,  2.6042, -0.3595],\n",
      "        [ 0.7394,  1.0108,  0.8930,  ..., -0.6333,  1.2931, -1.0812],\n",
      "        [ 0.3957,  0.0320,  1.2374,  ..., -1.1304,  1.1273,  0.6728],\n",
      "        ...,\n",
      "        [-0.4120, -0.1676,  0.9630,  ...,  0.1590,  2.6042, -0.3595],\n",
      "        [-1.2487, -1.2888,  2.0585,  ..., -0.9645, -0.2871,  0.4951],\n",
      "        [-0.3800,  0.9512, -1.1746,  ..., -0.4860,  2.3120, -0.8005]],\n",
      "       device='mps:0', grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([4274, 100])\n"
     ]
    }
   ],
   "source": [
    "# for the vocab, we must create a nn embedding\n",
    "# we will use the nn.Embedding class from pytorch\n",
    "\n",
    "# create a dictionary to map words to indices\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "print(word_to_idx)\n",
    "\n",
    "embedding = nn.Embedding(len(vocab), 100, device=device)\n",
    "print(embedding)\n",
    "\n",
    "# create a tensor of indices for the words in the first sentence\n",
    "sentence = df_train[df_train['id'] == 1]\n",
    "print(len(sentence))\n",
    "# print(sentence)\n",
    "\n",
    "word_indices = [word_to_idx.get(word, word_to_idx['<unk>']) for word in sentence['form']]\n",
    "print(word_indices)\n",
    "\n",
    "word_indices = torch.tensor(word_indices, dtype=torch.long, device=device)\n",
    "print(word_indices)\n",
    "\n",
    "# pass the tensor of indices to the embedding\n",
    "embedded = embedding(word_indices)\n",
    "print(embedded)\n",
    "print(embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n",
      "[-0.32307  -0.87616   0.21977   0.25268   0.22976   0.7388   -0.37954\n",
      " -0.35307  -0.84369  -1.1113   -0.30266   0.33178  -0.25113   0.30448\n",
      " -0.077491 -0.89815   0.092496 -1.1407   -0.58324   0.66869  -0.23122\n",
      " -0.95855   0.28262  -0.078848  0.75315   0.26584   0.3422   -0.33949\n",
      "  0.95608   0.065641  0.45747   0.39835   0.57965   0.39267  -0.21851\n",
      "  0.58795  -0.55999   0.63368  -0.043983 -0.68731  -0.37841   0.38026\n",
      "  0.61641  -0.88269  -0.12346  -0.37928  -0.38318   0.23868   0.6685\n",
      " -0.43321  -0.11065   0.081723  1.1569    0.78958  -0.21223  -2.3211\n",
      " -0.67806   0.44561   0.65707   0.1045    0.46217   0.19912   0.25802\n",
      "  0.057194  0.53443  -0.43133  -0.34311   0.59789  -0.58417   0.068995\n",
      "  0.23944  -0.85181   0.30379  -0.34177  -0.25746  -0.031101 -0.16285\n",
      "  0.45169  -0.91627   0.64521   0.73281  -0.22752   0.30226   0.044801\n",
      " -0.83741   0.55006  -0.52506  -1.7357    0.4751   -0.70487   0.056939\n",
      " -0.7132    0.089623  0.41394  -1.3363   -0.61915  -0.33089  -0.52881\n",
      "  0.16483  -0.98878 ]\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained word2vec model - wiki 100\n",
    "word2vec = api.load(\"glove-wiki-gigaword-100\")\n",
    "print('loaded')\n",
    "print(word2vec['king'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing label rep. to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the pos tags to one-hot vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLUDataset(Dataset):\n",
    "    def __init__(self, conllu_file):\n",
    "        self.data = self.load_conllu(conllu_file)\n",
    "\n",
    "    def load_conllu(self, conllu_file):\n",
    "        dataset = conllu.parse_incr(open(conllu_file))\n",
    "        data = []\n",
    "        for tokenlist in dataset:\n",
    "            for token in tokenlist:\n",
    "                data.append(token)\n",
    "        dataset = pd.DataFrame(data)\n",
    "        # only retain the columns form and upos\n",
    "        dataset = dataset[['form', 'upos']]\n",
    "        # convert dataset to normal list\n",
    "        dataset = dataset.values.tolist()\n",
    "        return dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'PRON']\n"
     ]
    }
   ],
   "source": [
    "# print a element in the dataset\n",
    "dataset = CoNLLUDataset(dataset_path_train)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a FNN which takes n dim input and returns pos tag\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, embed_dim, prev_n, succ_n, hidden_params, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        # for each element in hidden_params, we will create a linear layer\n",
    "        hidden_layers = []\n",
    "        hidden_layers.append(nn.Linear(embed_dim * (prev_n + 1 + succ_n), hidden_params[0]))\n",
    "        hidden_layers.append(nn.ReLU())\n",
    "        for i in range(1, len(hidden_params)):\n",
    "            hidden_layers.append(nn.Linear(hidden_params[i-1], hidden_params[i]))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "        # softmax layer for output\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "        self.output_layer = nn.Linear(hidden_params[-1], output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# create a model\n",
    "input_dim = 100\n",
    "hidden_params = [100, 50]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
