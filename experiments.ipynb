{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INLP - Assignment 2\n",
    "## Harshavardhan P - 2021111003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import conllu\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pprint as pp\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = ''\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word   pos\n",
      "0  what  PRON\n",
      "1    is   AUX\n",
      "2   the   DET\n",
      "3  cost  NOUN\n",
      "4    of   ADP\n",
      "       word   pos\n",
      "0         i  PRON\n",
      "1     would   AUX\n",
      "2      like  VERB\n",
      "3       the   DET\n",
      "4  cheapest   ADJ\n",
      "      word   pos\n",
      "0     what  PRON\n",
      "1      are   AUX\n",
      "2      the   DET\n",
      "3    coach  NOUN\n",
      "4  flights  NOUN\n"
     ]
    }
   ],
   "source": [
    "# import the data files\n",
    "dataset_path_train = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-train.conllu'\n",
    "dataset_path_dev = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-dev.conllu'\n",
    "dataset_path_test = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-test.conllu'\n",
    "\n",
    "dataset_train = conllu.parse_incr(open(dataset_path_train))\n",
    "dataset_dev = conllu.parse_incr(open(dataset_path_dev))\n",
    "dataset_test = conllu.parse_incr(open(dataset_path_test))\n",
    "\n",
    "# create a dataframe from the data\n",
    "def create_dataframe(dataset):\n",
    "    data = []\n",
    "    for tokenlist in dataset:\n",
    "        for token in tokenlist:\n",
    "            # data.append([token['form'], token['upostag']])\n",
    "            data.append([token['form'], token['upostag']])\n",
    "    # return pd.DataFrame(data, columns=['', 'word', 'pos'])\n",
    "    return pd.DataFrame(data, columns=['word', 'pos'])\n",
    "\n",
    "df_train = create_dataframe(dataset_train)\n",
    "df_dev = create_dataframe(dataset_dev)\n",
    "df_test = create_dataframe(dataset_test)\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_dev.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train\n",
    "vocab = set(df['word'])\n",
    "pos_tags = set(df['pos'])\n",
    "word_vectors_all = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(df, embedding_type='glove-wiki-gigaword-100'):\n",
    "    vocab = set(df['word'])\n",
    "    pos_tags = set(df['pos'])\n",
    "    word_vectors_all = api.load(embedding_type)\n",
    "\n",
    "    word_vectors = {}\n",
    "    for word in vocab:\n",
    "        if word in word_vectors_all:\n",
    "            word_vectors[word] = word_vectors_all[word]\n",
    "        else:\n",
    "            word_vectors[word] = np.zeros(len(word_vectors_all['the']))\n",
    "\n",
    "        # one hot encode the pos tags\n",
    "    pos_tags_one_hot = {}\n",
    "    for i, tag in enumerate(pos_tags):\n",
    "        one_hot = np.zeros(len(pos_tags) + 1)\n",
    "        one_hot[i] = 1\n",
    "        pos_tags_one_hot[tag] = one_hot\n",
    "    pos_tags_one_hot[''] = np.zeros(len(pos_tags) + 1)\n",
    "    pos_tags_one_hot[''][-1] = 1\n",
    "\n",
    "    # convert the df to list\n",
    "    data = df.values.tolist()\n",
    "    dataset = [[word_vectors[data[i][0]], pos_tags_one_hot[data[i][1]]] for i in range(len(data))]\n",
    "\n",
    "    return dataset, word_vectors, pos_tags_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dev_test(df, word_vectors, pos_tags_one_hot):\n",
    "    data = df.values.tolist()\n",
    "\n",
    "    dataset = []\n",
    "    for i in range(len(data)):\n",
    "        word = data[i][0]\n",
    "        if word in word_vectors:\n",
    "            word_vector = word_vectors[word]\n",
    "        else:\n",
    "            word_vector = np.zeros(len(word_vectors['the']))\n",
    "        if data[i][1] in pos_tags_one_hot:\n",
    "            pos_vector = pos_tags_one_hot[data[i][1]]\n",
    "        else:\n",
    "            pos_vector = pos_tags_one_hot['']\n",
    "        dataset.append([word_vector, pos_vector])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLUDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_vector = self.dataset[idx][0]\n",
    "        output_vector = self.dataset[idx][1]\n",
    "        return input_vector, output_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, word_vectors, pos_tags_one_hot = preprocess_train(df_train)\n",
    "dev_data = preprocess_dev_test(df_dev, word_vectors, pos_tags_one_hot)\n",
    "test_data = preprocess_dev_test(df_test, word_vectors, pos_tags_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conllu_dataset = CoNLLUDataset(train_data)\n",
    "dev_conllu_dataset = CoNLLUDataset(dev_data)\n",
    "test_conllu_dataset = CoNLLUDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_conllu_dataset, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_conllu_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_conllu_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an RNN which takes n dim input and returns pos tag vector\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, output_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.04014709782898414\n",
      "Epoch: 1, Loss: 0.0207097017594915\n",
      "Epoch: 2, Loss: 0.018503134911501795\n",
      "Epoch: 3, Loss: 0.017462969621071526\n",
      "Epoch: 4, Loss: 0.01690218048910338\n",
      "Epoch: 5, Loss: 0.016379896818681936\n",
      "Epoch: 6, Loss: 0.016009851857939617\n",
      "Epoch: 7, Loss: 0.015709694654801414\n",
      "Epoch: 8, Loss: 0.015433296497446167\n",
      "Epoch: 9, Loss: 0.01534674612018889\n",
      "Loss: 0.018215998272913005\n",
      "Accuracy: 0.9575556893437688\n",
      "Precision: 0.9481333968672028\n",
      "Recall: 0.9575556893437688\n",
      "F1: 0.9519920958945615\n"
     ]
    }
   ],
   "source": [
    "# Loss for classification\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def train_model(model, dataloader, epochs, optimizer, criterion):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch: {epoch}, Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            running_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    print(f'Loss: {running_loss / len(dataloader)}')\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_preds = all_preds.argmax(axis=1)\n",
    "    all_labels = all_labels.argmax(axis=1)\n",
    "    print(f'Accuracy: {accuracy_score(all_labels, all_preds)}')\n",
    "    print(f'Precision: {precision_score(all_labels, all_preds, average=\"weighted\")}')\n",
    "    print(f'Recall: {recall_score(all_labels, all_preds, average=\"weighted\")}')\n",
    "    print(f'F1: {f1_score(all_labels, all_preds, average=\"weighted\")}')\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Train the model\n",
    "model = RNN(100, 100, len(pos_tags_one_hot))\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_model(model, train_loader, 10, optimizer, criterion)\n",
    "\n",
    "# Evaluate the model\n",
    "all_labels, all_preds = evaluate_model(model, dev_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data files\n",
    "dataset_path_train = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-train.conllu'\n",
    "dataset_path_dev = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-dev.conllu'\n",
    "dataset_path_test = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-test.conllu'\n",
    "\n",
    "dataset_train = conllu.parse_incr(open(dataset_path_train))\n",
    "dataset_dev = conllu.parse_incr(open(dataset_path_dev))\n",
    "dataset_test = conllu.parse_incr(open(dataset_path_test))\n",
    "\n",
    "# create a dataframe from the data\n",
    "def create_dataframe(dataset):\n",
    "    data = []\n",
    "    for tokenlist in dataset:\n",
    "        for token in tokenlist:\n",
    "            # data.append([token['form'], token['upostag']])\n",
    "            data.append([token['id'], token['form'], token['upostag']])\n",
    "    # return pd.DataFrame(data, columns=['', 'word', 'pos'])\n",
    "    return pd.DataFrame(data, columns=['id', 'word', 'pos'])\n",
    "\n",
    "df_train = create_dataframe(dataset_train)\n",
    "df_dev = create_dataframe(dataset_dev)\n",
    "df_test = create_dataframe(dataset_test)\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_dev.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train\n",
    "vocab = set(df['word'])\n",
    "pos_tags = set(df['pos'])\n",
    "word_vectors_all = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find vocabulary and POS tags, as well as load the word embeddings to be used\n",
    "def preprocess_train(df, p=3, s=3, embedding_type='glove-wiki-gigaword-100'):\n",
    "    vocab = set(df['word'])\n",
    "    pos_tags = set(df['pos'])\n",
    "    word_vectors_all = api.load(embedding_type)\n",
    "\n",
    "    word_vectors = {}\n",
    "    for word in vocab:\n",
    "        if word in word_vectors_all:\n",
    "            word_vectors[word] = word_vectors_all[word]\n",
    "        else:\n",
    "            word_vectors[word] = torch.zeros(len(word_vectors_all['the']))\n",
    "\n",
    "    # one hot encode the POS tags\n",
    "    pos_tags_one_hot = {}\n",
    "    for i, tag in enumerate(pos_tags):\n",
    "        one_hot = torch.zeros(len(pos_tags) + 1)\n",
    "        one_hot[i] = 1\n",
    "        pos_tags_one_hot[tag] = one_hot\n",
    "    pos_tags_one_hot[''] = torch.zeros(len(pos_tags) + 1)\n",
    "    pos_tags_one_hot[''][-1] = 1\n",
    "\n",
    "    # convert the df to list\n",
    "    data = df.values.tolist()\n",
    "    dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "    # pp.pprint(dataset)\n",
    "\n",
    "    max = 0\n",
    "    for i in range(len(dataset)):\n",
    "        if data[i][0] > max:\n",
    "            max = data[i][0]\n",
    "\n",
    "    split_dataset = []\n",
    "    curr = 0\n",
    "    for i in range(1, len(dataset)):\n",
    "        if data[i][0] == 1:\n",
    "            split_dataset.append(dataset[curr:i])\n",
    "            curr = i\n",
    "    split_dataset.append(dataset[curr:])\n",
    "    # print(len(split_dataset))\n",
    "    # print(max)\n",
    "\n",
    "    final_dataset = []\n",
    "    for i in range(len(split_dataset)):\n",
    "        dataset = split_dataset[i]\n",
    "        dataset1 = dataset.copy()\n",
    "        dataset2 = dataset.copy()\n",
    "        for j in range(p):\n",
    "            dataset1 = dataset1[:-1]\n",
    "            dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "            dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "        for j in range(s):\n",
    "            dataset2 = dataset2[1:]\n",
    "            dataset2 = np.append(dataset2, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "            dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "        final_dataset.append(dataset)\n",
    "\n",
    "    # pp.pprint(final_dataset[0][0])\n",
    "    # pp.pprint(final_dataset[0][-1])\n",
    "\n",
    "    dataset = []\n",
    "    for lst in final_dataset:\n",
    "        dataset.extend(lst)\n",
    "    dataset = np.reshape(dataset, (len(dataset), 1, len(dataset[0])))\n",
    "    print(dataset.shape)\n",
    "\n",
    "    final_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        final_dataset.append([torch.tensor(dataset[i][0]), pos_tags_one_hot[data[i][2]]])\n",
    "\n",
    "    print(len(final_dataset))\n",
    "    print(len(final_dataset[0]))\n",
    "    print(len(final_dataset[0][0]))\n",
    "    print(len(final_dataset[0][1]))\n",
    "    # print(type(final_dataset[0][0]))\n",
    "    # print(type(final_dataset[0][1]))\n",
    "    # print(final_dataset[22][0])\n",
    "    # print(final_dataset[0][1])\n",
    "\n",
    "    return final_dataset, word_vectors, pos_tags_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess the dev and test data, using the word vectors and POS tags from the training data\n",
    "def preprocess_dev_test(df, word_vectors, pos_tags_one_hot, p=3, s=3):\n",
    "    data = df.values.tolist()\n",
    "    # dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "    dataset = []\n",
    "    for i in range(len(data)):\n",
    "        if data[i][1] in word_vectors:\n",
    "            dataset.append(word_vectors[data[i][1]])\n",
    "        else:\n",
    "            dataset.append(torch.zeros(len(word_vectors['the'])))\n",
    "    dataset = np.array(dataset, dtype=np.float32)\n",
    "    # pp.pprint(dataset)\n",
    "\n",
    "    max = 0\n",
    "    for i in range(len(dataset)):\n",
    "        if data[i][0] > max:\n",
    "            max = data[i][0]\n",
    "\n",
    "    split_dataset = []\n",
    "    curr = 0\n",
    "    for i in range(1, len(dataset)):\n",
    "        if data[i][0] == 1:\n",
    "            split_dataset.append(dataset[curr:i])\n",
    "            curr = i\n",
    "    split_dataset.append(dataset[curr:])\n",
    "    # print(len(split_dataset))\n",
    "    # print(max)\n",
    "\n",
    "    final_dataset = []\n",
    "    for i in range(len(split_dataset)):\n",
    "        dataset = split_dataset[i]\n",
    "        dataset1 = dataset.copy()\n",
    "        dataset2 = dataset.copy()\n",
    "        for j in range(p):\n",
    "            dataset1 = dataset1[:-1]\n",
    "            dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors['the']))], axis=0)\n",
    "            dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "        for j in range(s):\n",
    "            dataset2 = dataset2[1:]\n",
    "            dataset2 = np.append(dataset2, [np.zeros(len(word_vectors['the']))], axis=0)\n",
    "            dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "        final_dataset.append(dataset)\n",
    "\n",
    "    # pp.pprint(final_dataset[0][0])\n",
    "    # pp.pprint(final_dataset[0][-1])\n",
    "\n",
    "    dataset = []\n",
    "    for lst in final_dataset:\n",
    "        dataset.extend(lst)\n",
    "    dataset = np.reshape(dataset, (len(dataset), 1, len(dataset[0])))\n",
    "    print(dataset.shape)\n",
    "\n",
    "    final_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        # final_dataset.append([torch.tensor(dataset[i][0]), pos_tags_one_hot[data[i][2]]])\n",
    "        tensor1 = torch.tensor(dataset[i][0])\n",
    "        try:\n",
    "            tensor2 = pos_tags_one_hot[data[i][2]]\n",
    "        except:\n",
    "            tensor2 = pos_tags_one_hot['']\n",
    "        final_dataset.append([tensor1, tensor2])\n",
    "\n",
    "    print(len(final_dataset))\n",
    "    print(len(final_dataset[0]))\n",
    "    print(len(final_dataset[0][0]))\n",
    "    print(len(final_dataset[0][1]))\n",
    "    # print(type(final_dataset[0][0]))\n",
    "    # print(type(final_dataset[0][1]))\n",
    "    # print(final_dataset[22][0])\n",
    "    # print(final_dataset[0][1])\n",
    "\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLUDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.dataset = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # input_tensor = torch.tensor(self.dataset[idx][0], dtype=torch.float32, device=device)\n",
    "        input_tensor = self.dataset[idx][0].to(device)\n",
    "        # target_tensor = torch.tensor(self.dataset[idx][1], dtype=torch.float32, device=device)\n",
    "        target_tensor = self.dataset[idx][1].to(device)\n",
    "        return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "train_data, word_vectors, pos_tags_one_hot = preprocess_train(df_train)\n",
    "dev_data = preprocess_dev_test(df_dev, word_vectors, pos_tags_one_hot)\n",
    "test_data = preprocess_dev_test(df_test, word_vectors, pos_tags_one_hot)\n",
    "\n",
    "# create the dataloaders\n",
    "train_conllu_dataset = CoNLLUDataset(train_data)\n",
    "dev_conllu_dataset = CoNLLUDataset(dev_data)\n",
    "test_conllu_dataset = CoNLLUDataset(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_conllu_dataset, batch_size=64, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_conllu_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_conllu_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an FNN which takes n dim input and returns pos tag vector\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, embed_dim, prev_n, succ_n, hidden_params, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        # for each element in hidden_params, we will create a linear layer\n",
    "        hidden_layers = []\n",
    "        hidden_layers.append(nn.Linear(embed_dim * (prev_n + 1 + succ_n), hidden_params[0]))\n",
    "        hidden_layers.append(nn.ReLU())\n",
    "        for i in range(1, len(hidden_params)):\n",
    "            hidden_layers.append(nn.Linear(hidden_params[i-1], hidden_params[i]))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "        # softmax layer for output\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "        self.output_layer = nn.Linear(hidden_params[-1], output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss for classification\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# function to train the model\n",
    "def train_model(model, dataloader, epochs, optimizer, criterion):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        running_loss = 0.0\n",
    "        # for i, data in enumerate(dataloader, 0):\n",
    "        for i, data in enumerate(tqdm.tqdm(dataloader, position=0, leave=True), 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "# function to test the model\n",
    "def test_model(model, dataloader, criterion, pos_tags_one_hot):\n",
    "    # find loss, accuracy, precision, recall, f1 score\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_outputs = []\n",
    "    total_labels = []\n",
    "    with torch.no_grad():\n",
    "        # for data in dataloader:\n",
    "        for data in tqdm.tqdm(dataloader, position=0, leave=True):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, actual = torch.max(labels, 1)\n",
    "            outputs_copy = outputs.clone().detach().cpu().numpy()\n",
    "            outputs_one_hot = np.zeros(outputs_copy.shape)\n",
    "            outputs_one_hot[np.arange(outputs_copy.shape[0]), np.argmax(outputs_copy, axis=1)] = 1\n",
    "\n",
    "            total_outputs.extend(outputs_one_hot)\n",
    "            total_labels.extend(labels.clone().detach().cpu().numpy())\n",
    "\n",
    "    total_outputs = np.array(total_outputs)\n",
    "    total_labels = np.array(total_labels)\n",
    "\n",
    "    print()\n",
    "    print(f\"Loss: {running_loss/len(dataloader)}\")\n",
    "    print()\n",
    "    print(f\"Accuracy: {accuracy_score(total_labels, total_outputs)}\")\n",
    "    print(f\"Precision: {precision_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "    print(f\"Recall: {recall_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "    print(f\"F1 Score: {f1_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "\n",
    "    confusion_matrix = np.zeros((len(pos_tags_one_hot), len(pos_tags_one_hot)))\n",
    "    for i in range(len(total_labels)):\n",
    "        actual = np.argmax(total_labels[i])\n",
    "        predicted = np.argmax(total_outputs[i])\n",
    "        confusion_matrix[actual][predicted] += 1\n",
    "    confusion_matrix2 = confusion_matrix / np.sum(confusion_matrix, axis=1)\n",
    "\n",
    "    plt.imshow(confusion_matrix)\n",
    "    plt.show()\n",
    "    plt.imshow(confusion_matrix2)\n",
    "    plt.show()\n",
    "\n",
    "# train and test the FNN model\n",
    "fnn_model = FNN(100, 3, 3, [20, 20], len(pos_tags_one_hot)).to(device)\n",
    "optimizer = torch.optim.Adam(fnn_model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "train_model(fnn_model, train_dataloader, 20, optimizer, criterion)\n",
    "test_model(fnn_model, test_dataloader, criterion, pos_tags_one_hot)\n",
    "test_model(fnn_model, dev_dataloader, criterion, pos_tags_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNTrainer:\n",
    "    def __init__(self, pos_tags_one_hot, embedding_type, df_train, df_test, df_dev, criterion='bce', optimizer='adam'):\n",
    "        self.pos_tags_one_hot = pos_tags_one_hot\n",
    "        self.criterion, self.optimizer = self.setup_cr_op(criterion, optimizer)\n",
    "        self.train_dataloader, self.test_dataloader, self.dev_dataloader = self.setup_dataloaders(df_train, df_test, df_dev)\n",
    "\n",
    "    def create_model(self, embed_dim, prev_n, succ_n, hidden_params, output_dim):\n",
    "        self.model =  self.FNN(embed_dim, prev_n, succ_n, hidden_params, output_dim)\n",
    "\n",
    "    def setup_cr_op(self, criterion, optimizer):\n",
    "        criterion_ = None\n",
    "        optimizer_ = None\n",
    "        if criterion == 'cross_entropy':\n",
    "            criterion_ = nn.CrossEntropyLoss()\n",
    "        elif criterion == 'bce':\n",
    "            criterion_ = nn.BCELoss()\n",
    "        else:\n",
    "            print('Invalid criterion')\n",
    "\n",
    "        if optimizer == 'adam':\n",
    "            optimizer_ = torch.optim.Adam(self.model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "        elif optimizer == 'sgd':\n",
    "            optimizer_ = torch.optim.SGD(self.model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "        else:\n",
    "            print('Invalid optimizer')\n",
    "\n",
    "        return criterion_, optimizer_\n",
    "\n",
    "    def setup_dataloaders(self, df_train, df_test, df_dev):\n",
    "        train_data, word_vectors, pos_tags_one_hot = self.preprocess_train(df_train)\n",
    "        dev_data = self.preprocess_dev_test(df_dev, word_vectors, pos_tags_one_hot)\n",
    "        test_data = self.preprocess_dev_test(df_test, word_vectors, pos_tags_one_hot)\n",
    "\n",
    "        train_conllu_dataset = self.CoNLLUDataset(train_data)\n",
    "        dev_conllu_dataset = self.CoNLLUDataset(dev_data)\n",
    "        test_conllu_dataset = self.CoNLLUDataset(test_data)\n",
    "\n",
    "        train_dataloader = DataLoader(train_conllu_dataset, batch_size=64, shuffle=True)\n",
    "        dev_dataloader = DataLoader(dev_conllu_dataset, batch_size=64, shuffle=True)\n",
    "        test_dataloader = DataLoader(test_conllu_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        return train_dataloader, test_dataloader, dev_dataloader\n",
    "\n",
    "    # creating an FNN which takes n dim input and returns pos tag vector\n",
    "    class FNN(nn.Module):\n",
    "        def __init__(self, embed_dim, prev_n, succ_n, hidden_params, output_dim):\n",
    "            super(FNN, self).__init__()\n",
    "            # for each element in hidden_params, we will create a linear layer\n",
    "            hidden_layers = []\n",
    "            hidden_layers.append(nn.Linear(embed_dim * (prev_n + 1 + succ_n), hidden_params[0]))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "            for i in range(1, len(hidden_params)):\n",
    "                hidden_layers.append(nn.Linear(hidden_params[i-1], hidden_params[i]))\n",
    "                hidden_layers.append(nn.ReLU())\n",
    "            # softmax layer for output\n",
    "            self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "            self.output_layer = nn.Linear(hidden_params[-1], output_dim)\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.hidden_layers(x)\n",
    "            x = self.output_layer(x)\n",
    "            x = self.softmax(x)\n",
    "            return x\n",
    "\n",
    "    # function to find vocabulary and POS tags, as well as load the word embeddings to be used\n",
    "    def preprocess_train(self, df, p=3, s=3, embedding_type='glove-wiki-gigaword-100'):\n",
    "        vocab = set(df['word'])\n",
    "        pos_tags = set(df['pos'])\n",
    "        word_vectors_all = api.load(embedding_type)\n",
    "\n",
    "        word_vectors = {}\n",
    "        for word in vocab:\n",
    "            if word in word_vectors_all:\n",
    "                word_vectors[word] = word_vectors_all[word]\n",
    "            else:\n",
    "                word_vectors[word] = torch.zeros(len(word_vectors_all['the']))\n",
    "\n",
    "        # one hot encode the POS tags\n",
    "        pos_tags_one_hot = {}\n",
    "        for i, tag in enumerate(pos_tags):\n",
    "            one_hot = torch.zeros(len(pos_tags) + 1)\n",
    "            one_hot[i] = 1\n",
    "            pos_tags_one_hot[tag] = one_hot\n",
    "        pos_tags_one_hot[''] = torch.zeros(len(pos_tags) + 1)\n",
    "        pos_tags_one_hot[''][-1] = 1\n",
    "\n",
    "        # convert the df to list\n",
    "        data = df.values.tolist()\n",
    "        dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "        # pp.pprint(dataset)\n",
    "\n",
    "        max = 0\n",
    "        for i in range(len(dataset)):\n",
    "            if data[i][0] > max:\n",
    "                max = data[i][0]\n",
    "\n",
    "        split_dataset = []\n",
    "        curr = 0\n",
    "        for i in range(1, len(dataset)):\n",
    "            if data[i][0] == 1:\n",
    "                split_dataset.append(dataset[curr:i])\n",
    "                curr = i\n",
    "        split_dataset.append(dataset[curr:])\n",
    "        # print(len(split_dataset))\n",
    "        # print(max)\n",
    "\n",
    "        final_dataset = []\n",
    "        for i in range(len(split_dataset)):\n",
    "            dataset = split_dataset[i]\n",
    "            dataset1 = dataset.copy()\n",
    "            dataset2 = dataset.copy()\n",
    "            for j in range(p):\n",
    "                dataset1 = dataset1[:-1]\n",
    "                dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "                dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "            for j in range(s):\n",
    "                dataset2 = dataset2[1:]\n",
    "                dataset2 = np.append(dataset2, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "                dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "            final_dataset.append(dataset)\n",
    "\n",
    "        # pp.pprint(final_dataset[0][0])\n",
    "        # pp.pprint(final_dataset[0][-1])\n",
    "\n",
    "        dataset = []\n",
    "        for lst in final_dataset:\n",
    "            dataset.extend(lst)\n",
    "        dataset = np.reshape(dataset, (len(dataset), 1, len(dataset[0])))\n",
    "        print(dataset.shape)\n",
    "\n",
    "        final_dataset = []\n",
    "        for i in range(len(dataset)):\n",
    "            final_dataset.append([torch.tensor(dataset[i][0]), pos_tags_one_hot[data[i][2]]])\n",
    "\n",
    "        print(len(final_dataset))\n",
    "        print(len(final_dataset[0]))\n",
    "        print(len(final_dataset[0][0]))\n",
    "        print(len(final_dataset[0][1]))\n",
    "        # print(type(final_dataset[0][0]))\n",
    "        # print(type(final_dataset[0][1]))\n",
    "        # print(final_dataset[22][0])\n",
    "        # print(final_dataset[0][1])\n",
    "\n",
    "        return final_dataset, word_vectors, pos_tags_one_hot\n",
    "\n",
    "    # function to preprocess the dev and test data, using the word vectors and POS tags from the training data\n",
    "    def preprocess_dev_test(self, df, word_vectors, pos_tags_one_hot, p=3, s=3):\n",
    "        data = df.values.tolist()\n",
    "        # dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "        dataset = []\n",
    "        for i in range(len(data)):\n",
    "            if data[i][1] in word_vectors:\n",
    "                dataset.append(word_vectors[data[i][1]])\n",
    "            else:\n",
    "                dataset.append(torch.zeros(len(word_vectors['the'])))\n",
    "        dataset = np.array(dataset, dtype=np.float32)\n",
    "        # pp.pprint(dataset)\n",
    "\n",
    "        max = 0\n",
    "        for i in range(len(dataset)):\n",
    "            if data[i][0] > max:\n",
    "                max = data[i][0]\n",
    "\n",
    "        split_dataset = []\n",
    "        curr = 0\n",
    "        for i in range(1, len(dataset)):\n",
    "            if data[i][0] == 1:\n",
    "                split_dataset.append(dataset[curr:i])\n",
    "                curr = i\n",
    "        split_dataset.append(dataset[curr:])\n",
    "        # print(len(split_dataset))\n",
    "        # print(max)\n",
    "\n",
    "        final_dataset = []\n",
    "        for i in range(len(split_dataset)):\n",
    "            dataset = split_dataset[i]\n",
    "            dataset1 = dataset.copy()\n",
    "            dataset2 = dataset.copy()\n",
    "            for j in range(p):\n",
    "                dataset1 = dataset1[:-1]\n",
    "                dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors['the']))], axis=0)\n",
    "                dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "            for j in range(s):\n",
    "                dataset2 = dataset2[1:]\n",
    "                dataset2 = np.append(dataset2, [np.zeros(len(word_vectors['the']))], axis=0)\n",
    "                dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "            final_dataset.append(dataset)\n",
    "\n",
    "        # pp.pprint(final_dataset[0][0])\n",
    "        # pp.pprint(final_dataset[0][-1])\n",
    "\n",
    "        dataset = []\n",
    "        for lst in final_dataset:\n",
    "            dataset.extend(lst)\n",
    "        dataset = np.reshape(dataset, (len(dataset), 1, len(dataset[0])))\n",
    "        print(dataset.shape)\n",
    "\n",
    "        final_dataset = []\n",
    "        for i in range(len(dataset)):\n",
    "            # final_dataset.append([torch.tensor(dataset[i][0]), pos_tags_one_hot[data[i][2]]])\n",
    "            tensor1 = torch.tensor(dataset[i][0])\n",
    "            try:\n",
    "                tensor2 = pos_tags_one_hot[data[i][2]]\n",
    "            except:\n",
    "                tensor2 = pos_tags_one_hot['']\n",
    "            final_dataset.append([tensor1, tensor2])\n",
    "\n",
    "        print(len(final_dataset))\n",
    "        print(len(final_dataset[0]))\n",
    "        print(len(final_dataset[0][0]))\n",
    "        print(len(final_dataset[0][1]))\n",
    "        # print(type(final_dataset[0][0]))\n",
    "        # print(type(final_dataset[0][1]))\n",
    "        # print(final_dataset[22][0])\n",
    "        # print(final_dataset[0][1])\n",
    "\n",
    "        return final_dataset\n",
    "\n",
    "    class CoNLLUDataset(Dataset):\n",
    "        def __init__(self, data):\n",
    "            self.dataset = data\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # input_tensor = torch.tensor(self.dataset[idx][0], dtype=torch.float32, device=device)\n",
    "            input_tensor = self.dataset[idx][0].to(device)\n",
    "            # target_tensor = torch.tensor(self.dataset[idx][1], dtype=torch.float32, device=device)\n",
    "            target_tensor = self.dataset[idx][1].to(device)\n",
    "            return input_tensor, target_tensor\n",
    "\n",
    "    def train(self, epochs, train_dataloader):\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1}\")\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(tqdm.tqdm(train_dataloader, position=0, leave=True), 0):\n",
    "                inputs, labels = data\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print(f\"Epoch {epoch+1}, loss: {running_loss/len(train_dataloader)}\")\n",
    "\n",
    "    def test(self, test_dataloader):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        total_outputs = []\n",
    "        total_labels = []\n",
    "\n",
    "        print('Test Set Results:\\n')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm.tqdm(test_dataloader, position=0, leave=True):\n",
    "                inputs, labels = data\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, actual = torch.max(labels, 1)\n",
    "                outputs_copy = outputs.clone().detach().cpu().numpy()\n",
    "                outputs_one_hot = np.zeros(outputs_copy.shape)\n",
    "                outputs_one_hot[np.arange(outputs_copy.shape[0]), np.argmax(outputs_copy, axis=1)] = 1\n",
    "\n",
    "                total_outputs.extend(outputs_one_hot)\n",
    "                total_labels.extend(labels.clone().detach().cpu().numpy())\n",
    "\n",
    "        total_outputs = np.array(total_outputs)\n",
    "        total_labels = np.array(total_labels)\n",
    "\n",
    "        print()\n",
    "        print(f\"Loss: {running_loss/len(test_dataloader)}\")\n",
    "        print()\n",
    "        print(f\"Accuracy: {accuracy_score(total_labels, total_outputs)}\")\n",
    "        print(f\"Precision: {precision_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "        print(f\"Recall: {recall_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "        print(f\"F1 Score: {f1_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "\n",
    "        confusion_matrix = np.zeros((len(self.pos_tags_one_hot), len(self.pos_tags_one_hot)))\n",
    "        for i in range(len(total_labels)):\n",
    "            actual = np.argmax(total_labels[i])\n",
    "            predicted = np.argmax(total_outputs[i])\n",
    "            confusion_matrix[actual][predicted] += 1\n",
    "        confusion_matrix2 = confusion_matrix / np.sum(confusion_matrix, axis=1)\n",
    "\n",
    "        plt.imshow(confusion_matrix)\n",
    "        plt.show()\n",
    "        plt.imshow(confusion_matrix2)\n",
    "        plt.show()\n",
    "\n",
    "    def dev(self, dev_dataloader):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        total_outputs = []\n",
    "        total_labels = []\n",
    "\n",
    "        print('Dev Set Results:\\n')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm.tqdm(dev_dataloader, position=0, leave=True):\n",
    "                inputs, labels = data\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, actual = torch.max(labels, 1)\n",
    "                outputs_copy = outputs.clone().detach().cpu().numpy()\n",
    "                outputs_one_hot = np.zeros(outputs_copy.shape)\n",
    "                outputs_one_hot[np.arange(outputs_copy.shape[0]), np.argmax(outputs_copy, axis=1)] = 1\n",
    "\n",
    "                total_outputs.extend(outputs_one_hot)\n",
    "                total_labels.extend(labels.clone().detach().cpu().numpy())\n",
    "\n",
    "        total_outputs = np.array(total_outputs)\n",
    "        total_labels = np.array(total_labels)\n",
    "\n",
    "        print()\n",
    "        print(f\"Loss: {running_loss/len(dev_dataloader)}\")\n",
    "        print()\n",
    "        print(f\"Accuracy: {accuracy_score(total_labels, total_outputs)}\")\n",
    "        print(f\"Precision: {precision_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "        print(f\"Recall: {recall_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "        print(f\"F1 Score: {f1_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "\n",
    "        confusion_matrix = np.zeros((len(self.pos_tags_one_hot), len(self.pos_tags_one_hot)))\n",
    "        for i in range(len(total_labels)):\n",
    "            actual = np.argmax(total_labels[i])\n",
    "            predicted = np.argmax(total_outputs[i])\n",
    "            confusion_matrix[actual][predicted] += 1\n",
    "        confusion_matrix2 = confusion_matrix / np.sum(confusion_matrix, axis=1)\n",
    "\n",
    "        plt.imshow(confusion_matrix)\n",
    "        plt.show()\n",
    "        plt.imshow(confusion_matrix2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_trainer = FNNTrainer(pos_tags_one_hot, 'glove-wiki-gigaword-100', df_train, df_test, df_dev)\n",
    "fnn_trainer.train(20, train_dataloader)\n",
    "fnn_trainer.test(test_dataloader)\n",
    "fnn_trainer.dev(dev_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import conllu from dataset paths\n",
    "dataset_path_train = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-train.conllu'\n",
    "dataset_path_dev = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-dev.conllu'\n",
    "dataset_path_test = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-test.conllu'\n",
    "\n",
    "dataset_train = conllu.parse_incr(open(dataset_path_train))\n",
    "dataset_dev = conllu.parse_incr(open(dataset_path_dev))\n",
    "dataset_test = conllu.parse_incr(open(dataset_path_test))\n",
    "\n",
    "print(\"Number of sentences in training dataset: \", len(list(dataset_train)))\n",
    "print(\"Number of sentences in dev dataset: \", len(list(dataset_dev)))\n",
    "print(\"Number of sentences in test dataset: \", len(list(dataset_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring the pointer back to the beginning of the file\n",
    "dataset_train = conllu.parse_incr(open(dataset_path_train))\n",
    "dataset_dev = conllu.parse_incr(open(dataset_path_dev))\n",
    "dataset_test = conllu.parse_incr(open(dataset_path_test))\n",
    "\n",
    "# convert this data to a pandas dataframe\n",
    "def conllu_to_pandas(dataset):\n",
    "    data = []\n",
    "    for tokenlist in dataset:\n",
    "        for token in tokenlist:\n",
    "            data.append(token)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_train = conllu_to_pandas(dataset_train)\n",
    "df_dev = conllu_to_pandas(dataset_dev)\n",
    "df_test = conllu_to_pandas(dataset_test)\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary of the words in the training set\n",
    "vocab = df_train['form'].unique()\n",
    "print(\"Number of unique words in the training set: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add <s>, </s> and <unk> to the vocabulary\n",
    "vocab = ['<s>', '</s>', '<unk>'] + list(vocab)\n",
    "print(\"Number of unique words in the training set after adding <s>, </s>, <unk>: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding all unique pos tags\n",
    "upos = df_train['upos'].unique()\n",
    "print(\"Number of unique upos in the training set: \", len(upos))\n",
    "print(upos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding pos tags for <s>, </s>, <unk> to the upos\n",
    "upos = ['STRT', 'END', 'UNK'] + list(upos)\n",
    "\n",
    "print(\"Number of unique upos in the training set after adding STRT, END, UNK: \", len(upos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the vocab, we must create a nn embedding\n",
    "# we will use the nn.Embedding class from pytorch\n",
    "\n",
    "# create a dictionary to map words to indices\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "print(word_to_idx)\n",
    "\n",
    "embedding = nn.Embedding(len(vocab), 100, device=device)\n",
    "print(embedding)\n",
    "\n",
    "# create a tensor of indices for the words in the first sentence\n",
    "sentence = df_train[df_train['id'] == 1]\n",
    "print(len(sentence))\n",
    "# print(sentence)\n",
    "\n",
    "word_indices = [word_to_idx.get(word, word_to_idx['<unk>']) for word in sentence['form']]\n",
    "print(word_indices)\n",
    "\n",
    "word_indices = torch.tensor(word_indices, dtype=torch.long, device=device)\n",
    "print(word_indices)\n",
    "\n",
    "# pass the tensor of indices to the embedding\n",
    "embedded = embedding(word_indices)\n",
    "print(embedded)\n",
    "print(embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word2vec model - wiki 100\n",
    "word2vec = api.load(\"glove-wiki-gigaword-100\")\n",
    "print('loaded')\n",
    "print(word2vec['king'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing label rep. to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the pos tags to one-hot vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLUDataset(Dataset):\n",
    "    def __init__(self, conllu_file):\n",
    "        self.data = self.load_conllu(conllu_file)\n",
    "\n",
    "    def load_conllu(self, conllu_file):\n",
    "        dataset = conllu.parse_incr(open(conllu_file))\n",
    "        data = []\n",
    "        for tokenlist in dataset:\n",
    "            for token in tokenlist:\n",
    "                data.append(token)\n",
    "        dataset = pd.DataFrame(data)\n",
    "        # only retain the columns form and upos\n",
    "        dataset = dataset[['form', 'upos']]\n",
    "        # convert dataset to normal list\n",
    "        dataset = dataset.values.tolist()\n",
    "        return dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a element in the dataset\n",
    "dataset = CoNLLUDataset(dataset_path_train)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find vocabulary and POS tags, as well as load the word embeddings to be used\n",
    "def preprocess_train(df, p=3, s=3, embedding_type='glove-wiki-gigaword-100'):\n",
    "    vocab = set(df['word'])\n",
    "    pos_tags = set(df['pos'])\n",
    "    word_vectors_all = api.load(embedding_type)\n",
    "\n",
    "    word_vectors = {}\n",
    "    for word in vocab:\n",
    "        if word in word_vectors_all:\n",
    "            word_vectors[word] = word_vectors_all[word]\n",
    "        else:\n",
    "            word_vectors[word] = torch.zeros(len(word_vectors_all['the']))\n",
    "\n",
    "    # one hot encode the POS tags\n",
    "    pos_tags_one_hot = {}\n",
    "    for i, tag in enumerate(pos_tags):\n",
    "        one_hot = torch.zeros(len(pos_tags))\n",
    "        one_hot[i] = 1\n",
    "        pos_tags_one_hot[tag] = one_hot\n",
    "\n",
    "    # # convert the df to list\n",
    "    # data = df.values.tolist()\n",
    "    # dataset = []\n",
    "    # for i in range(len(data)):\n",
    "    #     vector = []\n",
    "    #     for j in range(p):\n",
    "    #         if i - j >= 0:\n",
    "    #             vector.append(data[i - j][0])\n",
    "    #         else:\n",
    "    #             vector.append(torch.zeros(len(word_vectors_all['the'])))\n",
    "    #     # for j in range(s):\n",
    "\n",
    "    #     dataset.append([word_vectors[data[i][0]], pos_tags_one_hot[data[i][1]]])\n",
    "\n",
    "    # convert the df to list\n",
    "    data = df.values.tolist()\n",
    "    dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "    # pp.pprint(dataset)\n",
    "\n",
    "    # make copies of the np.array\n",
    "    dataset1 = dataset.copy()\n",
    "    dataset2 = dataset.copy()\n",
    "\n",
    "    for i in range(p):\n",
    "        dataset1 = dataset1[:-1]\n",
    "        dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "        # extend the ith element of the dataset with the ith element of dataset1\n",
    "        dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "\n",
    "    for i in range(s):\n",
    "        dataset2 = dataset2[1:]\n",
    "        dataset2 = np.insert(dataset2, -1, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "        # extend the ith element of the dataset with the ith element of dataset1\n",
    "        dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "\n",
    "    print(dataset.shape)\n",
    "\n",
    "    return dataset, word_vectors, pos_tags_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = {}\n",
    "for word in vocab:\n",
    "    if word in word_vectors_all:\n",
    "        word_vectors[word] = word_vectors_all[word]\n",
    "    else:\n",
    "        word_vectors[word] = torch.zeros(len(word_vectors_all['the']))\n",
    "\n",
    "# one hot encode the POS tags\n",
    "pos_tags_one_hot = {}\n",
    "for i, tag in enumerate(pos_tags):\n",
    "    one_hot = torch.zeros(len(pos_tags))\n",
    "    one_hot[i] = 1\n",
    "    pos_tags_one_hot[tag] = one_hot\n",
    "\n",
    "data = df.values.tolist()\n",
    "dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "# pp.pprint(dataset)\n",
    "\n",
    "# make copies of the np.array\n",
    "dataset1 = dataset.copy()\n",
    "dataset2 = dataset.copy()\n",
    "\n",
    "for i in range(3):\n",
    "    dataset1 = dataset1[:-1]\n",
    "    dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "    # extend the ith element of the dataset with the ith element of dataset1\n",
    "    dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "\n",
    "for i in range(3):\n",
    "    dataset2 = dataset2[1:]\n",
    "    dataset2 = np.insert(dataset2, -1, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "    # extend the ith element of the dataset with the ith element of dataset1\n",
    "    dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values.tolist()\n",
    "\n",
    "dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "# pp.pprint(dataset)\n",
    "\n",
    "# find max value of the dataset's elements 0th index\n",
    "max = 0\n",
    "for i in range(len(dataset)):\n",
    "    if data[i][0] > max:\n",
    "        max = data[i][0]\n",
    "\n",
    "split_dataset = []\n",
    "curr = 0\n",
    "for i in range(1, len(dataset)):\n",
    "    if data[i][0] == 1:\n",
    "        split_dataset.append(dataset[curr:i])\n",
    "        curr = i\n",
    "split_dataset.append(dataset[curr:])\n",
    "print(len(split_dataset))\n",
    "print(max)\n",
    "\n",
    "final_dataset = []\n",
    "for i in range(len(split_dataset)):\n",
    "    dataset = split_dataset[i]\n",
    "    dataset1 = dataset.copy()\n",
    "    dataset2 = dataset.copy()\n",
    "    for j in range(3):\n",
    "        dataset1 = dataset1[:-1]\n",
    "        dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "        dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "    for j in range(3):\n",
    "        dataset2 = dataset2[1:]\n",
    "        dataset2 = np.insert(dataset2, -1, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "        dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "    final_dataset.append(dataset)\n",
    "\n",
    "dataset = []\n",
    "for lst in final_dataset:\n",
    "    dataset.extend(lst)\n",
    "dataset = np.reshape(dataset, (len(dataset), 1, len(dataset[0])))\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a FNN which takes n dim input and returns pos tag\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, embed_dim, prev_n, succ_n, hidden_params, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        # for each element in hidden_params, we will create a linear layer\n",
    "        hidden_layers = []\n",
    "        hidden_layers.append(nn.Linear(embed_dim * (prev_n + 1 + succ_n), hidden_params[0]))\n",
    "        hidden_layers.append(nn.ReLU())\n",
    "        for i in range(1, len(hidden_params)):\n",
    "            hidden_layers.append(nn.Linear(hidden_params[i-1], hidden_params[i]))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "        # softmax layer for output\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "        self.output_layer = nn.Linear(hidden_params[-1], output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# create a model\n",
    "input_dim = 100\n",
    "hidden_params = [100, 50]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
