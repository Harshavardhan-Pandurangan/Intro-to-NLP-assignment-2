{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INLP - Assignment 2\n",
    "## Harshavardhan P - 2021111003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import conllu\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pprint as pp\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = ''\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  word   pos\n",
      "0   1  what  PRON\n",
      "1   2    is   AUX\n",
      "2   3   the   DET\n",
      "3   4  cost  NOUN\n",
      "4   5    of   ADP\n",
      "   id      word   pos\n",
      "0   1         i  PRON\n",
      "1   2     would   AUX\n",
      "2   3      like  VERB\n",
      "3   4       the   DET\n",
      "4   5  cheapest   ADJ\n",
      "   id     word   pos\n",
      "0   1     what  PRON\n",
      "1   2      are   AUX\n",
      "2   3      the   DET\n",
      "3   4    coach  NOUN\n",
      "4   5  flights  NOUN\n"
     ]
    }
   ],
   "source": [
    "# import the data files\n",
    "dataset_path_train = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-train.conllu'\n",
    "dataset_path_dev = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-dev.conllu'\n",
    "dataset_path_test = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-test.conllu'\n",
    "\n",
    "dataset_train = conllu.parse_incr(open(dataset_path_train))\n",
    "dataset_dev = conllu.parse_incr(open(dataset_path_dev))\n",
    "dataset_test = conllu.parse_incr(open(dataset_path_test))\n",
    "\n",
    "# create a dataframe from the data\n",
    "def create_dataframe(dataset):\n",
    "    data = []\n",
    "    for tokenlist in dataset:\n",
    "        for token in tokenlist:\n",
    "            # data.append([token['form'], token['upostag']])\n",
    "            data.append([token['id'], token['form'], token['upostag']])\n",
    "    # return pd.DataFrame(data, columns=['', 'word', 'pos'])\n",
    "    return pd.DataFrame(data, columns=['id', 'word', 'pos'])\n",
    "\n",
    "df_train = create_dataframe(dataset_train)\n",
    "df_dev = create_dataframe(dataset_dev)\n",
    "df_test = create_dataframe(dataset_test)\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_dev.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train\n",
    "vocab = set(df['word'])\n",
    "pos_tags = set(df['pos'])\n",
    "word_vectors_all = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find vocabulary and POS tags, as well as load the word embeddings to be used\n",
    "def preprocess_train(df, p=3, s=3, embedding_type='glove-wiki-gigaword-100'):\n",
    "    vocab = set(df['word'])\n",
    "    pos_tags = set(df['pos'])\n",
    "    word_vectors_all = api.load(embedding_type)\n",
    "\n",
    "    word_vectors = {}\n",
    "    for word in vocab:\n",
    "        if word in word_vectors_all:\n",
    "            word_vectors[word] = word_vectors_all[word]\n",
    "        else:\n",
    "            word_vectors[word] = torch.zeros(len(word_vectors_all['the']))\n",
    "\n",
    "    # one hot encode the POS tags\n",
    "    pos_tags_one_hot = {}\n",
    "    for i, tag in enumerate(pos_tags):\n",
    "        one_hot = torch.zeros(len(pos_tags) + 1)\n",
    "        one_hot[i] = 1\n",
    "        pos_tags_one_hot[tag] = one_hot\n",
    "    pos_tags_one_hot[''] = torch.zeros(len(pos_tags) + 1)\n",
    "    pos_tags_one_hot[''][-1] = 1\n",
    "\n",
    "    # convert the df to list\n",
    "    data = df.values.tolist()\n",
    "    dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "    # pp.pprint(dataset)\n",
    "\n",
    "    max = 0\n",
    "    for i in range(len(dataset)):\n",
    "        if data[i][0] > max:\n",
    "            max = data[i][0]\n",
    "\n",
    "    split_dataset = []\n",
    "    curr = 0\n",
    "    for i in range(1, len(dataset)):\n",
    "        if data[i][0] == 1:\n",
    "            split_dataset.append(dataset[curr:i])\n",
    "            curr = i\n",
    "    split_dataset.append(dataset[curr:])\n",
    "    # print(len(split_dataset))\n",
    "    # print(max)\n",
    "\n",
    "    final_dataset = []\n",
    "    for i in range(len(split_dataset)):\n",
    "        dataset = split_dataset[i]\n",
    "        dataset1 = dataset.copy()\n",
    "        dataset2 = dataset.copy()\n",
    "        for j in range(p):\n",
    "            dataset1 = dataset1[:-1]\n",
    "            dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "            dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "        for j in range(s):\n",
    "            dataset2 = dataset2[1:]\n",
    "            dataset2 = np.append(dataset2, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "            dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "        final_dataset.append(dataset)\n",
    "\n",
    "    # pp.pprint(final_dataset[0][0])\n",
    "    # pp.pprint(final_dataset[0][-1])\n",
    "\n",
    "    dataset = []\n",
    "    for lst in final_dataset:\n",
    "        dataset.extend(lst)\n",
    "    dataset = np.reshape(dataset, (len(dataset), 1, len(dataset[0])))\n",
    "    print(dataset.shape)\n",
    "\n",
    "    final_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        final_dataset.append([torch.tensor(dataset[i][0]), pos_tags_one_hot[data[i][2]]])\n",
    "\n",
    "    print(len(final_dataset))\n",
    "    print(len(final_dataset[0]))\n",
    "    print(len(final_dataset[0][0]))\n",
    "    print(len(final_dataset[0][1]))\n",
    "    # print(type(final_dataset[0][0]))\n",
    "    # print(type(final_dataset[0][1]))\n",
    "    # print(final_dataset[22][0])\n",
    "    # print(final_dataset[0][1])\n",
    "\n",
    "    return final_dataset, word_vectors, pos_tags_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess the dev and test data, using the word vectors and POS tags from the training data\n",
    "def preprocess_dev_test(df, word_vectors, pos_tags_one_hot, p=3, s=3):\n",
    "    data = df.values.tolist()\n",
    "    # dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "    dataset = []\n",
    "    for i in range(len(data)):\n",
    "        if data[i][1] in word_vectors:\n",
    "            dataset.append(word_vectors[data[i][1]])\n",
    "        else:\n",
    "            dataset.append(torch.zeros(len(word_vectors['the'])))\n",
    "    dataset = np.array(dataset, dtype=np.float32)\n",
    "    # pp.pprint(dataset)\n",
    "\n",
    "    max = 0\n",
    "    for i in range(len(dataset)):\n",
    "        if data[i][0] > max:\n",
    "            max = data[i][0]\n",
    "\n",
    "    split_dataset = []\n",
    "    curr = 0\n",
    "    for i in range(1, len(dataset)):\n",
    "        if data[i][0] == 1:\n",
    "            split_dataset.append(dataset[curr:i])\n",
    "            curr = i\n",
    "    split_dataset.append(dataset[curr:])\n",
    "    # print(len(split_dataset))\n",
    "    # print(max)\n",
    "\n",
    "    final_dataset = []\n",
    "    for i in range(len(split_dataset)):\n",
    "        dataset = split_dataset[i]\n",
    "        dataset1 = dataset.copy()\n",
    "        dataset2 = dataset.copy()\n",
    "        for j in range(p):\n",
    "            dataset1 = dataset1[:-1]\n",
    "            dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors['the']))], axis=0)\n",
    "            dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "        for j in range(s):\n",
    "            dataset2 = dataset2[1:]\n",
    "            dataset2 = np.append(dataset2, [np.zeros(len(word_vectors['the']))], axis=0)\n",
    "            dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "        final_dataset.append(dataset)\n",
    "\n",
    "    # pp.pprint(final_dataset[0][0])\n",
    "    # pp.pprint(final_dataset[0][-1])\n",
    "\n",
    "    dataset = []\n",
    "    for lst in final_dataset:\n",
    "        dataset.extend(lst)\n",
    "    dataset = np.reshape(dataset, (len(dataset), 1, len(dataset[0])))\n",
    "    print(dataset.shape)\n",
    "\n",
    "    final_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        # final_dataset.append([torch.tensor(dataset[i][0]), pos_tags_one_hot[data[i][2]]])\n",
    "        tensor1 = torch.tensor(dataset[i][0])\n",
    "        try:\n",
    "            tensor2 = pos_tags_one_hot[data[i][2]]\n",
    "        except:\n",
    "            tensor2 = pos_tags_one_hot['']\n",
    "        final_dataset.append([tensor1, tensor2])\n",
    "\n",
    "    print(len(final_dataset))\n",
    "    print(len(final_dataset[0]))\n",
    "    print(len(final_dataset[0][0]))\n",
    "    print(len(final_dataset[0][1]))\n",
    "    # print(type(final_dataset[0][0]))\n",
    "    # print(type(final_dataset[0][1]))\n",
    "    # print(final_dataset[22][0])\n",
    "    # print(final_dataset[0][1])\n",
    "\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLUDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.dataset = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # input_tensor = torch.tensor(self.dataset[idx][0], dtype=torch.float32, device=device)\n",
    "        input_tensor = self.dataset[idx][0].to(device)\n",
    "        # target_tensor = torch.tensor(self.dataset[idx][1], dtype=torch.float32, device=device)\n",
    "        target_tensor = self.dataset[idx][1].to(device)\n",
    "        return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48655, 1, 700)\n",
      "48655\n",
      "2\n",
      "700\n",
      "14\n",
      "(6644, 1, 700)\n",
      "6644\n",
      "2\n",
      "700\n",
      "14\n",
      "(6580, 1, 700)\n",
      "6580\n",
      "2\n",
      "700\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# preprocess the data\n",
    "train_data, word_vectors, pos_tags_one_hot = preprocess_train(df_train)\n",
    "dev_data = preprocess_dev_test(df_dev, word_vectors, pos_tags_one_hot)\n",
    "test_data = preprocess_dev_test(df_test, word_vectors, pos_tags_one_hot)\n",
    "\n",
    "# create the dataloaders\n",
    "train_conllu_dataset = CoNLLUDataset(train_data)\n",
    "dev_conllu_dataset = CoNLLUDataset(dev_data)\n",
    "test_conllu_dataset = CoNLLUDataset(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_conllu_dataset, batch_size=64, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_conllu_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_conllu_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an FNN which takes n dim input and returns pos tag vector\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, embed_dim, prev_n, succ_n, hidden_params, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        # for each element in hidden_params, we will create a linear layer\n",
    "        hidden_layers = []\n",
    "        hidden_layers.append(nn.Linear(embed_dim * (prev_n + 1 + succ_n), hidden_params[0]))\n",
    "        hidden_layers.append(nn.ReLU())\n",
    "        for i in range(1, len(hidden_params)):\n",
    "            hidden_layers.append(nn.Linear(hidden_params[i-1], hidden_params[i]))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "        # softmax layer for output\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "        self.output_layer = nn.Linear(hidden_params[-1], output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an RNN which takes n dim input and returns pos tag vector\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, output_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:01<00:00, 517.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.03351430920578059\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:01<00:00, 595.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss: 0.013180957747312476\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [00:01<00:00, 564.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss: 0.011306621873158956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:00<00:00, 1152.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.0120525477664292\n",
      "\n",
      "Accuracy: 0.9756838905775076\n",
      "Precision: 0.9754318772132632\n",
      "Recall: 0.9756838905775076\n",
      "F1 Score: 0.9752996708843468\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:00<00:00, 978.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.01390953288332989\n",
      "\n",
      "Accuracy: 0.9697471402769416\n",
      "Precision: 0.9697908266417944\n",
      "Recall: 0.9697471402769416\n",
      "F1 Score: 0.9693767335213349\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss for classification\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# function to train the model\n",
    "def train_model(model, dataloader, epochs, optimizer, criterion):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        running_loss = 0.0\n",
    "        # for i, data in enumerate(dataloader, 0):\n",
    "        for i, data in enumerate(tqdm.tqdm(dataloader, position=0, leave=True), 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "# function to test the model\n",
    "def test_model(model, dataloader, criterion, pos_tags_one_hot):\n",
    "    # find loss, accuracy, precision, recall, f1 score\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_outputs = []\n",
    "    total_labels = []\n",
    "    with torch.no_grad():\n",
    "        # for data in dataloader:\n",
    "        for data in tqdm.tqdm(dataloader, position=0, leave=True):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, actual = torch.max(labels, 1)\n",
    "            outputs_copy = outputs.clone().detach().cpu().numpy()\n",
    "            outputs_one_hot = np.zeros(outputs_copy.shape)\n",
    "            outputs_one_hot[np.arange(outputs_copy.shape[0]), np.argmax(outputs_copy, axis=1)] = 1\n",
    "\n",
    "            total_outputs.extend(outputs_one_hot)\n",
    "            total_labels.extend(labels.clone().detach().cpu().numpy())\n",
    "\n",
    "    total_outputs = np.array(total_outputs)\n",
    "    total_labels = np.array(total_labels)\n",
    "    print()\n",
    "    print(f\"Loss: {running_loss/len(dataloader)}\")\n",
    "    print()\n",
    "    print(f\"Accuracy: {accuracy_score(total_labels, total_outputs)}\")\n",
    "    print(f\"Precision: {precision_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "    print(f\"Recall: {recall_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "    print(f\"F1 Score: {f1_score(total_labels, total_outputs, average='weighted', zero_division=0)}\")\n",
    "    confusion_matrix = np.zeros((len(pos_tags_one_hot), len(pos_tags_one_hot)))\n",
    "    for i in range(len(total_labels)):\n",
    "        actual = np.argmax(total_labels[i])\n",
    "        predicted = np.argmax(total_outputs[i])\n",
    "        confusion_matrix[actual][predicted] += 1\n",
    "    confusion_matrix2 = confusion_matrix / np.sum(confusion_matrix, axis=1)\n",
    "    plt.imshow(confusion_matrix)\n",
    "    plt.show()\n",
    "    plt.imshow(confusion_matrix2)\n",
    "    plt.show()\n",
    "\n",
    "# train and test the FNN model\n",
    "fnn_model = FNN(100, 3, 3, [100], len(pos_tags_one_hot)).to(device)\n",
    "optimizer = torch.optim.Adam(fnn_model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "train_model(fnn_model, train_dataloader, 3, optimizer, criterion)\n",
    "test_model(fnn_model, test_dataloader, criterion, pos_tags_one_hot)\n",
    "test_model(fnn_model, dev_dataloader, criterion, pos_tags_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in training dataset:  4274\n",
      "Number of sentences in dev dataset:  572\n",
      "Number of sentences in test dataset:  586\n"
     ]
    }
   ],
   "source": [
    "# import conllu from dataset paths\n",
    "dataset_path_train = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-train.conllu'\n",
    "dataset_path_dev = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-dev.conllu'\n",
    "dataset_path_test = 'ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-test.conllu'\n",
    "\n",
    "dataset_train = conllu.parse_incr(open(dataset_path_train))\n",
    "dataset_dev = conllu.parse_incr(open(dataset_path_dev))\n",
    "dataset_test = conllu.parse_incr(open(dataset_path_test))\n",
    "\n",
    "print(\"Number of sentences in training dataset: \", len(list(dataset_train)))\n",
    "print(\"Number of sentences in dev dataset: \", len(list(dataset_dev)))\n",
    "print(\"Number of sentences in test dataset: \", len(list(dataset_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  form lemma  upos  xpos  \\\n",
      "0   1  what  what  PRON  None   \n",
      "1   2    is    be   AUX  None   \n",
      "2   3   the   the   DET  None   \n",
      "3   4  cost  cost  NOUN  None   \n",
      "4   5    of    of   ADP  None   \n",
      "\n",
      "                                               feats  head deprel  deps  misc  \n",
      "0                            {'PronType': 'Int,Rel'}     0   root  None  None  \n",
      "1  {'Mood': 'Ind', 'Number': 'Sing', 'Person': '3...     1    cop  None  None  \n",
      "2                                {'PronType': 'Art'}     4    det  None  None  \n",
      "3                                 {'Number': 'Sing'}     1  nsubj  None  None  \n",
      "4                                               None     7   case  None  None  \n"
     ]
    }
   ],
   "source": [
    "# bring the pointer back to the beginning of the file\n",
    "dataset_train = conllu.parse_incr(open(dataset_path_train))\n",
    "dataset_dev = conllu.parse_incr(open(dataset_path_dev))\n",
    "dataset_test = conllu.parse_incr(open(dataset_path_test))\n",
    "\n",
    "# convert this data to a pandas dataframe\n",
    "def conllu_to_pandas(dataset):\n",
    "    data = []\n",
    "    for tokenlist in dataset:\n",
    "        for token in tokenlist:\n",
    "            data.append(token)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_train = conllu_to_pandas(dataset_train)\n",
    "df_dev = conllu_to_pandas(dataset_dev)\n",
    "df_test = conllu_to_pandas(dataset_test)\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the training set:  863\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary of the words in the training set\n",
    "vocab = df_train['form'].unique()\n",
    "print(\"Number of unique words in the training set: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the training set after adding <s>, </s>, <unk>:  866\n"
     ]
    }
   ],
   "source": [
    "# add <s>, </s> and <unk> to the vocabulary\n",
    "vocab = ['<s>', '</s>', '<unk>'] + list(vocab)\n",
    "print(\"Number of unique words in the training set after adding <s>, </s>, <unk>: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique upos in the training set:  13\n",
      "['PRON' 'AUX' 'DET' 'NOUN' 'ADP' 'PROPN' 'VERB' 'NUM' 'ADJ' 'CCONJ' 'ADV'\n",
      " 'PART' 'INTJ']\n"
     ]
    }
   ],
   "source": [
    "# finding all unique pos tags\n",
    "upos = df_train['upos'].unique()\n",
    "print(\"Number of unique upos in the training set: \", len(upos))\n",
    "print(upos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique upos in the training set after adding STRT, END, UNK:  16\n"
     ]
    }
   ],
   "source": [
    "# adding pos tags for <s>, </s>, <unk> to the upos\n",
    "upos = ['STRT', 'END', 'UNK'] + list(upos)\n",
    "\n",
    "print(\"Number of unique upos in the training set after adding STRT, END, UNK: \", len(upos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s>': 0, '</s>': 1, '<unk>': 2, 'what': 3, 'is': 4, 'the': 5, 'cost': 6, 'of': 7, 'a': 8, 'round': 9, 'trip': 10, 'flight': 11, 'from': 12, 'pittsburgh': 13, 'to': 14, 'atlanta': 15, 'beginning': 16, 'on': 17, 'april': 18, 'twenty': 19, 'fifth': 20, 'and': 21, 'returning': 22, 'may': 23, 'sixth': 24, 'now': 25, 'i': 26, 'need': 27, 'leaving': 28, 'fort': 29, 'worth': 30, 'arriving': 31, 'in': 32, 'denver': 33, 'no': 34, 'later': 35, 'than': 36, '2': 37, 'pm': 38, 'next': 39, 'monday': 40, 'fly': 41, 'kansas': 42, 'city': 43, 'chicago': 44, 'wednesday': 45, 'following': 46, 'day': 47, 'meaning': 48, 'meal': 49, 'code': 50, 's': 51, 'show': 52, 'me': 53, 'all': 54, 'flights': 55, 'which': 56, 'serve': 57, 'for': 58, 'after': 59, 'tomorrow': 60, 'us': 61, 'air': 62, 'list': 63, 'nonstop': 64, 'early': 65, 'tuesday': 66, 'morning': 67, 'dallas': 68, 'st.': 69, 'petersburg': 70, 'toronto': 71, 'that': 72, 'arrive': 73, 'listing': 74, 'new': 75, 'york': 76, 'montreal': 77, 'canada': 78, 'departing': 79, 'thursday': 80, 'american': 81, 'airlines': 82, 'ontario': 83, 'with': 84, 'stopover': 85, 'louis': 86, 'ground': 87, 'transportation': 88, 'houston': 89, 'afternoon': 90, 'schedule': 91, 'philadelphia': 92, 'san': 93, 'francisco': 94, 'evening': 95, 'diego': 96, 'layover': 97, 'washington': 98, 'dc': 99, 'are': 100, 'there': 101, 'any': 102, 'boston': 103, 'stop': 104, 'restrictions': 105, 'cheapest': 106, 'one': 107, 'way': 108, 'fare': 109, 'between': 110, 'oakland': 111, 'airfare': 112, '416': 113, 'dollars': 114, \"'s\": 115, 'restriction': 116, 'ap68': 117, 'california': 118, 'airports': 119, 'available': 120, 'texas': 121, 'airport': 122, 'closest': 123, 'nevada': 124, 'arizona': 125, 'actually': 126, 'las': 127, 'vegas': 128, 'burbank': 129, 'saturday': 130, 'two': 131, 'how': 132, 'many': 133, 'going': 134, 'july': 135, 'seventh': 136, 'would': 137, 'like': 138, 'an': 139, 'february': 140, 'eighth': 141, 'before': 142, '9': 143, 'am': 144, 'second': 145, 'late': 146, 'okay': 147, 'june': 148, 'first': 149, \"'d\": 150, 'go': 151, 'phoenix': 152, 'detroit': 153, 'milwaukee': 154, 'indianapolis': 155, 'does': 156, 'ls': 157, 'stand': 158, 'designate': 159, 'as': 160, 'baltimore': 161, '1115': 162, '1245': 163, 'miami': 164, 'daily': 165, '8': 166, 'trans': 167, 'world': 168, 'airline': 169, '1030': 170, '1130': 171, '5': 172, '730': 173, 'leave': 174, 'charlotte': 175, 'north': 176, 'carolina': 177, '4': 178, 'find': 179, 'newark': 180, 'jersey': 181, 'cleveland': 182, 'ohio': 183, 'do': 184, 'you': 185, 'have': 186, 'connect': 187, 'international': 188, 'minneapolis': 189, 'rental': 190, 'cars': 191, \"'ll\": 192, 'rent': 193, 'car': 194, 'sort': 195, 'near': 196, 'fine': 197, 'can': 198, 'give': 199, 'information': 200, 'downtown': 201, 'economy': 202, 'class': 203, 'fares': 204, 'december': 205, 'sixteenth': 206, 'codes': 207, 'belong': 208, 'coach': 209, 'night': 210, 'service': 211, 'november': 212, 'twelfth': 213, 'eleventh': 214, 'want': 215, 'know': 216, 'or': 217, '1': 218, \"o'clock\": 219, '3': 220, '6': 221, '10': 222, 'august': 223, 'display': 224, 'depart': 225, 'please': 226, 'snacks': 227, 'served': 228, 'tower': 229, 'types': 230, 'meals': 231, 'ever': 232, 'my': 233, 'options': 234, '382': 235, 'get': 236, 'bwi': 237, 'eastern': 238, '210': 239, 'delta': 240, '852': 241, 'latest': 242, 'return': 243, 'same': 244, 'back': 245, 'most': 246, 'hours': 247, 'take': 248, 'so': 249, 'when': 250, 'will': 251, 'maximum': 252, 'amount': 253, 'time': 254, 'still': 255, 'earliest': 256, 'departure': 257, 'be': 258, 'travel': 259, 'at': 260, 'around': 261, '7': 262, 'route': 263, 'lastest': 264, 'longest': 265, 'but': 266, 'possible': 267, 'only': 268, 'weekdays': 269, 'red': 270, 'eye': 271, 'los': 272, 'angeles': 273, 'ten': 274, 'people': 275, 'during': 276, 'week': 277, 'days': 278, 'out': 279, 'arrives': 280, 'salt': 281, 'lake': 282, 'cincinnati': 283, 'area': 284, 'explain': 285, 'ap': 286, '57': 287, '20': 288, 'mean': 289, '80': 290, 'twa': 291, '497766': 292, 'has': 293, 'stops': 294, 'friday': 295, '705': 296, 'number': 297, 'book': 298, 'least': 299, '813': 300, 'goes': 301, 'straight': 302, 'through': 303, 'without': 304, 'stopping': 305, 'another': 306, 'florida': 307, 'tell': 308, 'about': 309, 'by': 310, 'memphis': 311, 'tennessee': 312, 'noon': 313, '530': 314, 'off': 315, 'love': 316, 'field': 317, 'united': 318, 'la': 319, 'guardia': 320, 'jfk': 321, 'mco': 322, 'sfo': 323, '1991': 324, 'orlando': 325, 'lowest': 326, 'dfw': 327, 'ticket': 328, 'oak': 329, 'atl': 330, 'logan': 331, 'march': 332, 'numbers': 333, 'expensive': 334, 'continental': 335, 'leaves': 336, '1220': 337, 'seattle': 338, 'columbus': 339, 'minnesota': 340, 'those': 341, 'via': 342, 'rentals': 343, 'sunday': 344, 'rates': 345, 'costs': 346, 'limousine': 347, 'taxi': 348, 'operation': 349, 'ap80': 350, 'ap57': 351, 'ninth': 352, '12': 353, 'america': 354, 'west': 355, 'could': 356, 'fifteenth': 357, 'serves': 358, 'dinner': 359, 'provided': 360, 'cities': 361, 'where': 362, 'lester': 363, 'pearson': 364, 'canadian': 365, 'other': 366, 'earlier': 367, '1017': 368, 'northwest': 369, 'general': 370, 'mitchell': 371, 'located': 372, 'both': 373, 'nationair': 374, 'midwest': 375, 'express': 376, 'flies': 377, 'zone': 378, 'flying': 379, 'into': 380, 'much': 381, 'price': 382, 'it': 383, 'tacoma': 384, 'anywhere': 385, '1850': 386, 'midnight': 387, 'january': 388, '1992': 389, 'not': 390, 'exceeding': 391, '300': 392, 'tenth': 393, '1993': 394, '1505': 395, 'october': 396, '1994': 397, 'carries': 398, 'smallest': 399, 'passengers': 400, 'thirty': 401, 'third': 402, 'arrival': 403, 'schedules': 404, 'times': 405, 'your': 406, '269': 407, '428': 408, 'westchester': 409, 'county': 410, 'right': 411, 'september': 412, 'twentieth': 413, 'f28': 414, '755': 415, 'nights': 416, 'their': 417, 'prices': 418, '1039': 419, 'less': 420, '1100': 421, 'nashville': 422, 'again': 423, 'repeat': 424, 'make': 425, 'iah': 426, 'ord': 427, 'ewr': 428, 'dca': 429, 'cvg': 430, 'bna': 431, 'mci': 432, 'hou': 433, 'lga': 434, 'lax': 435, 'yyz': 436, 'bur': 437, 'long': 438, 'distance': 439, 'far': 440, 'paul': 441, 'miles': 442, 'name': 443, 'serviced': 444, 'regarding': 445, 'tampa': 446, 'names': 447, 'describe': 448, 'nineteenth': 449, 'seating': 450, 'capacity': 451, 'fourteenth': 452, 'aircraft': 453, 'largest': 454, 'plane': 455, 'eight': 456, 'sixteen': 457, 'departures': 458, 'seventeenth': 459, 'arrivals': 460, 'type': 461, 'greatest': 462, 'more': 463, 'business': 464, 'total': 465, 'instead': 466, 'besides': 467, 'turboprop': 468, '1059': 469, 'advertises': 470, 'having': 471, 'land': 472, 'various': 473, 'dulles': 474, 'boeing': 475, '767': 476, '466': 477, '329': 478, 'under': 479, '932': 480, '1000': 481, '200': 482, '124': 483, 'along': 484, 'equal': 485, '150': 486, 'each': 487, '400': 488, 'fit': 489, '72s': 490, 'airplane': 491, 'l1011': 492, 'hold': 493, '733': 494, 'airplanes': 495, 'uses': 496, '73s': 497, 'seats': 498, '734': 499, 'm80': 500, 'l10': 501, 'carried': 502, 'capacities': 503, '757': 504, 'planes': 505, 'd9s': 506, '100': 507, 'thrift': 508, 'level': 509, 'see': 510, 'thirtieth': 511, '505': 512, '163': 513, 'tonight': 514, 'connecting': 515, 'also': 516, 'making': 517, \"'m\": 518, 'looking': 519, 'hopefully': 520, 'makes': 521, 'yes': 522, 'breakfast': 523, 'direct': 524, 'itinerary': 525, 'departs': 526, '1940': 527, 'connects': 528, 'provide': 529, 'used': 530, 'including': 531, 'connections': 532, 'if': 533, 'either': 534, 'preferably': 535, 'local': 536, 'beach': 537, 'then': 538, 'mornings': 539, 'four': 540, 'combination': 541, 'thank': 542, 'using': 543, 'well': 544, 'run': 545, 'colorado': 546, 'fourth': 547, 'who': 548, 'sure': 549, 'determine': 550, 'use': 551, '1765': 552, 'lufthansa': 553, 'eighteenth': 554, 'f': 555, 'today': 556, 'come': 557, '320': 558, 'booking': 559, 'k': 560, 'classes': 561, 'yn': 562, 'j31': 563, 'different': 564, 'dh8': 565, 'minimum': 566, 'connection': 567, 'intercontinental': 568, 'last': 569, 'aa': 570, '459': 571, 'limousines': 572, 'services': 573, 'jose': 574, 'too': 575, 'train': 576, 'stapleton': 577, 'limo': 578, 'georgia': 579, 'pennsylvania': 580, 'utah': 581, 'missouri': 582, 'interested': 583, 'shortest': 584, 'quebec': 585, 'michigan': 586, 'indiana': 587, 'this': 588, 'wednesdays': 589, '82': 590, '139': 591, 'tickets': 592, 'sounds': 593, 'great': 594, 'let': 595, 'takeoffs': 596, 'landings': 597, 'grounds': 598, 'offer': 599, 'transport': 600, 'kind': 601, 'hi': 602, 'calling': 603, 'coming': 604, 'soon': 605, 'thereafter': 606, 'anything': 607, 'bring': 608, 'up': 609, 'y': 610, 'm': 611, 'difference': 612, 'q': 613, 'b': 614, 'qo': 615, 'qw': 616, 'qx': 617, 'fn': 618, 'qualify': 619, 'h': 620, 'basis': 621, 'bh': 622, 'offers': 623, 'included': 624, 'serving': 625, 'trying': 626, 'include': 627, 'whether': 628, 'offered': 629, 'ua': 630, '270': 631, 'being': 632, '747': 633, 'be1': 634, '737': 635, 'very': 636, 'working': 637, 'scenario': 638, 'three': 639, '727': 640, 'called': 641, 'dc10': 642, 'abbreviation': 643, 'd10': 644, 'includes': 645, '296': 646, 'should': 647, 'lunch': 648, '343': 649, 'travels': 650, 'snack': 651, 'supper': 652, '838': 653, '1110': 654, 'reaches': 655, 'sometime': 656, 'some': 657, 'reaching': 658, 'saturdays': 659, 'vicinity': 660, 'good': 661, '1800': 662, 'overnight': 663, 'final': 664, 'destination': 665, 'over': 666, 'summer': 667, '297': 668, '1222': 669, '281': 670, 'listed': 671, 'dl': 672, '1055': 673, '405': 674, '201': 675, '315': 676, '21': 677, '486': 678, '825': 679, '555': 680, '1207': 681, '1500': 682, '639': 683, '217': 684, '71': 685, '106': 686, '539': 687, '3724': 688, '271': 689, '1291': 690, '4400': 691, '3357': 692, '345': 693, '771': 694, 'co': 695, '1209': 696, 'ea': 697, '212': 698, '257': 699, '608': 700, '746': 701, 'taking': 702, '311': 703, '417': 704, 'try': 705, 'inform': 706, 'kinds': 707, 'traveling': 708, '419': 709, 'they': 710, 'these': 711, 'kindly': 712, 'proper': 713, 'town': 714, 'takeoff': 715, 'kennedy': 716, 'close': 717, '230': 718, 'nonstops': 719, 'thursdays': 720, \"'re\": 721, '1230': 722, '1200': 723, 'within': 724, 'reservation': 725, 'friends': 726, 'visit': 727, 'here': 728, 'them': 729, 'lives': 730, '0900': 731, '1600': 732, '11': 733, 'we': 734, 'nighttime': 735, 'southwest': 736, 'usa': 737, 'able': 738, 'put': 739, '630': 740, 'nw': 741, 'hp': 742, 'define': 743, 'ff': 744, 'symbols': 745, 'stands': 746, 'kw': 747, 'sam': 748, 'ac': 749, '718': 750, 'wn': 751, 'arrangements': 752, 'sorry': 753, 'must': 754, 'originating': 755, '225': 756, '1158': 757, 'equipment': 758, 'choices': 759, '1205': 760, '1145': 761, 'abbreviations': 762, 'jet': 763, 'companies': 764, 'continuing': 765, 'represented': 766, 'database': 767, 'single': 768, 'rate': 769, 'trips': 770, 'stopovers': 771, 'directly': 772, 'starting': 773, 'afterwards': 774, 'reservations': 775, 'scheduled': 776, 'seat': 777, 'india': 778, 'buy': 779, 'six': 780, '1700': 781, 'say': 782, 'mealtime': 783, '2100': 784, 'economic': 785, 'wish': 786, 'discount': 787, 'staying': 788, 'while': 789, 'look': 790, 'across': 791, 'continent': 792, 'transcontinental': 793, 'begins': 794, 'lands': 795, 'landing': 796, 'month': 797, 'help': 798, '720': 799, '110': 800, 'such': 801, '1045': 802, '934': 803, 'heading': 804, 'toward': 805, '430': 806, 'approximately': 807, '324': 808, '1300': 809, '723': 810, '1020': 811, '645': 812, 'weekday': 813, 'inexpensive': 814, 'thing': 815, 'cheap': 816, 'thanks': 817, 'question': 818, 'live': 819, 'spend': 820, 'seventeen': 821, 'highest': 822, 'priced': 823, 'charges': 824, 'dinnertime': 825, '305': 826, '845': 827, 'noontime': 828, '1026': 829, '823': 830, '2134': 831, '1024': 832, '130': 833, \"'ve\": 834, 'got': 835, 'somebody': 836, 'else': 837, 'wants': 838, '420': 839, 'seven': 840, 'catch': 841, 'fifteen': 842, 'thirteenth': 843, 'tuesdays': 844, 'midway': 845, 'alaska': 846, 'arrange': 847, 'plan': 848, 'oh': 849, 'hello': 850, \"n't\": 851, 'prefer': 852, 'requesting': 853, 'comes': 854, 'mondays': 855, 'bound': 856, 'fridays': 857, 'sundays': 858, 'bay': 859, 'planning': 860, 'home': 861, 'reverse': 862, 'order': 863, 'a.m.': 864, 'philly': 865}\n",
      "Embedding(866, 100)\n",
      "4274\n",
      "[3, 25, 26, 3, 52, 52, 63, 52, 26, 52, 25, 52, 52, 26, 100, 26, 3, 3, 3, 63, 3, 56, 63, 126, 132, 26, 52, 4, 147, 26, 26, 3, 3, 3, 52, 63, 63, 63, 63, 52, 63, 26, 3, 26, 55, 3, 55, 52, 55, 190, 26, 3, 52, 52, 197, 52, 3, 198, 132, 3, 3, 54, 54, 26, 54, 63, 224, 226, 63, 3, 226, 55, 100, 3, 100, 3, 3, 3, 184, 52, 4, 179, 132, 3, 179, 179, 26, 179, 26, 63, 3, 3, 209, 26, 26, 26, 3, 3, 3, 3, 3, 3, 226, 179, 54, 52, 285, 3, 3, 3, 3, 285, 3, 3, 3, 4, 3, 4, 63, 26, 26, 92, 4, 52, 52, 26, 63, 3, 52, 26, 45, 308, 52, 52, 199, 100, 63, 52, 308, 3, 52, 55, 55, 3, 56, 52, 323, 52, 3, 3, 3, 132, 3, 26, 26, 3, 11, 26, 3, 11, 226, 23, 11, 11, 11, 11, 26, 63, 11, 3, 3, 26, 198, 3, 190, 3, 3, 3, 3, 4, 198, 4, 198, 52, 3, 52, 285, 285, 3, 3, 63, 52, 5, 56, 55, 52, 4, 3, 63, 52, 55, 3, 52, 52, 3, 52, 3, 54, 3, 356, 3, 52, 3, 3, 156, 3, 63, 3, 362, 3, 52, 100, 3, 362, 362, 56, 52, 52, 63, 3, 3, 3, 52, 3, 52, 52, 52, 100, 179, 3, 3, 132, 132, 132, 132, 3, 132, 132, 132, 132, 3, 3, 52, 55, 226, 4, 52, 3, 52, 3, 56, 3, 3, 3, 52, 52, 3, 3, 3, 52, 52, 11, 63, 226, 17, 26, 3, 226, 61, 226, 3, 3, 52, 54, 226, 3, 3, 52, 63, 226, 52, 226, 55, 226, 26, 63, 54, 52, 3, 3, 52, 52, 54, 54, 26, 63, 63, 26, 63, 149, 423, 52, 199, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 226, 3, 3, 3, 3, 226, 3, 3, 132, 3, 132, 132, 132, 63, 3, 3, 132, 63, 132, 3, 132, 132, 132, 132, 63, 132, 132, 132, 63, 3, 132, 3, 63, 156, 226, 52, 63, 63, 308, 119, 226, 63, 308, 3, 3, 447, 3, 3, 26, 448, 56, 52, 3, 3, 226, 3, 56, 3, 3, 56, 3, 56, 199, 3, 3, 466, 7, 3, 3, 3, 4, 52, 3, 3, 52, 52, 52, 26, 52, 52, 3, 52, 226, 52, 63, 63, 63, 26, 132, 226, 3, 226, 63, 52, 52, 21, 52, 63, 56, 52, 9, 52, 52, 52, 9, 9, 52, 55, 9, 63, 9, 63, 4, 226, 52, 54, 9, 9, 55, 199, 52, 9, 132, 132, 132, 3, 3, 132, 132, 3, 3, 3, 3, 3, 3, 132, 3, 63, 3, 63, 3, 132, 3, 3, 3, 3, 3, 3, 132, 132, 54, 9, 52, 3, 100, 3, 198, 198, 198, 52, 52, 3, 3, 3, 26, 3, 52, 52, 132, 147, 52, 26, 52, 127, 26, 13, 3, 26, 4, 147, 26, 26, 100, 179, 26, 56, 26, 4, 52, 26, 26, 522, 3, 26, 26, 26, 179, 26, 26, 522, 26, 26, 4, 26, 26, 52, 26, 100, 4, 3, 4, 100, 52, 4, 52, 52, 52, 52, 52, 4, 4, 56, 26, 100, 52, 52, 226, 26, 52, 52, 26, 4, 100, 63, 63, 52, 26, 52, 132, 538, 26, 26, 26, 52, 26, 17, 100, 26, 26, 52, 147, 26, 199, 26, 26, 26, 26, 63, 3, 26, 26, 147, 26, 147, 52, 63, 199, 17, 199, 26, 26, 3, 542, 3, 55, 63, 55, 63, 63, 56, 132, 26, 55, 3, 63, 63, 63, 63, 63, 55, 63, 55, 63, 356, 63, 63, 63, 180, 63, 63, 63, 165, 52, 63, 63, 52, 26, 4, 63, 26, 3, 52, 63, 63, 3, 3, 100, 226, 100, 549, 26, 226, 3, 100, 3, 3, 63, 147, 226, 26, 63, 63, 3, 63, 3, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 226, 132, 132, 63, 132, 132, 63, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 25, 3, 3, 26, 3, 3, 3, 52, 3, 3, 3, 3, 3, 226, 3, 198, 26, 3, 3, 3, 52, 26, 3, 52, 3, 3, 100, 198, 576, 3, 26, 198, 26, 198, 3, 32, 26, 4, 26, 26, 54, 63, 26, 584, 26, 52, 226, 226, 26, 26, 226, 63, 3, 63, 200, 26, 26, 63, 63, 26, 26, 52, 226, 3, 26, 3, 17, 63, 26, 3, 199, 3, 3, 26, 52, 56, 3, 3, 3, 3, 26, 3, 3, 3, 3, 26, 63, 5, 3, 3, 52, 3, 3, 26, 3, 3, 52, 63, 3, 3, 3, 3, 3, 3, 147, 52, 3, 52, 52, 26, 52, 52, 63, 3, 3, 63, 226, 3, 3, 63, 226, 63, 4, 156, 52, 4, 156, 4, 4, 52, 3, 3, 132, 3, 3, 308, 3, 4, 52, 87, 3, 52, 4, 52, 4, 26, 3, 3, 87, 87, 3, 226, 87, 26, 3, 32, 52, 52, 308, 87, 260, 52, 3, 26, 602, 52, 52, 52, 52, 55, 26, 26, 3, 56, 32, 63, 52, 3, 52, 52, 52, 52, 52, 199, 52, 26, 26, 26, 52, 52, 26, 607, 236, 608, 3, 26, 109, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 63, 3, 3, 3, 3, 3, 3, 3, 3, 3, 63, 3, 3, 3, 3, 3, 3, 3, 3, 3, 226, 3, 3, 3, 285, 3, 3, 285, 448, 52, 26, 199, 26, 226, 3, 52, 63, 3, 199, 26, 52, 3, 156, 3, 199, 63, 52, 52, 5, 179, 226, 52, 32, 17, 3, 199, 5, 63, 3, 199, 179, 26, 184, 63, 52, 308, 636, 356, 100, 3, 184, 3, 26, 26, 308, 3, 156, 156, 100, 142, 3, 52, 226, 63, 3, 26, 356, 179, 26, 3, 3, 4, 3, 17, 156, 198, 26, 52, 3, 179, 52, 179, 26, 3, 3, 26, 3, 3, 3, 3, 52, 226, 26, 52, 156, 26, 3, 226, 52, 3, 26, 3, 52, 26, 25, 569, 56, 26, 26, 26, 52, 52, 26, 26, 26, 25, 52, 26, 226, 198, 56, 63, 3, 26, 52, 26, 26, 26, 226, 63, 26, 52, 56, 4, 356, 52, 92, 26, 56, 56, 63, 26, 63, 179, 4, 11, 63, 226, 3, 63, 63, 55, 26, 54, 52, 3, 132, 26, 55, 3, 3, 52, 26, 63, 63, 63, 63, 26, 179, 3, 198, 3, 26, 52, 11, 156, 26, 156, 26, 3, 26, 519, 3, 519, 3, 12, 132, 4, 256, 3, 26, 63, 3, 26, 26, 52, 26, 3, 132, 3, 3, 156, 52, 226, 3, 3, 52, 17, 52, 52, 17, 3, 132, 3, 4, 3, 308, 52, 61, 3, 3, 26, 3, 3, 81, 52, 226, 198, 61, 238, 61, 3, 672, 52, 3, 61, 3, 17, 3, 362, 52, 132, 198, 132, 11, 132, 3, 3, 3, 28, 3, 52, 3, 63, 544, 3, 147, 3, 3, 52, 63, 3, 3, 226, 3, 3, 3, 226, 3, 3, 26, 26, 224, 3, 3, 3, 550, 63, 26, 3, 3, 3, 3, 4, 52, 54, 26, 712, 26, 26, 3, 3, 3, 3, 63, 3, 308, 3, 63, 52, 308, 132, 52, 4, 63, 63, 4, 4, 52, 52, 132, 356, 3, 184, 3, 3, 3, 21, 132, 52, 52, 4, 132, 26, 3, 52, 4, 226, 26, 63, 63, 3, 4, 52, 63, 4, 132, 3, 63, 52, 63, 52, 198, 26, 3, 199, 26, 26, 52, 25, 52, 26, 11, 3, 52, 26, 63, 26, 3, 56, 3, 52, 3, 26, 3, 4, 26, 226, 52, 52, 26, 52, 199, 26, 198, 226, 52, 26, 3, 3, 226, 3, 3, 199, 52, 3, 26, 26, 26, 3, 3, 4, 199, 52, 63, 52, 55, 63, 56, 199, 63, 63, 26, 3, 199, 63, 26, 52, 3, 199, 100, 63, 63, 26, 26, 5, 3, 3, 199, 3, 63, 63, 63, 63, 52, 3, 3, 63, 3, 52, 11, 63, 3, 52, 147, 26, 236, 100, 52, 8, 8, 52, 198, 92, 226, 39, 56, 3, 61, 26, 52, 26, 3, 199, 3, 3, 39, 226, 179, 26, 25, 54, 52, 56, 56, 52, 26, 226, 26, 3, 308, 198, 147, 226, 226, 52, 199, 52, 542, 26, 26, 184, 184, 26, 8, 199, 52, 56, 3, 26, 198, 52, 26, 56, 3, 52, 63, 52, 199, 3, 55, 3, 52, 156, 56, 52, 26, 26, 199, 184, 156, 54, 55, 26, 55, 52, 52, 54, 56, 52, 52, 52, 200, 3, 52, 199, 3, 26, 63, 3, 12, 3, 52, 3, 52, 52, 52, 54, 52, 56, 54, 26, 63, 63, 52, 52, 54, 4, 52, 52, 52, 226, 132, 26, 52, 55, 26, 26, 17, 23, 26, 26, 3, 3, 55, 26, 26, 23, 11, 17, 26, 226, 3, 55, 3, 11, 63, 52, 602, 3, 23, 3, 56, 23, 52, 26, 52, 11, 156, 3, 147, 3, 56, 26, 26, 63, 226, 3, 224, 3, 155, 3, 26, 3, 3, 17, 11, 26, 226, 3, 52, 224, 106, 199, 26, 3, 26, 3, 17, 52, 15, 56, 4, 3, 52, 199, 52, 3, 3, 17, 26, 137, 23, 308, 199, 3, 52, 740, 106, 308, 63, 3, 224, 184, 356, 132, 3, 26, 56, 17, 199, 12, 3, 4, 106, 226, 26, 3, 3, 3, 743, 3, 3, 3, 3, 3, 285, 3, 3, 52, 3, 3, 156, 3, 356, 3, 3, 3, 3, 26, 26, 3, 3, 3, 52, 26, 52, 4, 56, 3, 52, 52, 52, 56, 3, 209, 3, 3, 3, 3, 169, 3, 226, 26, 26, 3, 156, 226, 132, 3, 3, 26, 56, 26, 3, 3, 52, 3, 3, 3, 743, 3, 3, 4, 4, 55, 3, 3, 26, 26, 26, 179, 3, 52, 3, 179, 63, 26, 100, 3, 3, 3, 11, 52, 17, 147, 4, 52, 226, 179, 56, 26, 63, 52, 3, 179, 3, 26, 26, 179, 522, 26, 3, 3, 56, 199, 3, 26, 179, 3, 26, 26, 52, 26, 3, 52, 3, 25, 224, 26, 63, 26, 52, 56, 26, 17, 3, 52, 3, 56, 26, 26, 26, 26, 52, 26, 52, 226, 52, 226, 156, 56, 3, 26, 56, 56, 52, 3, 56, 3, 226, 56, 3, 56, 63, 184, 3, 3, 356, 226, 3, 52, 63, 156, 3, 56, 56, 56, 56, 3, 52, 52, 3, 56, 226, 56, 3, 3, 63, 56, 3, 198, 226, 226, 226, 3, 63, 52, 3, 52, 3, 52, 156, 3, 4, 82, 3, 56, 52, 226, 56, 52, 56, 3, 199, 198, 3, 3, 52, 3, 226, 3, 4, 56, 198, 4, 52, 308, 56, 63, 52, 3, 56, 3, 52, 56, 3, 56, 52, 199, 226, 63, 52, 52, 198, 26, 179, 52, 52, 64, 4, 9, 3, 179, 52, 226, 26, 52, 63, 63, 52, 199, 52, 52, 26, 52, 52, 100, 52, 26, 26, 52, 26, 64, 3, 199, 64, 52, 3, 198, 26, 132, 52, 4, 52, 147, 52, 63, 63, 52, 63, 199, 52, 23, 139, 3, 52, 52, 149, 3, 26, 52, 52, 52, 132, 200, 132, 156, 147, 52, 52, 52, 3, 26, 132, 52, 3, 226, 3, 3, 52, 52, 3, 107, 226, 63, 26, 356, 3, 3, 54, 56, 52, 132, 3, 3, 52, 3, 52, 52, 3, 26, 3, 52, 52, 52, 52, 132, 226, 3, 3, 132, 26, 3, 52, 26, 52, 3, 226, 149, 3, 3, 52, 189, 52, 200, 3, 3, 26, 54, 63, 52, 9, 132, 74, 26, 52, 26, 52, 52, 226, 52, 3, 52, 26, 52, 26, 26, 3, 179, 26, 52, 52, 52, 26, 26, 52, 52, 308, 3, 3, 3, 3, 149, 3, 3, 52, 3, 3, 56, 3, 3, 54, 199, 26, 132, 3, 63, 87, 87, 198, 3, 3, 3, 156, 26, 3, 87, 52, 3, 52, 26, 52, 3, 32, 63, 3, 156, 308, 52, 226, 26, 63, 3, 52, 3, 3, 87, 3, 52, 52, 156, 52, 3, 3, 3, 3, 87, 68, 4, 15, 156, 52, 4, 52, 3, 87, 156, 52, 87, 4, 3, 789, 32, 87, 32, 226, 52, 3, 226, 26, 3, 87, 52, 4, 26, 3, 87, 26, 4, 132, 156, 87, 3, 200, 52, 3, 87, 448, 3, 3, 3, 87, 3, 52, 3, 103, 3, 3, 52, 63, 26, 26, 100, 198, 56, 4, 56, 3, 54, 602, 26, 26, 63, 26, 56, 26, 26, 26, 26, 5, 63, 198, 184, 318, 26, 184, 26, 26, 26, 3, 26, 26, 54, 3, 3, 179, 56, 52, 184, 56, 56, 100, 52, 52, 4, 179, 3, 179, 3, 26, 26, 26, 26, 369, 184, 52, 32, 100, 56, 3, 224, 26, 184, 224, 318, 26, 56, 4, 52, 52, 26, 179, 3, 26, 26, 54, 26, 56, 3, 179, 522, 26, 26, 26, 52, 93, 52, 26, 3, 26, 132, 26, 63, 52, 26, 3, 156, 318, 63, 52, 184, 3, 4, 356, 52, 179, 52, 26, 26, 3, 26, 100, 226, 179, 4, 3, 3, 198, 198, 4, 132, 26, 4, 3, 26, 200, 87, 87, 3, 198, 3, 3, 52, 179, 63, 52, 26, 3, 26, 26, 17, 4, 54, 226, 52, 26, 3, 100, 52, 17, 226, 179, 26, 3, 59, 21, 52, 3, 26, 179, 63, 63, 63, 26, 52, 26, 52, 26, 12, 54, 602, 198, 226, 3, 52, 26, 56, 52, 55, 17, 226, 63, 224, 63, 26, 26, 3, 54, 26, 52, 100, 174, 52, 63, 17, 52, 3, 3, 226, 69, 147, 3, 63, 26, 52, 55, 179, 3, 26, 3, 26, 3, 52, 184, 3, 55, 4, 308, 26, 542, 3, 52, 52, 52, 26, 226, 198, 52, 226, 54, 3, 184, 52, 26, 26, 3, 3, 356, 17, 3, 52, 179, 238, 602, 26, 26, 26, 3, 63, 3, 15, 339, 100, 52, 26, 3, 3, 63, 199, 26, 52, 63, 26, 100, 184, 3, 52, 52, 52, 63, 54, 52, 63, 4, 226, 26, 26, 63, 4, 4, 63, 3, 63, 63, 26, 52, 3, 3, 226, 3, 3, 3, 52, 52, 63, 26, 52, 3, 179, 3, 52, 3, 3, 52, 3, 3, 26, 3, 3, 63, 3, 52, 3, 242, 147, 3, 52, 179, 3, 26, 3, 52, 27, 3, 149, 139, 3, 63, 52, 3, 3, 3, 63, 3, 236, 584, 3, 3, 198, 3, 3, 226, 26, 3, 26, 52, 356, 26, 356, 3, 3, 52, 52, 3, 3, 26, 236, 3, 26, 3, 3, 26, 52, 3, 3, 3, 3, 3, 3, 3, 356, 198, 3, 308, 3, 63, 3, 52, 3, 3, 26, 26, 3, 3, 26, 198, 3, 817, 516, 3, 26, 3, 3, 3, 26, 28, 3, 3, 26, 3, 3, 198, 3, 3, 3, 3, 26, 52, 3, 443, 3, 233, 3, 250, 147, 58, 52, 3, 63, 26, 26, 3, 26, 3, 179, 3, 3, 3, 3, 256, 226, 584, 26, 3, 3, 3, 250, 250, 3, 52, 813, 52, 584, 63, 26, 3, 106, 3, 63, 3, 52, 3, 3, 26, 3, 52, 52, 52, 3, 26, 3, 26, 106, 3, 3, 63, 179, 3, 26, 379, 26, 3, 3, 3, 26, 26, 3, 52, 3, 3, 26, 3, 3, 26, 326, 52, 52, 26, 26, 179, 356, 26, 3, 198, 106, 106, 52, 63, 179, 132, 3, 199, 326, 26, 199, 3, 3, 3, 52, 52, 3, 26, 3, 26, 356, 3, 3, 26, 52, 3, 3, 3, 52, 3, 26, 3, 17, 3, 199, 52, 3, 226, 52, 26, 3, 52, 106, 3, 3, 3, 26, 3, 3, 179, 356, 179, 3, 26, 52, 226, 3, 52, 3, 3, 226, 3, 54, 198, 52, 26, 3, 3, 3, 3, 3, 52, 106, 198, 3, 137, 3, 3, 3, 52, 3, 3, 3, 3, 3, 106, 3, 52, 52, 26, 52, 3, 52, 356, 26, 3, 52, 3, 3, 26, 26, 26, 63, 3, 63, 3, 226, 52, 3, 226, 3, 63, 26, 3, 3, 3, 52, 68, 63, 63, 55, 26, 356, 3, 26, 26, 89, 3, 226, 21, 63, 63, 3, 226, 52, 3, 52, 3, 54, 11, 356, 26, 52, 52, 198, 9, 356, 55, 3, 198, 26, 199, 3, 63, 226, 226, 52, 26, 3, 3, 52, 26, 52, 9, 179, 26, 63, 198, 9, 52, 52, 52, 3, 226, 26, 602, 3, 3, 179, 226, 8, 26, 147, 26, 356, 107, 52, 3, 52, 63, 147, 132, 9, 26, 26, 3, 25, 63, 52, 54, 52, 63, 52, 3, 25, 63, 26, 179, 52, 52, 52, 52, 52, 52, 52, 132, 356, 226, 52, 52, 52, 52, 26, 226, 26, 4, 17, 26, 26, 226, 3, 63, 26, 63, 26, 63, 3, 52, 52, 13, 522, 224, 26, 17, 226, 17, 26, 198, 52, 522, 26, 226, 26, 3, 52, 56, 56, 233, 100, 26, 184, 55, 56, 26, 3, 52, 26, 199, 52, 52, 26, 26, 52, 52, 63, 52, 26, 522, 52, 3, 56, 26, 3, 226, 156, 3, 17, 52, 3, 26, 224, 26, 26, 224, 55, 63, 226, 52, 52, 100, 26, 184, 11, 52, 26, 226, 17, 26, 224, 26, 3, 26, 3, 26, 26, 3, 100, 3, 63, 26, 26, 17, 26, 226, 26, 55, 3, 3, 3, 26, 26, 52, 3, 63, 3, 236, 226, 382, 226, 52, 3, 52, 204, 3, 52, 52, 3, 382, 132, 198, 132, 132, 236, 52, 52, 132, 132, 54, 3, 3, 26, 52, 236, 132, 3, 3, 3, 204, 52, 3, 3, 52, 236, 204, 204, 236, 52, 3, 52, 204, 224, 3, 52, 52, 3, 132, 52, 63, 3, 3, 26, 156, 63, 200, 26, 199, 4, 63, 3, 63, 4, 362, 3, 55, 3, 226, 3, 226, 147, 63, 199, 52, 226, 243, 199, 54, 63, 26, 147, 156, 226, 226, 226, 63, 81, 3, 63, 26, 3, 26, 156, 156, 199, 63, 156, 26, 26, 156, 63, 184, 52, 3, 52, 3, 63, 26, 132, 26, 26, 52, 63, 199, 156, 849, 52, 156, 26, 199, 63, 26, 63, 3, 156, 147, 379, 26, 52, 52, 26, 52, 156, 26, 81, 3, 17, 52, 199, 3, 26, 26, 4, 226, 3, 63, 56, 200, 147, 156, 199, 52, 156, 226, 52, 52, 3, 184, 100, 4, 63, 4, 199, 56, 52, 226, 8, 52, 199, 850, 156, 4, 130, 52, 3, 100, 156, 26, 63, 52, 26, 156, 156, 156, 26, 55, 226, 602, 3, 226, 63, 147, 63, 52, 4, 81, 56, 3, 3, 63, 63, 199, 44, 56, 52, 199, 182, 3, 63, 3, 52, 3, 3, 3, 3, 52, 67, 52, 198, 3, 52, 52, 272, 3, 226, 132, 90, 26, 26, 52, 26, 200, 52, 52, 26, 42, 226, 226, 63, 52, 21, 52, 26, 199, 52, 63, 52, 3, 52, 63, 63, 55, 56, 156, 3, 156, 44, 67, 226, 199, 226, 52, 4, 26, 52, 226, 3, 3, 52, 226, 40, 132, 52, 63, 26, 26, 52, 3, 3, 63, 144, 26, 3, 3, 226, 3, 52, 226, 52, 3, 52, 250, 3, 226, 26, 26, 3, 164, 26, 52, 26, 199, 26, 52, 3, 226, 38, 52, 26, 26, 26, 100, 3, 55, 12, 52, 52, 3, 138, 3, 75, 26, 67, 75, 26, 199, 26, 3, 3, 26, 3, 26, 3, 311, 63, 54, 63, 272, 52, 67, 3, 3, 45, 3, 272, 55, 52, 63, 52, 226, 11, 226, 13, 3, 52, 52, 52, 26, 63, 17, 40, 3, 63, 63, 226, 26, 3, 66, 198, 26, 63, 3, 55, 26, 226, 52, 26, 3, 3, 55, 226, 199, 26, 226, 63, 356, 26, 199, 26, 226, 25, 12, 26, 26, 52, 52, 3, 11, 52, 26, 199, 199, 63, 63, 226, 52, 26, 38, 3, 3, 52, 81, 52, 26, 17, 226, 55, 3, 52, 54, 226, 198, 17, 52, 26, 184, 26, 199, 3, 199, 3, 52, 63, 52, 516, 52, 52, 55, 356, 26, 52, 26, 52, 26, 26, 226, 26, 26, 26, 52, 26, 63, 226, 3, 52, 52, 52, 132, 52, 52, 226, 26, 52, 54, 52, 55, 56, 26, 52, 179, 4, 184, 226, 12, 63, 226, 81, 52, 236, 63, 3, 63, 55, 3, 63, 93, 63, 198, 198, 52, 52, 3, 23, 308, 240, 52, 55, 63, 52, 26, 226, 52, 226, 63, 52, 26, 11, 63, 3, 26, 156, 130, 3, 52, 52, 3, 26, 3, 63, 26, 26, 3, 26, 226, 199, 26, 226, 26, 3, 56, 52, 226, 52, 75, 55, 52, 226, 3, 52, 52, 55, 26, 199, 55, 226, 63, 3, 199, 226, 26, 55, 26, 226, 52, 130, 26, 63, 52, 52, 132, 52, 26, 63, 226, 26, 3, 52, 55, 52, 3, 55, 52, 52, 55, 3, 226, 52, 3, 3, 56, 3, 52, 362, 179, 3, 63, 3, 156, 41, 156, 156, 3, 100, 100, 52, 44, 3, 52, 156, 81, 63, 156, 3, 240, 63, 26, 3, 52, 52, 52, 100, 850, 52, 199, 63, 184, 226, 3, 63, 52, 52, 132, 4, 3, 3, 25, 3, 156, 156, 26, 199, 56, 52, 81, 4, 26, 184, 199, 3, 63, 240, 63, 52, 156, 55, 3, 226, 26, 52, 3, 52, 63, 3, 3, 63, 52, 152, 52, 3, 26, 55, 26, 3, 52, 147, 26, 12, 45, 63, 63, 3, 103, 356, 52, 26, 175, 68, 92, 3, 63, 63, 3, 52, 3, 52, 52, 52, 52, 152, 3, 52, 52, 853, 25, 226, 63, 52, 226, 52, 3, 179, 63, 26, 26, 63, 52, 3, 52, 55, 63, 55, 3, 179, 199, 63, 68, 26, 63, 52, 853, 26, 63, 152, 179, 52, 15, 52, 63, 52, 17, 226, 344, 26, 92, 3, 52, 26, 52, 26, 13, 3, 52, 154, 295, 26, 26, 3, 180, 199, 26, 179, 3, 52, 55, 25, 189, 152, 3, 180, 226, 52, 92, 3, 63, 55, 226, 52, 26, 26, 68, 52, 199, 3, 3, 26, 26, 52, 3, 26, 92, 52, 26, 3, 199, 52, 3, 111, 52, 3, 52, 3, 3, 52, 3, 17, 63, 226, 3, 226, 55, 26, 52, 26, 308, 52, 52, 3, 198, 26, 26, 3, 226, 63, 26, 356, 25, 26, 12, 198, 26, 52, 52, 52, 63, 52, 55, 3, 26, 26, 52, 11, 26, 55, 26, 55, 55, 55, 26, 184, 226, 26, 52, 52, 26, 3, 63, 226, 52, 52, 52, 198, 734, 63, 52, 55, 519, 26, 26, 52, 52, 26, 55, 3, 26, 226, 26, 519, 26, 26, 52, 3, 63, 226, 54, 11, 52, 3, 3, 3, 52, 26, 26, 26, 26, 3, 26, 226, 63, 12, 52, 26, 200, 63, 26, 52, 3, 26, 226, 200, 3, 3, 519, 52, 56, 26, 52, 52, 26, 63, 52, 26, 52, 26, 63, 26, 4, 52, 3, 226, 63, 23, 52, 63, 3, 522, 11, 54, 519, 52, 11, 54, 26, 26, 12, 92, 200, 26, 26, 26, 26, 3, 147, 54, 52, 3, 3, 63, 26, 100, 26, 199, 26, 26, 52, 26, 54, 63, 52, 52, 26, 11, 26, 52, 26, 26, 226, 26, 26, 26, 55, 52, 52, 226, 52, 52, 52, 184, 26, 52, 52, 63, 52, 103, 63, 52, 52, 200, 184, 41, 63, 3, 52, 226, 52, 3, 3, 3, 3, 26, 26, 52, 26, 26, 3, 52, 23, 3, 52, 26, 308, 3, 68, 52, 56, 3, 3, 26, 199, 26, 3, 63, 52, 26, 55, 26, 52, 3, 3, 13, 198, 26, 26, 3, 55, 198, 52, 52, 11, 52, 52, 3, 602, 11, 52, 3, 3, 138, 55, 26, 52, 55, 63, 52, 55, 226, 226, 52, 226, 56, 147, 55, 3, 226, 26, 52, 226, 26, 52, 63, 198, 52, 55, 55, 26, 26, 26, 52, 26, 226, 3, 26, 12, 26, 52, 26, 26, 26, 179, 26, 147, 199, 3, 26, 54, 52, 52, 63, 26, 147, 542, 3, 356, 26, 26, 12, 26, 3, 26, 52, 236, 26, 26, 3, 111, 52, 52, 52, 52, 3, 356, 52, 26, 52, 3, 52, 519, 52, 26, 52, 52, 199, 132, 54, 595, 26, 52, 11, 52, 3, 147, 3, 3, 52, 26, 198, 3, 52, 198, 11, 3, 52, 3, 63, 52, 356, 3, 3, 26, 3, 199, 52, 52, 26, 52, 44, 52, 52, 52, 52, 12, 63, 3, 226, 26, 63, 179, 52, 56, 25, 52, 63, 55, 3, 52, 63, 3, 26, 52, 52, 100, 26, 25, 26, 55, 52, 26, 63, 26, 11, 26, 55, 26, 52, 55, 52, 52, 356, 3, 199, 55, 226, 55, 226, 26, 3, 152, 52, 3, 52, 132, 52, 3, 52, 26, 132, 63, 63, 26, 52, 3, 226, 26, 198, 26, 63, 3, 26, 63, 26, 63, 52, 26, 63, 52, 198, 3, 52, 26, 52, 250, 52, 161, 226, 52, 26, 27, 55, 52, 200, 26, 3, 356, 26, 52, 52, 184, 549, 52, 3, 100, 52, 52, 26, 26, 26, 198, 26, 52, 3, 26, 33, 26, 26, 52, 26, 3, 26, 26, 52, 63, 189, 63, 63, 52, 26, 26, 26, 26, 52, 52, 52, 54, 54, 52, 26, 52, 850, 25, 52, 26, 52, 3, 56, 226, 200, 26, 26, 26, 52, 63, 11, 3, 55, 26, 356, 52, 26, 52, 3, 11, 26, 26, 3, 63, 63, 63, 3, 26, 26, 55, 52, 3, 100, 52, 52, 52, 26, 3, 52, 52, 52, 26, 3, 226, 52]\n",
      "tensor([  3,  25,  26,  ...,   3, 226,  52], device='mps:0')\n",
      "tensor([[-0.4120, -0.1676,  0.9630,  ...,  0.1590,  2.6042, -0.3595],\n",
      "        [ 0.7394,  1.0108,  0.8930,  ..., -0.6333,  1.2931, -1.0812],\n",
      "        [ 0.3957,  0.0320,  1.2374,  ..., -1.1304,  1.1273,  0.6728],\n",
      "        ...,\n",
      "        [-0.4120, -0.1676,  0.9630,  ...,  0.1590,  2.6042, -0.3595],\n",
      "        [-1.2487, -1.2888,  2.0585,  ..., -0.9645, -0.2871,  0.4951],\n",
      "        [-0.3800,  0.9512, -1.1746,  ..., -0.4860,  2.3120, -0.8005]],\n",
      "       device='mps:0', grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([4274, 100])\n"
     ]
    }
   ],
   "source": [
    "# for the vocab, we must create a nn embedding\n",
    "# we will use the nn.Embedding class from pytorch\n",
    "\n",
    "# create a dictionary to map words to indices\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "print(word_to_idx)\n",
    "\n",
    "embedding = nn.Embedding(len(vocab), 100, device=device)\n",
    "print(embedding)\n",
    "\n",
    "# create a tensor of indices for the words in the first sentence\n",
    "sentence = df_train[df_train['id'] == 1]\n",
    "print(len(sentence))\n",
    "# print(sentence)\n",
    "\n",
    "word_indices = [word_to_idx.get(word, word_to_idx['<unk>']) for word in sentence['form']]\n",
    "print(word_indices)\n",
    "\n",
    "word_indices = torch.tensor(word_indices, dtype=torch.long, device=device)\n",
    "print(word_indices)\n",
    "\n",
    "# pass the tensor of indices to the embedding\n",
    "embedded = embedding(word_indices)\n",
    "print(embedded)\n",
    "print(embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n",
      "[-0.32307  -0.87616   0.21977   0.25268   0.22976   0.7388   -0.37954\n",
      " -0.35307  -0.84369  -1.1113   -0.30266   0.33178  -0.25113   0.30448\n",
      " -0.077491 -0.89815   0.092496 -1.1407   -0.58324   0.66869  -0.23122\n",
      " -0.95855   0.28262  -0.078848  0.75315   0.26584   0.3422   -0.33949\n",
      "  0.95608   0.065641  0.45747   0.39835   0.57965   0.39267  -0.21851\n",
      "  0.58795  -0.55999   0.63368  -0.043983 -0.68731  -0.37841   0.38026\n",
      "  0.61641  -0.88269  -0.12346  -0.37928  -0.38318   0.23868   0.6685\n",
      " -0.43321  -0.11065   0.081723  1.1569    0.78958  -0.21223  -2.3211\n",
      " -0.67806   0.44561   0.65707   0.1045    0.46217   0.19912   0.25802\n",
      "  0.057194  0.53443  -0.43133  -0.34311   0.59789  -0.58417   0.068995\n",
      "  0.23944  -0.85181   0.30379  -0.34177  -0.25746  -0.031101 -0.16285\n",
      "  0.45169  -0.91627   0.64521   0.73281  -0.22752   0.30226   0.044801\n",
      " -0.83741   0.55006  -0.52506  -1.7357    0.4751   -0.70487   0.056939\n",
      " -0.7132    0.089623  0.41394  -1.3363   -0.61915  -0.33089  -0.52881\n",
      "  0.16483  -0.98878 ]\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained word2vec model - wiki 100\n",
    "word2vec = api.load(\"glove-wiki-gigaword-100\")\n",
    "print('loaded')\n",
    "print(word2vec['king'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing label rep. to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the pos tags to one-hot vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLUDataset(Dataset):\n",
    "    def __init__(self, conllu_file):\n",
    "        self.data = self.load_conllu(conllu_file)\n",
    "\n",
    "    def load_conllu(self, conllu_file):\n",
    "        dataset = conllu.parse_incr(open(conllu_file))\n",
    "        data = []\n",
    "        for tokenlist in dataset:\n",
    "            for token in tokenlist:\n",
    "                data.append(token)\n",
    "        dataset = pd.DataFrame(data)\n",
    "        # only retain the columns form and upos\n",
    "        dataset = dataset[['form', 'upos']]\n",
    "        # convert dataset to normal list\n",
    "        dataset = dataset.values.tolist()\n",
    "        return dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'PRON']\n"
     ]
    }
   ],
   "source": [
    "# print a element in the dataset\n",
    "dataset = CoNLLUDataset(dataset_path_train)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find vocabulary and POS tags, as well as load the word embeddings to be used\n",
    "def preprocess_train(df, p=3, s=3, embedding_type='glove-wiki-gigaword-100'):\n",
    "    vocab = set(df['word'])\n",
    "    pos_tags = set(df['pos'])\n",
    "    word_vectors_all = api.load(embedding_type)\n",
    "\n",
    "    word_vectors = {}\n",
    "    for word in vocab:\n",
    "        if word in word_vectors_all:\n",
    "            word_vectors[word] = word_vectors_all[word]\n",
    "        else:\n",
    "            word_vectors[word] = torch.zeros(len(word_vectors_all['the']))\n",
    "\n",
    "    # one hot encode the POS tags\n",
    "    pos_tags_one_hot = {}\n",
    "    for i, tag in enumerate(pos_tags):\n",
    "        one_hot = torch.zeros(len(pos_tags))\n",
    "        one_hot[i] = 1\n",
    "        pos_tags_one_hot[tag] = one_hot\n",
    "\n",
    "    # # convert the df to list\n",
    "    # data = df.values.tolist()\n",
    "    # dataset = []\n",
    "    # for i in range(len(data)):\n",
    "    #     vector = []\n",
    "    #     for j in range(p):\n",
    "    #         if i - j >= 0:\n",
    "    #             vector.append(data[i - j][0])\n",
    "    #         else:\n",
    "    #             vector.append(torch.zeros(len(word_vectors_all['the'])))\n",
    "    #     # for j in range(s):\n",
    "\n",
    "    #     dataset.append([word_vectors[data[i][0]], pos_tags_one_hot[data[i][1]]])\n",
    "\n",
    "    # convert the df to list\n",
    "    data = df.values.tolist()\n",
    "    dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "    # pp.pprint(dataset)\n",
    "\n",
    "    # make copies of the np.array\n",
    "    dataset1 = dataset.copy()\n",
    "    dataset2 = dataset.copy()\n",
    "\n",
    "    for i in range(p):\n",
    "        dataset1 = dataset1[:-1]\n",
    "        dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "        # extend the ith element of the dataset with the ith element of dataset1\n",
    "        dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "\n",
    "    for i in range(s):\n",
    "        dataset2 = dataset2[1:]\n",
    "        dataset2 = np.insert(dataset2, -1, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "        # extend the ith element of the dataset with the ith element of dataset1\n",
    "        dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "\n",
    "    print(dataset.shape)\n",
    "\n",
    "    return dataset, word_vectors, pos_tags_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48655, 700)\n"
     ]
    }
   ],
   "source": [
    "word_vectors = {}\n",
    "for word in vocab:\n",
    "    if word in word_vectors_all:\n",
    "        word_vectors[word] = word_vectors_all[word]\n",
    "    else:\n",
    "        word_vectors[word] = torch.zeros(len(word_vectors_all['the']))\n",
    "\n",
    "# one hot encode the POS tags\n",
    "pos_tags_one_hot = {}\n",
    "for i, tag in enumerate(pos_tags):\n",
    "    one_hot = torch.zeros(len(pos_tags))\n",
    "    one_hot[i] = 1\n",
    "    pos_tags_one_hot[tag] = one_hot\n",
    "\n",
    "data = df.values.tolist()\n",
    "dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "# pp.pprint(dataset)\n",
    "\n",
    "# make copies of the np.array\n",
    "dataset1 = dataset.copy()\n",
    "dataset2 = dataset.copy()\n",
    "\n",
    "for i in range(3):\n",
    "    dataset1 = dataset1[:-1]\n",
    "    dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "    # extend the ith element of the dataset with the ith element of dataset1\n",
    "    dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "\n",
    "for i in range(3):\n",
    "    dataset2 = dataset2[1:]\n",
    "    dataset2 = np.insert(dataset2, -1, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "    # extend the ith element of the dataset with the ith element of dataset1\n",
    "    dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4274\n",
      "46\n",
      "(48655, 1, 700)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data = df.values.tolist()\n",
    "\n",
    "dataset = np.array([word_vectors[data[i][1]] for i in range(len(data))], dtype=np.float32)\n",
    "# pp.pprint(dataset)\n",
    "\n",
    "# find max value of the dataset's elements 0th index\n",
    "max = 0\n",
    "for i in range(len(dataset)):\n",
    "    if data[i][0] > max:\n",
    "        max = data[i][0]\n",
    "\n",
    "split_dataset = []\n",
    "curr = 0\n",
    "for i in range(1, len(dataset)):\n",
    "    if data[i][0] == 1:\n",
    "        split_dataset.append(dataset[curr:i])\n",
    "        curr = i\n",
    "split_dataset.append(dataset[curr:])\n",
    "print(len(split_dataset))\n",
    "print(max)\n",
    "\n",
    "final_dataset = []\n",
    "for i in range(len(split_dataset)):\n",
    "    dataset = split_dataset[i]\n",
    "    dataset1 = dataset.copy()\n",
    "    dataset2 = dataset.copy()\n",
    "    for j in range(3):\n",
    "        dataset1 = dataset1[:-1]\n",
    "        dataset1 = np.insert(dataset1, 0, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "        dataset = np.hstack((dataset1[:, ], dataset), dtype=np.float32)\n",
    "    for j in range(3):\n",
    "        dataset2 = dataset2[1:]\n",
    "        dataset2 = np.insert(dataset2, -1, [np.zeros(len(word_vectors_all['the']))], axis=0)\n",
    "        dataset = np.hstack((dataset, dataset2[:, ]), dtype=np.float32)\n",
    "    final_dataset.append(dataset)\n",
    "\n",
    "dataset = []\n",
    "for lst in final_dataset:\n",
    "    dataset.extend(lst)\n",
    "dataset = np.reshape(dataset, (len(dataset), 1, len(dataset[0])))\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a FNN which takes n dim input and returns pos tag\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, embed_dim, prev_n, succ_n, hidden_params, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        # for each element in hidden_params, we will create a linear layer\n",
    "        hidden_layers = []\n",
    "        hidden_layers.append(nn.Linear(embed_dim * (prev_n + 1 + succ_n), hidden_params[0]))\n",
    "        hidden_layers.append(nn.ReLU())\n",
    "        for i in range(1, len(hidden_params)):\n",
    "            hidden_layers.append(nn.Linear(hidden_params[i-1], hidden_params[i]))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "        # softmax layer for output\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "        self.output_layer = nn.Linear(hidden_params[-1], output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# create a model\n",
    "input_dim = 100\n",
    "hidden_params = [100, 50]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
